# Client.inference.putLlama

Create a Llama inference endpoint. Create an inference endpoint to perform an inference task with the `llama` service.

## Method Signature

```typescript
client.inference.putLlama(this: That, params: T.InferencePutLlamaRequest, options?: TransportRequestOptionsWithOutMeta): Promise<T.InferencePutLlamaResponse>
```

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `this` | `That` | - |
| `params` | [`InferencePutLlamaRequest`](../types/InferencePutLlamaRequest.md) | - |
| `options?` | `TransportRequestOptionsWithOutMeta` | - |

### Returns

`Promise<T.InferencePutLlamaResponse>`

## See Also

- [Client](../client.md)
- [All APIs](../index.md)
