[[api-reference]]
////////
===========================================================================================================================
||                                                                                                                       ||
||                                                                                                                       ||
||                                                                                                                       ||
||        ██████╗ ███████╗ █████╗ ██████╗ ███╗   ███╗███████╗                                                            ||
||        ██╔══██╗██╔════╝██╔══██╗██╔══██╗████╗ ████║██╔════╝                                                            ||
||        ██████╔╝█████╗  ███████║██║  ██║██╔████╔██║█████╗                                                              ||
||        ██╔══██╗██╔══╝  ██╔══██║██║  ██║██║╚██╔╝██║██╔══╝                                                              ||
||        ██║  ██║███████╗██║  ██║██████╔╝██║ ╚═╝ ██║███████╗                                                            ||
||        ╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝╚═════╝ ╚═╝     ╚═╝╚══════╝                                                            ||
||                                                                                                                       ||
||                                                                                                                       ||
||    This file is autogenerated, DO NOT send pull requests that changes this file directly.                             ||
||    You should update the script that does the generation, which can be found in:                                      ||
||    https://github.com/elastic/elastic-client-generator-js                                                             ||
||                                                                                                                       ||
||    You can run the script with the following command:                                                                 ||
||       npm run elasticsearch -- --version <version>                                                                    ||
||                                                                                                                       ||
||                                                                                                                       ||
||                                                                                                                       ||
===========================================================================================================================
////////
== API Reference

[discrete]
=== bulk
Bulk index or delete documents.
Performs multiple indexing or delete operations in a single API call.
This reduces overhead and can greatly increase indexing speed.

{ref}/docs-bulk.html[Endpoint documentation]
[source,ts]
----
client.bulk({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string)*: Name of the data stream, index, or index alias to perform bulk actions on.
** *`operations` (Optional, { index, create, update, delete } | { detect_noop, doc, doc_as_upsert, script, scripted_upsert, _source, upsert } | object[])*
** *`pipeline` (Optional, string)*: ID of the pipeline to use to preprocess incoming documents.
If the index has a default ingest pipeline specified, then setting the value to `_none` disables the default ingest pipeline for this request.
If a final pipeline is configured it will always run, regardless of the value of this parameter.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true`, Elasticsearch refreshes the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` do nothing with refreshes.
Valid values: `true`, `false`, `wait_for`.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`_source` (Optional, boolean | string | string[])*: `true` or `false` to return the `_source` field or not, or a list of fields to return.
** *`_source_excludes` (Optional, string | string[])*: A list of source fields to exclude from the response.
** *`_source_includes` (Optional, string | string[])*: A list of source fields to include in the response.
** *`timeout` (Optional, string | -1 | 0)*: Period each action waits for the following operations: automatic index creation, dynamic mapping updates, waiting for active shards.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to all or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).
** *`require_alias` (Optional, boolean)*: If `true`, the request’s actions must target an index alias.

[discrete]
=== clear_scroll
Clear a scrolling search.

Clear the search context and results for a scrolling search.

{ref}/clear-scroll-api.html[Endpoint documentation]
[source,ts]
----
client.clearScroll({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`scroll_id` (Optional, string | string[])*: List of scroll IDs to clear.
To clear all scroll IDs, use `_all`.

[discrete]
=== close_point_in_time
Close a point in time.

A point in time must be opened explicitly before being used in search requests.
The `keep_alive` parameter tells Elasticsearch how long it should persist.
A point in time is automatically closed when the `keep_alive` period has elapsed.
However, keeping points in time has a cost; close them as soon as they are no longer required for search requests.

{ref}/point-in-time-api.html[Endpoint documentation]
[source,ts]
----
client.closePointInTime({ id })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: The ID of the point-in-time.

[discrete]
=== count
Returns number of documents matching a query.

{ref}/search-count.html[Endpoint documentation]
[source,ts]
----
client.count({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases to search.
Supports wildcards (`*`).
To search all data streams and indices, omit this parameter or use `*` or `_all`.
** *`query` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Defines the search definition using the Query DSL.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`analyzer` (Optional, string)*: Analyzer to use for the query string.
This parameter can only be used when the `q` query string parameter is specified.
** *`analyze_wildcard` (Optional, boolean)*: If `true`, wildcard and prefix queries are analyzed.
This parameter can only be used when the `q` query string parameter is specified.
** *`default_operator` (Optional, Enum("and" | "or"))*: The default operator for query string query: `AND` or `OR`.
This parameter can only be used when the `q` query string parameter is specified.
** *`df` (Optional, string)*: Field to use as default where no field prefix is given in the query string.
This parameter can only be used when the `q` query string parameter is specified.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
** *`ignore_throttled` (Optional, boolean)*: If `true`, concrete, expanded or aliased indices are ignored when frozen.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`lenient` (Optional, boolean)*: If `true`, format-based query failures (such as providing text to a numeric field) in the query string will be ignored.
** *`min_score` (Optional, number)*: Sets the minimum `_score` value that documents must have to be included in the result.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`terminate_after` (Optional, number)*: Maximum number of documents to collect for each shard.
If a query reaches this limit, Elasticsearch terminates the query early.
Elasticsearch collects documents before sorting.
** *`q` (Optional, string)*: Query in the Lucene query string syntax.

[discrete]
=== create
Index a document.
Adds a JSON document to the specified data stream or index and makes it searchable.
If the target is an index and the document already exists, the request updates the document and increments its version.

{ref}/docs-index_.html[Endpoint documentation]
[source,ts]
----
client.create({ id, index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Unique identifier for the document.
** *`index` (string)*: Name of the data stream or index to target.
If the target doesn’t exist and matches the name or wildcard (`*`) pattern of an index template with a `data_stream` definition, this request creates the data stream.
If the target doesn’t exist and doesn’t match a data stream template, this request creates the index.
** *`document` (Optional, object)*: A document.
** *`pipeline` (Optional, string)*: ID of the pipeline to use to preprocess incoming documents.
If the index has a default ingest pipeline specified, then setting the value to `_none` disables the default ingest pipeline for this request.
If a final pipeline is configured it will always run, regardless of the value of this parameter.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true`, Elasticsearch refreshes the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` do nothing with refreshes.
Valid values: `true`, `false`, `wait_for`.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`timeout` (Optional, string | -1 | 0)*: Period the request waits for the following operations: automatic index creation, dynamic mapping updates, waiting for active shards.
** *`version` (Optional, number)*: Explicit version number for concurrency control.
The specified version must match the current version of the document for the request to succeed.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type: `external`, `external_gte`.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to `all` or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).

[discrete]
=== delete
Delete a document.
Removes a JSON document from the specified index.

{ref}/docs-delete.html[Endpoint documentation]
[source,ts]
----
client.delete({ id, index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Unique identifier for the document.
** *`index` (string)*: Name of the target index.
** *`if_primary_term` (Optional, number)*: Only perform the operation if the document has this primary term.
** *`if_seq_no` (Optional, number)*: Only perform the operation if the document has this sequence number.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true`, Elasticsearch refreshes the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` do nothing with refreshes.
Valid values: `true`, `false`, `wait_for`.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for active shards.
** *`version` (Optional, number)*: Explicit version number for concurrency control.
The specified version must match the current version of the document for the request to succeed.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type: `external`, `external_gte`.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to `all` or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).

[discrete]
=== delete_by_query
Delete documents.
Deletes documents that match the specified query.

{ref}/docs-delete-by-query.html[Endpoint documentation]
[source,ts]
----
client.deleteByQuery({ index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of data streams, indices, and aliases to search.
Supports wildcards (`*`).
To search all data streams or indices, omit this parameter or use `*` or `_all`.
** *`max_docs` (Optional, number)*: The maximum number of documents to delete.
** *`query` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Specifies the documents to delete using the Query DSL.
** *`slice` (Optional, { field, id, max })*: Slice the request manually using the provided slice ID and total number of slices.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
For example, a request targeting `foo*,bar*` returns an error if an index starts with `foo` but no index starts with `bar`.
** *`analyzer` (Optional, string)*: Analyzer to use for the query string.
** *`analyze_wildcard` (Optional, boolean)*: If `true`, wildcard and prefix queries are analyzed.
** *`conflicts` (Optional, Enum("abort" | "proceed"))*: What to do if delete by query hits version conflicts: `abort` or `proceed`.
** *`default_operator` (Optional, Enum("and" | "or"))*: The default operator for query string query: `AND` or `OR`.
** *`df` (Optional, string)*: Field to use as default where no field prefix is given in the query string.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`. Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`from` (Optional, number)*: Starting offset (default: 0)
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`lenient` (Optional, boolean)*: If `true`, format-based query failures (such as providing text to a numeric field) in the query string will be ignored.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`refresh` (Optional, boolean)*: If `true`, Elasticsearch refreshes all shards involved in the delete by query after the request completes.
** *`request_cache` (Optional, boolean)*: If `true`, the request cache is used for this request.
Defaults to the index-level setting.
** *`requests_per_second` (Optional, float)*: The throttle for this request in sub-requests per second.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`q` (Optional, string)*: Query in the Lucene query string syntax.
** *`scroll` (Optional, string | -1 | 0)*: Period to retain the search context for scrolling.
** *`scroll_size` (Optional, number)*: Size of the scroll request that powers the operation.
** *`search_timeout` (Optional, string | -1 | 0)*: Explicit timeout for each search request.
Defaults to no timeout.
** *`search_type` (Optional, Enum("query_then_fetch" | "dfs_query_then_fetch"))*: The type of the search operation.
Available options: `query_then_fetch`, `dfs_query_then_fetch`.
** *`slices` (Optional, number | Enum("auto"))*: The number of slices this task should be divided into.
** *`sort` (Optional, string[])*: A list of <field>:<direction> pairs.
** *`stats` (Optional, string[])*: Specific `tag` of the request for logging and statistical purposes.
** *`terminate_after` (Optional, number)*: Maximum number of documents to collect for each shard.
If a query reaches this limit, Elasticsearch terminates the query early.
Elasticsearch collects documents before sorting.
Use with caution.
Elasticsearch applies this parameter to each shard handling the request.
When possible, let Elasticsearch perform early termination automatically.
Avoid specifying this parameter for requests that target data streams with backing indices across multiple data tiers.
** *`timeout` (Optional, string | -1 | 0)*: Period each deletion request waits for active shards.
** *`version` (Optional, boolean)*: If `true`, returns the document version as part of a hit.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to all or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).
** *`wait_for_completion` (Optional, boolean)*: If `true`, the request blocks until the operation is complete.

[discrete]
=== delete_by_query_rethrottle
Throttle a delete by query operation.

Change the number of requests per second for a particular delete by query operation.
Rethrottling that speeds up the query takes effect immediately but rethrotting that slows down the query takes effect after completing the current batch to prevent scroll timeouts.

{ref}/docs-delete-by-query.html[Endpoint documentation]
[source,ts]
----
client.deleteByQueryRethrottle({ task_id })
----
[discrete]
==== Arguments

* *Request (object):*
** *`task_id` (string | number)*: The ID for the task.
** *`requests_per_second` (Optional, float)*: The throttle for this request in sub-requests per second.

[discrete]
=== delete_script
Delete a script or search template.
Deletes a stored script or search template.

{ref}/modules-scripting.html[Endpoint documentation]
[source,ts]
----
client.deleteScript({ id })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the stored script or search template.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
=== exists
Check a document.
Checks if a specified document exists.

{ref}/docs-get.html[Endpoint documentation]
[source,ts]
----
client.exists({ id, index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier of the document.
** *`index` (string)*: List of data streams, indices, and aliases.
Supports wildcards (`*`).
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`realtime` (Optional, boolean)*: If `true`, the request is real-time as opposed to near-real-time.
** *`refresh` (Optional, boolean)*: If `true`, Elasticsearch refreshes all shards involved in the delete by query after the request completes.
** *`routing` (Optional, string)*: Target the specified primary shard.
** *`_source` (Optional, boolean | string | string[])*: `true` or `false` to return the `_source` field or not, or a list of fields to return.
** *`_source_excludes` (Optional, string | string[])*: A list of source fields to exclude in the response.
** *`_source_includes` (Optional, string | string[])*: A list of source fields to include in the response.
** *`stored_fields` (Optional, string | string[])*: List of stored fields to return as part of a hit.
If no fields are specified, no stored fields are included in the response.
If this field is specified, the `_source` parameter defaults to false.
** *`version` (Optional, number)*: Explicit version number for concurrency control.
The specified version must match the current version of the document for the request to succeed.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type: `external`, `external_gte`.

[discrete]
=== exists_source
Check for a document source.
Checks if a document's `_source` is stored.

{ref}/docs-get.html[Endpoint documentation]
[source,ts]
----
client.existsSource({ id, index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier of the document.
** *`index` (string)*: List of data streams, indices, and aliases.
Supports wildcards (`*`).
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`realtime` (Optional, boolean)*: If true, the request is real-time as opposed to near-real-time.
** *`refresh` (Optional, boolean)*: If `true`, Elasticsearch refreshes all shards involved in the delete by query after the request completes.
** *`routing` (Optional, string)*: Target the specified primary shard.
** *`_source` (Optional, boolean | string | string[])*: `true` or `false` to return the `_source` field or not, or a list of fields to return.
** *`_source_excludes` (Optional, string | string[])*: A list of source fields to exclude in the response.
** *`_source_includes` (Optional, string | string[])*: A list of source fields to include in the response.
** *`version` (Optional, number)*: Explicit version number for concurrency control.
The specified version must match the current version of the document for the request to succeed.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type: `external`, `external_gte`.

[discrete]
=== explain
Explain a document match result.
Returns information about why a specific document matches, or doesn’t match, a query.

{ref}/search-explain.html[Endpoint documentation]
[source,ts]
----
client.explain({ id, index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Defines the document ID.
** *`index` (string)*: Index names used to limit the request.
Only a single index name can be provided to this parameter.
** *`query` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Defines the search definition using the Query DSL.
** *`analyzer` (Optional, string)*: Analyzer to use for the query string.
This parameter can only be used when the `q` query string parameter is specified.
** *`analyze_wildcard` (Optional, boolean)*: If `true`, wildcard and prefix queries are analyzed.
** *`default_operator` (Optional, Enum("and" | "or"))*: The default operator for query string query: `AND` or `OR`.
** *`df` (Optional, string)*: Field to use as default where no field prefix is given in the query string.
** *`lenient` (Optional, boolean)*: If `true`, format-based query failures (such as providing text to a numeric field) in the query string will be ignored.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`_source` (Optional, boolean | string | string[])*: True or false to return the `_source` field or not, or a list of fields to return.
** *`_source_excludes` (Optional, string | string[])*: A list of source fields to exclude from the response.
** *`_source_includes` (Optional, string | string[])*: A list of source fields to include in the response.
** *`stored_fields` (Optional, string | string[])*: A list of stored fields to return in the response.
** *`q` (Optional, string)*: Query in the Lucene query string syntax.

[discrete]
=== field_caps
Get the field capabilities.

Get information about the capabilities of fields among multiple indices.

For data streams, the API returns field capabilities among the stream’s backing indices.
It returns runtime fields like any other field.
For example, a runtime field with a type of keyword is returned the same as any other field that belongs to the `keyword` family.

{ref}/search-field-caps.html[Endpoint documentation]
[source,ts]
----
client.fieldCaps({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit the request. Supports wildcards (*). To target all data streams and indices, omit this parameter or use * or _all.
** *`fields` (Optional, string | string[])*: List of fields to retrieve capabilities for. Wildcard (`*`) expressions are supported.
** *`index_filter` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Allows to filter indices if the provided query rewrites to match_none on every shard.
** *`runtime_mappings` (Optional, Record<string, { fields, fetch_fields, format, input_field, target_field, target_index, script, type }>)*: Defines ad-hoc runtime fields in the request similar to the way it is done in search requests.
These fields exist only as part of the query and take precedence over fields defined with the same name in the index mappings.
** *`allow_no_indices` (Optional, boolean)*: If false, the request returns an error if any wildcard expression, index alias,
or `_all` value targets only missing or closed indices. This behavior applies even if the request targets other open indices. For example, a request
targeting `foo*,bar*` returns an error if an index starts with foo but no index starts with bar.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match. If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams. Supports a list of values, such as `open,hidden`.
** *`ignore_unavailable` (Optional, boolean)*: If `true`, missing or closed indices are not included in the response.
** *`include_unmapped` (Optional, boolean)*: If true, unmapped fields are included in the response.
** *`filters` (Optional, string)*: An optional set of filters: can include +metadata,-metadata,-nested,-multifield,-parent
** *`types` (Optional, string[])*: Only return results for fields that have one of the types in the list
** *`include_empty_fields` (Optional, boolean)*: If false, empty fields are not included in the response.

[discrete]
=== get
Get a document by its ID.
Retrieves the document with the specified ID from an index.

{ref}/docs-get.html[Endpoint documentation]
[source,ts]
----
client.get({ id, index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Unique identifier of the document.
** *`index` (string)*: Name of the index that contains the document.
** *`force_synthetic_source` (Optional, boolean)*: Should this request force synthetic _source?
Use this to test if the mapping supports synthetic _source and to get a sense of the worst case performance.
Fetches with this enabled will be slower the enabling synthetic source natively in the index.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on. Random by default.
** *`realtime` (Optional, boolean)*: If `true`, the request is real-time as opposed to near-real-time.
** *`refresh` (Optional, boolean)*: If true, Elasticsearch refreshes the affected shards to make this operation visible to search. If false, do nothing with refreshes.
** *`routing` (Optional, string)*: Target the specified primary shard.
** *`_source` (Optional, boolean | string | string[])*: True or false to return the _source field or not, or a list of fields to return.
** *`_source_excludes` (Optional, string | string[])*: A list of source fields to exclude in the response.
** *`_source_includes` (Optional, string | string[])*: A list of source fields to include in the response.
** *`stored_fields` (Optional, string | string[])*: List of stored fields to return as part of a hit.
If no fields are specified, no stored fields are included in the response.
If this field is specified, the `_source` parameter defaults to false.
** *`version` (Optional, number)*: Explicit version number for concurrency control. The specified version must match the current version of the document for the request to succeed.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type: internal, external, external_gte.

[discrete]
=== get_script
Get a script or search template.
Retrieves a stored script or search template.

{ref}/modules-scripting.html[Endpoint documentation]
[source,ts]
----
client.getScript({ id })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the stored script or search template.
** *`master_timeout` (Optional, string | -1 | 0)*: Specify timeout for connection to master

[discrete]
=== get_script_context
Get script contexts.

Get a list of supported script contexts and their methods.

{painless}/painless-contexts.html[Endpoint documentation]
[source,ts]
----
client.getScriptContext()
----

[discrete]
=== get_script_languages
Get script languages.

Get a list of available script types, languages, and contexts.

{ref}/modules-scripting.html[Endpoint documentation]
[source,ts]
----
client.getScriptLanguages()
----

[discrete]
=== get_source
Get a document's source.
Returns the source of a document.

{ref}/docs-get.html[Endpoint documentation]
[source,ts]
----
client.getSource({ id, index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Unique identifier of the document.
** *`index` (string)*: Name of the index that contains the document.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on. Random by default.
** *`realtime` (Optional, boolean)*: Boolean) If true, the request is real-time as opposed to near-real-time.
** *`refresh` (Optional, boolean)*: If true, Elasticsearch refreshes the affected shards to make this operation visible to search. If false, do nothing with refreshes.
** *`routing` (Optional, string)*: Target the specified primary shard.
** *`_source` (Optional, boolean | string | string[])*: True or false to return the _source field or not, or a list of fields to return.
** *`_source_excludes` (Optional, string | string[])*: A list of source fields to exclude in the response.
** *`_source_includes` (Optional, string | string[])*: A list of source fields to include in the response.
** *`stored_fields` (Optional, string | string[])*
** *`version` (Optional, number)*: Explicit version number for concurrency control. The specified version must match the current version of the document for the request to succeed.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type: internal, external, external_gte.

[discrete]
=== health_report
Returns the health of the cluster.

{ref}/health-api.html[Endpoint documentation]
[source,ts]
----
client.healthReport({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`feature` (Optional, string | string[])*: A feature of the cluster, as returned by the top-level health report API.
** *`timeout` (Optional, string | -1 | 0)*: Explicit operation timeout.
** *`verbose` (Optional, boolean)*: Opt-in for more information about the health of the system.
** *`size` (Optional, number)*: Limit the number of affected resources the health report API returns.

[discrete]
=== index
Index a document.
Adds a JSON document to the specified data stream or index and makes it searchable.
If the target is an index and the document already exists, the request updates the document and increments its version.

{ref}/docs-index_.html[Endpoint documentation]
[source,ts]
----
client.index({ index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: Name of the data stream or index to target.
** *`id` (Optional, string)*: Unique identifier for the document.
** *`document` (Optional, object)*: A document.
** *`if_primary_term` (Optional, number)*: Only perform the operation if the document has this primary term.
** *`if_seq_no` (Optional, number)*: Only perform the operation if the document has this sequence number.
** *`op_type` (Optional, Enum("index" | "create"))*: Set to create to only index the document if it does not already exist (put if absent).
If a document with the specified `_id` already exists, the indexing operation will fail.
Same as using the `<index>/_create` endpoint.
Valid values: `index`, `create`.
If document id is specified, it defaults to `index`.
Otherwise, it defaults to `create`.
** *`pipeline` (Optional, string)*: ID of the pipeline to use to preprocess incoming documents.
If the index has a default ingest pipeline specified, then setting the value to `_none` disables the default ingest pipeline for this request.
If a final pipeline is configured it will always run, regardless of the value of this parameter.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true`, Elasticsearch refreshes the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` do nothing with refreshes.
Valid values: `true`, `false`, `wait_for`.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`timeout` (Optional, string | -1 | 0)*: Period the request waits for the following operations: automatic index creation, dynamic mapping updates, waiting for active shards.
** *`version` (Optional, number)*: Explicit version number for concurrency control.
The specified version must match the current version of the document for the request to succeed.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type: `external`, `external_gte`.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to all or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).
** *`require_alias` (Optional, boolean)*: If `true`, the destination must be an index alias.

[discrete]
=== info
Get cluster info.
Returns basic information about the cluster.

{ref}/index.html[Endpoint documentation]
[source,ts]
----
client.info()
----

[discrete]
=== knn_search
Run a knn search.

NOTE: The kNN search API has been replaced by the `knn` option in the search API.

Perform a k-nearest neighbor (kNN) search on a dense_vector field and return the matching documents.
Given a query vector, the API finds the k closest vectors and returns those documents as search hits.

Elasticsearch uses the HNSW algorithm to support efficient kNN search.
Like most kNN algorithms, HNSW is an approximate method that sacrifices result accuracy for improved search speed.
This means the results returned are not always the true k closest neighbors.

The kNN search API supports restricting the search using a filter.
The search will return the top k documents that also match the filter query.

{ref}/search-search.html[Endpoint documentation]
[source,ts]
----
client.knnSearch({ index, knn })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: A list of index names to search;
use `_all` or to perform the operation on all indices
** *`knn` ({ field, query_vector, k, num_candidates })*: kNN query to execute
** *`_source` (Optional, boolean | { excludes, includes })*: Indicates which source fields are returned for matching documents. These
fields are returned in the hits._source property of the search response.
** *`docvalue_fields` (Optional, { field, format, include_unmapped }[])*: The request returns doc values for field names matching these patterns
in the hits.fields property of the response. Accepts wildcard (*) patterns.
** *`stored_fields` (Optional, string | string[])*: List of stored fields to return as part of a hit. If no fields are specified,
no stored fields are included in the response. If this field is specified, the _source
parameter defaults to false. You can pass _source: true to return both source fields
and stored fields in the search response.
** *`fields` (Optional, string | string[])*: The request returns values for field names matching these patterns
in the hits.fields property of the response. Accepts wildcard (*) patterns.
** *`filter` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type } | { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type }[])*: Query to filter the documents that can match. The kNN search will return the top
`k` documents that also match this filter. The value can be a single query or a
list of queries. If `filter` isn't provided, all documents are allowed to match.
** *`routing` (Optional, string)*: A list of specific routing values

[discrete]
=== mget
Get multiple documents.

Get multiple JSON documents by ID from one or more indices.
If you specify an index in the request URI, you only need to specify the document IDs in the request body.
To ensure fast responses, this multi get (mget) API responds with partial results if one or more shards fail.

{ref}/docs-multi-get.html[Endpoint documentation]
[source,ts]
----
client.mget({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string)*: Name of the index to retrieve documents from when `ids` are specified, or when a document in the `docs` array does not specify an index.
** *`docs` (Optional, { _id, _index, routing, _source, stored_fields, version, version_type }[])*: The documents you want to retrieve. Required if no index is specified in the request URI.
** *`ids` (Optional, string | string[])*: The IDs of the documents you want to retrieve. Allowed when the index is specified in the request URI.
** *`force_synthetic_source` (Optional, boolean)*: Should this request force synthetic _source?
Use this to test if the mapping supports synthetic _source and to get a sense of the worst case performance.
Fetches with this enabled will be slower the enabling synthetic source natively in the index.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on. Random by default.
** *`realtime` (Optional, boolean)*: If `true`, the request is real-time as opposed to near-real-time.
** *`refresh` (Optional, boolean)*: If `true`, the request refreshes relevant shards before retrieving documents.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`_source` (Optional, boolean | string | string[])*: True or false to return the `_source` field or not, or a list of fields to return.
** *`_source_excludes` (Optional, string | string[])*: A list of source fields to exclude from the response.
You can also use this parameter to exclude fields from the subset specified in `_source_includes` query parameter.
** *`_source_includes` (Optional, string | string[])*: A list of source fields to include in the response.
If this parameter is specified, only these source fields are returned. You can exclude fields from this subset using the `_source_excludes` query parameter.
If the `_source` parameter is `false`, this parameter is ignored.
** *`stored_fields` (Optional, string | string[])*: If `true`, retrieves the document fields stored in the index rather than the document `_source`.

[discrete]
=== msearch
Run multiple searches.

The format of the request is similar to the bulk API format and makes use of the newline delimited JSON (NDJSON) format.
The structure is as follows:

```
header\n
body\n
header\n
body\n
```

This structure is specifically optimized to reduce parsing if a specific search ends up redirected to another node.

IMPORTANT: The final line of data must end with a newline character `\n`.
Each newline character may be preceded by a carriage return `\r`.
When sending requests to this endpoint the `Content-Type` header should be set to `application/x-ndjson`.

{ref}/search-multi-search.html[Endpoint documentation]
[source,ts]
----
client.msearch({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and index aliases to search.
** *`searches` (Optional, { allow_no_indices, expand_wildcards, ignore_unavailable, index, preference, request_cache, routing, search_type, ccs_minimize_roundtrips, allow_partial_search_results, ignore_throttled } | { aggregations, collapse, query, explain, ext, stored_fields, docvalue_fields, knn, from, highlight, indices_boost, min_score, post_filter, profile, rescore, script_fields, search_after, size, sort, _source, fields, terminate_after, stats, timeout, track_scores, track_total_hits, version, runtime_mappings, seq_no_primary_term, pit, suggest }[])*
** *`allow_no_indices` (Optional, boolean)*: If false, the request returns an error if any wildcard expression, index alias, or _all value targets only missing or closed indices. This behavior applies even if the request targets other open indices. For example, a request targeting foo*,bar* returns an error if an index starts with foo but no index starts with bar.
** *`ccs_minimize_roundtrips` (Optional, boolean)*: If true, network roundtrips between the coordinating node and remote clusters are minimized for cross-cluster search requests.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard expressions can match. If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
** *`ignore_throttled` (Optional, boolean)*: If true, concrete, expanded or aliased indices are ignored when frozen.
** *`ignore_unavailable` (Optional, boolean)*: If true, missing or closed indices are not included in the response.
** *`include_named_queries_score` (Optional, boolean)*: Indicates whether hit.matched_queries should be rendered as a map that includes
the name of the matched query associated with its score (true)
or as an array containing the name of the matched queries (false)
This functionality reruns each named query on every hit in a search response.
Typically, this adds a small overhead to a request.
However, using computationally expensive named queries on a large number of hits may add significant overhead.
** *`max_concurrent_searches` (Optional, number)*: Maximum number of concurrent searches the multi search API can execute.
** *`max_concurrent_shard_requests` (Optional, number)*: Maximum number of concurrent shard requests that each sub-search request executes per node.
** *`pre_filter_shard_size` (Optional, number)*: Defines a threshold that enforces a pre-filter roundtrip to prefilter search shards based on query rewriting if the number of shards the search request expands to exceeds the threshold. This filter roundtrip can limit the number of shards significantly if for instance a shard can not match any documents based on its rewrite method i.e., if date filters are mandatory to match but the shard bounds and the query are disjoint.
** *`rest_total_hits_as_int` (Optional, boolean)*: If true, hits.total are returned as an integer in the response. Defaults to false, which returns an object.
** *`routing` (Optional, string)*: Custom routing value used to route search operations to a specific shard.
** *`search_type` (Optional, Enum("query_then_fetch" | "dfs_query_then_fetch"))*: Indicates whether global term and document frequencies should be used when scoring returned documents.
** *`typed_keys` (Optional, boolean)*: Specifies whether aggregation and suggester names should be prefixed by their respective types in the response.

[discrete]
=== msearch_template
Run multiple templated searches.

{ref}/search-multi-search.html[Endpoint documentation]
[source,ts]
----
client.msearchTemplate({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases to search.
Supports wildcards (`*`).
To search all data streams and indices, omit this parameter or use `*`.
** *`search_templates` (Optional, { allow_no_indices, expand_wildcards, ignore_unavailable, index, preference, request_cache, routing, search_type, ccs_minimize_roundtrips, allow_partial_search_results, ignore_throttled } | { aggregations, collapse, query, explain, ext, stored_fields, docvalue_fields, knn, from, highlight, indices_boost, min_score, post_filter, profile, rescore, script_fields, search_after, size, sort, _source, fields, terminate_after, stats, timeout, track_scores, track_total_hits, version, runtime_mappings, seq_no_primary_term, pit, suggest }[])*
** *`ccs_minimize_roundtrips` (Optional, boolean)*: If `true`, network round-trips are minimized for cross-cluster search requests.
** *`max_concurrent_searches` (Optional, number)*: Maximum number of concurrent searches the API can run.
** *`search_type` (Optional, Enum("query_then_fetch" | "dfs_query_then_fetch"))*: The type of the search operation.
Available options: `query_then_fetch`, `dfs_query_then_fetch`.
** *`rest_total_hits_as_int` (Optional, boolean)*: If `true`, the response returns `hits.total` as an integer.
If `false`, it returns `hits.total` as an object.
** *`typed_keys` (Optional, boolean)*: If `true`, the response prefixes aggregation and suggester names with their respective types.

[discrete]
=== mtermvectors
Get multiple term vectors.

You can specify existing documents by index and ID or provide artificial documents in the body of the request.
You can specify the index in the request body or request URI.
The response contains a `docs` array with all the fetched termvectors.
Each element has the structure provided by the termvectors API.

{ref}/docs-multi-termvectors.html[Endpoint documentation]
[source,ts]
----
client.mtermvectors({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string)*: Name of the index that contains the documents.
** *`docs` (Optional, { _id, _index, routing, _source, stored_fields, version, version_type }[])*: Array of existing or artificial documents.
** *`ids` (Optional, string[])*: Simplified syntax to specify documents by their ID if they're in the same index.
** *`fields` (Optional, string | string[])*: List or wildcard expressions of fields to include in the statistics.
Used as the default list unless a specific field list is provided in the `completion_fields` or `fielddata_fields` parameters.
** *`field_statistics` (Optional, boolean)*: If `true`, the response includes the document count, sum of document frequencies, and sum of total term frequencies.
** *`offsets` (Optional, boolean)*: If `true`, the response includes term offsets.
** *`payloads` (Optional, boolean)*: If `true`, the response includes term payloads.
** *`positions` (Optional, boolean)*: If `true`, the response includes term positions.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`realtime` (Optional, boolean)*: If true, the request is real-time as opposed to near-real-time.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`term_statistics` (Optional, boolean)*: If true, the response includes term frequency and document frequency.
** *`version` (Optional, number)*: If `true`, returns the document version as part of a hit.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type.

[discrete]
=== open_point_in_time
Open a point in time.

A search request by default runs against the most recent visible data of the target indices,
which is called point in time. Elasticsearch pit (point in time) is a lightweight view into the
state of the data as it existed when initiated. In some cases, it’s preferred to perform multiple
search requests using the same point in time. For example, if refreshes happen between
`search_after` requests, then the results of those requests might not be consistent as changes happening
between searches are only visible to the more recent point in time.

A point in time must be opened explicitly before being used in search requests.
The `keep_alive` parameter tells Elasticsearch how long it should persist.

{ref}/point-in-time-api.html[Endpoint documentation]
[source,ts]
----
client.openPointInTime({ index, keep_alive })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: A list of index names to open point in time; use `_all` or empty string to perform the operation on all indices
** *`keep_alive` (string | -1 | 0)*: Extends the time to live of the corresponding point in time.
** *`index_filter` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Allows to filter indices if the provided query rewrites to `match_none` on every shard.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`. Valid values are: `all`, `open`, `closed`, `hidden`, `none`.

[discrete]
=== ping
Ping the cluster.
Returns whether the cluster is running.

{ref}/index.html[Endpoint documentation]
[source,ts]
----
client.ping()
----

[discrete]
=== put_script
Create or update a script or search template.
Creates or updates a stored script or search template.

{ref}/modules-scripting.html[Endpoint documentation]
[source,ts]
----
client.putScript({ id, script })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the stored script or search template.
Must be unique within the cluster.
** *`script` ({ lang, options, source })*: Contains the script or search template, its parameters, and its language.
** *`context` (Optional, string)*: Context in which the script or search template should run.
To prevent errors, the API immediately compiles the script or template in this context.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
=== rank_eval
Evaluate ranked search results.

Evaluate the quality of ranked search results over a set of typical search queries.

{ref}/search-rank-eval.html[Endpoint documentation]
[source,ts]
----
client.rankEval({ requests })
----
[discrete]
==== Arguments

* *Request (object):*
** *`requests` ({ id, request, ratings, template_id, params }[])*: A set of typical search requests, together with their provided ratings.
** *`index` (Optional, string | string[])*: List of data streams, indices, and index aliases used to limit the request. Wildcard (`*`) expressions are supported.
To target all data streams and indices in a cluster, omit this parameter or use `_all` or `*`.
** *`metric` (Optional, { precision, recall, mean_reciprocal_rank, dcg, expected_reciprocal_rank })*: Definition of the evaluation metric to calculate.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices. This behavior applies even if the request targets other open indices. For example, a request targeting `foo*,bar*` returns an error if an index starts with `foo` but no index starts with `bar`.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Whether to expand wildcard expression to concrete indices that are open, closed or both.
** *`ignore_unavailable` (Optional, boolean)*: If `true`, missing or closed indices are not included in the response.
** *`search_type` (Optional, string)*: Search operation type

[discrete]
=== reindex
Reindex documents.
Copies documents from a source to a destination. The source can be any existing index, alias, or data stream. The destination must differ from the source. For example, you cannot reindex a data stream into itself.

{ref}/docs-reindex.html[Endpoint documentation]
[source,ts]
----
client.reindex({ dest, source })
----
[discrete]
==== Arguments

* *Request (object):*
** *`dest` ({ index, op_type, pipeline, routing, version_type })*: The destination you are copying to.
** *`source` ({ index, query, remote, size, slice, sort, _source, runtime_mappings })*: The source you are copying from.
** *`conflicts` (Optional, Enum("abort" | "proceed"))*: Set to proceed to continue reindexing even if there are conflicts.
** *`max_docs` (Optional, number)*: The maximum number of documents to reindex.
** *`script` (Optional, { source, id, params, lang, options })*: The script to run to update the document source or metadata when reindexing.
** *`size` (Optional, number)*
** *`refresh` (Optional, boolean)*: If `true`, the request refreshes affected shards to make this operation visible to search.
** *`requests_per_second` (Optional, float)*: The throttle for this request in sub-requests per second.
Defaults to no throttle.
** *`scroll` (Optional, string | -1 | 0)*: Specifies how long a consistent view of the index should be maintained for scrolled search.
** *`slices` (Optional, number | Enum("auto"))*: The number of slices this task should be divided into.
Defaults to 1 slice, meaning the task isn’t sliced into subtasks.
** *`timeout` (Optional, string | -1 | 0)*: Period each indexing waits for automatic index creation, dynamic mapping updates, and waiting for active shards.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to `all` or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).
** *`wait_for_completion` (Optional, boolean)*: If `true`, the request blocks until the operation is complete.
** *`require_alias` (Optional, boolean)*: If `true`, the destination must be an index alias.

[discrete]
=== reindex_rethrottle
Throttle a reindex operation.

Change the number of requests per second for a particular reindex operation.

{ref}/docs-reindex.html[Endpoint documentation]
[source,ts]
----
client.reindexRethrottle({ task_id })
----
[discrete]
==== Arguments

* *Request (object):*
** *`task_id` (string)*: Identifier for the task.
** *`requests_per_second` (Optional, float)*: The throttle for this request in sub-requests per second.

[discrete]
=== render_search_template
Render a search template.

Render a search template as a search request body.

{ref}/render-search-template-api.html[Endpoint documentation]
[source,ts]
----
client.renderSearchTemplate({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: ID of the search template to render.
If no `source` is specified, this or the `id` request body parameter is required.
** *`file` (Optional, string)*
** *`params` (Optional, Record<string, User-defined value>)*: Key-value pairs used to replace Mustache variables in the template.
The key is the variable name.
The value is the variable value.
** *`source` (Optional, string)*: An inline search template.
Supports the same parameters as the search API's request body.
These parameters also support Mustache variables.
If no `id` or `<templated-id>` is specified, this parameter is required.

[discrete]
=== scripts_painless_execute
Run a script.
Runs a script and returns a result.

{painless}/painless-execute-api.html[Endpoint documentation]
[source,ts]
----
client.scriptsPainlessExecute({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`context` (Optional, string)*: The context that the script should run in.
** *`context_setup` (Optional, { document, index, query })*: Additional parameters for the `context`.
** *`script` (Optional, { source, id, params, lang, options })*: The Painless script to execute.

[discrete]
=== scroll
Run a scrolling search.

IMPORTANT: The scroll API is no longer recommend for deep pagination. If you need to preserve the index state while paging through more than 10,000 hits, use the `search_after` parameter with a point in time (PIT).

The scroll API gets large sets of results from a single scrolling search request.
To get the necessary scroll ID, submit a search API request that includes an argument for the `scroll` query parameter.
The `scroll` parameter indicates how long Elasticsearch should retain the search context for the request.
The search response returns a scroll ID in the `_scroll_id` response body parameter.
You can then use the scroll ID with the scroll API to retrieve the next batch of results for the request.
If the Elasticsearch security features are enabled, the access to the results of a specific scroll ID is restricted to the user or API key that submitted the search.

You can also use the scroll API to specify a new scroll parameter that extends or shortens the retention period for the search context.

IMPORTANT: Results from a scrolling search reflect the state of the index at the time of the initial search request. Subsequent indexing or document changes only affect later search and scroll requests.

{ref}/search-request-body.html[Endpoint documentation]
[source,ts]
----
client.scroll({ scroll_id })
----
[discrete]
==== Arguments

* *Request (object):*
** *`scroll_id` (string)*: Scroll ID of the search.
** *`scroll` (Optional, string | -1 | 0)*: Period to retain the search context for scrolling.
** *`rest_total_hits_as_int` (Optional, boolean)*: If true, the API response’s hit.total property is returned as an integer. If false, the API response’s hit.total property is returned as an object.

[discrete]
=== search
Run a search.

Get search hits that match the query defined in the request.
You can provide search queries using the `q` query string parameter or the request body.
If both are specified, only the query parameter is used.

{ref}/search-search.html[Endpoint documentation]
[source,ts]
----
client.search({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases to search.
Supports wildcards (`*`).
To search all data streams and indices, omit this parameter or use `*` or `_all`.
** *`aggregations` (Optional, Record<string, { aggregations, meta, adjacency_matrix, auto_date_histogram, avg, avg_bucket, boxplot, bucket_script, bucket_selector, bucket_sort, bucket_count_ks_test, bucket_correlation, cardinality, categorize_text, children, composite, cumulative_cardinality, cumulative_sum, date_histogram, date_range, derivative, diversified_sampler, extended_stats, extended_stats_bucket, frequent_item_sets, filter, filters, geo_bounds, geo_centroid, geo_distance, geohash_grid, geo_line, geotile_grid, geohex_grid, global, histogram, ip_range, ip_prefix, inference, line, matrix_stats, max, max_bucket, median_absolute_deviation, min, min_bucket, missing, moving_avg, moving_percentiles, moving_fn, multi_terms, nested, normalize, parent, percentile_ranks, percentiles, percentiles_bucket, range, rare_terms, rate, reverse_nested, random_sampler, sampler, scripted_metric, serial_diff, significant_terms, significant_text, stats, stats_bucket, string_stats, sum, sum_bucket, terms, time_series, top_hits, t_test, top_metrics, value_count, weighted_avg, variable_width_histogram }>)*: Defines the aggregations that are run as part of the search request.
** *`collapse` (Optional, { field, inner_hits, max_concurrent_group_searches, collapse })*: Collapses search results the values of the specified field.
** *`explain` (Optional, boolean)*: If true, returns detailed information about score computation as part of a hit.
** *`ext` (Optional, Record<string, User-defined value>)*: Configuration of search extensions defined by Elasticsearch plugins.
** *`from` (Optional, number)*: Starting document offset.
Needs to be non-negative.
By default, you cannot page through more than 10,000 hits using the `from` and `size` parameters.
To page through more hits, use the `search_after` parameter.
** *`highlight` (Optional, { encoder, fields })*: Specifies the highlighter to use for retrieving highlighted snippets from one or more fields in your search results.
** *`track_total_hits` (Optional, boolean | number)*: Number of hits matching the query to count accurately.
If `true`, the exact number of hits is returned at the cost of some performance.
If `false`, the  response does not include the total number of hits matching the query.
** *`indices_boost` (Optional, Record<string, number>[])*: Boosts the _score of documents from specified indices.
** *`docvalue_fields` (Optional, { field, format, include_unmapped }[])*: Array of wildcard (`*`) patterns.
The request returns doc values for field names matching these patterns in the `hits.fields` property of the response.
** *`knn` (Optional, { field, query_vector, query_vector_builder, k, num_candidates, boost, filter, similarity, inner_hits } | { field, query_vector, query_vector_builder, k, num_candidates, boost, filter, similarity, inner_hits }[])*: Defines the approximate kNN search to run.
** *`rank` (Optional, { rrf })*: Defines the Reciprocal Rank Fusion (RRF) to use.
** *`min_score` (Optional, number)*: Minimum `_score` for matching documents.
Documents with a lower `_score` are not included in the search results.
** *`post_filter` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Use the `post_filter` parameter to filter search results.
The search hits are filtered after the aggregations are calculated.
A post filter has no impact on the aggregation results.
** *`profile` (Optional, boolean)*: Set to `true` to return detailed timing information about the execution of individual components in a search request.
NOTE: This is a debugging tool and adds significant overhead to search execution.
** *`query` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Defines the search definition using the Query DSL.
** *`rescore` (Optional, { window_size, query, learning_to_rank } | { window_size, query, learning_to_rank }[])*: Can be used to improve precision by reordering just the top (for example 100 - 500) documents returned by the `query` and `post_filter` phases.
** *`retriever` (Optional, { standard, knn, rrf, text_similarity_reranker })*: A retriever is a specification to describe top documents returned from a search. A retriever replaces other elements of the search API that also return top documents such as query and knn.
** *`script_fields` (Optional, Record<string, { script, ignore_failure }>)*: Retrieve a script evaluation (based on different fields) for each hit.
** *`search_after` (Optional, number | number | string | boolean | null | User-defined value[])*: Used to retrieve the next page of hits using a set of sort values from the previous page.
** *`size` (Optional, number)*: The number of hits to return.
By default, you cannot page through more than 10,000 hits using the `from` and `size` parameters.
To page through more hits, use the `search_after` parameter.
** *`slice` (Optional, { field, id, max })*: Can be used to split a scrolled search into multiple slices that can be consumed independently.
** *`sort` (Optional, string | { _score, _doc, _geo_distance, _script } | string | { _score, _doc, _geo_distance, _script }[])*: A list of <field>:<direction> pairs.
** *`_source` (Optional, boolean | { excludes, includes })*: Indicates which source fields are returned for matching documents.
These fields are returned in the hits._source property of the search response.
** *`fields` (Optional, { field, format, include_unmapped }[])*: Array of wildcard (`*`) patterns.
The request returns values for field names matching these patterns in the `hits.fields` property of the response.
** *`suggest` (Optional, { text })*: Defines a suggester that provides similar looking terms based on a provided text.
** *`terminate_after` (Optional, number)*: Maximum number of documents to collect for each shard.
If a query reaches this limit, Elasticsearch terminates the query early.
Elasticsearch collects documents before sorting.
Use with caution.
Elasticsearch applies this parameter to each shard handling the request.
When possible, let Elasticsearch perform early termination automatically.
Avoid specifying this parameter for requests that target data streams with backing indices across multiple data tiers.
If set to `0` (default), the query does not terminate early.
** *`timeout` (Optional, string)*: Specifies the period of time to wait for a response from each shard.
If no response is received before the timeout expires, the request fails and returns an error.
Defaults to no timeout.
** *`track_scores` (Optional, boolean)*: If true, calculate and return document scores, even if the scores are not used for sorting.
** *`version` (Optional, boolean)*: If true, returns document version as part of a hit.
** *`seq_no_primary_term` (Optional, boolean)*: If `true`, returns sequence number and primary term of the last modification of each hit.
** *`stored_fields` (Optional, string | string[])*: List of stored fields to return as part of a hit.
If no fields are specified, no stored fields are included in the response.
If this field is specified, the `_source` parameter defaults to `false`.
You can pass `_source: true` to return both source fields and stored fields in the search response.
** *`pit` (Optional, { id, keep_alive })*: Limits the search to a point in time (PIT).
If you provide a PIT, you cannot specify an `<index>` in the request path.
** *`runtime_mappings` (Optional, Record<string, { fields, fetch_fields, format, input_field, target_field, target_index, script, type }>)*: Defines one or more runtime fields in the search request.
These fields take precedence over mapped fields with the same name.
** *`stats` (Optional, string[])*: Stats groups to associate with the search.
Each group maintains a statistics aggregation for its associated searches.
You can retrieve these stats using the indices stats API.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
For example, a request targeting `foo*,bar*` returns an error if an index starts with `foo` but no index starts with `bar`.
** *`allow_partial_search_results` (Optional, boolean)*: If true, returns partial results if there are shard request timeouts or shard failures. If false, returns an error with no partial results.
** *`analyzer` (Optional, string)*: Analyzer to use for the query string.
This parameter can only be used when the q query string parameter is specified.
** *`analyze_wildcard` (Optional, boolean)*: If true, wildcard and prefix queries are analyzed.
This parameter can only be used when the q query string parameter is specified.
** *`batched_reduce_size` (Optional, number)*: The number of shard results that should be reduced at once on the coordinating node.
This value should be used as a protection mechanism to reduce the memory overhead per search request if the potential number of shards in the request can be large.
** *`ccs_minimize_roundtrips` (Optional, boolean)*: If true, network round-trips between the coordinating node and the remote clusters are minimized when executing cross-cluster search (CCS) requests.
** *`default_operator` (Optional, Enum("and" | "or"))*: The default operator for query string query: AND or OR.
This parameter can only be used when the `q` query string parameter is specified.
** *`df` (Optional, string)*: Field to use as default where no field prefix is given in the query string.
This parameter can only be used when the q query string parameter is specified.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
** *`ignore_throttled` (Optional, boolean)*: If `true`, concrete, expanded or aliased indices will be ignored when frozen.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`include_named_queries_score` (Optional, boolean)*: Indicates whether hit.matched_queries should be rendered as a map that includes
the name of the matched query associated with its score (true)
or as an array containing the name of the matched queries (false)
This functionality reruns each named query on every hit in a search response.
Typically, this adds a small overhead to a request.
However, using computationally expensive named queries on a large number of hits may add significant overhead.
** *`lenient` (Optional, boolean)*: If `true`, format-based query failures (such as providing text to a numeric field) in the query string will be ignored.
This parameter can only be used when the `q` query string parameter is specified.
** *`max_concurrent_shard_requests` (Optional, number)*: Defines the number of concurrent shard requests per node this search executes concurrently.
This value should be used to limit the impact of the search on the cluster in order to limit the number of concurrent shard requests.
** *`min_compatible_shard_node` (Optional, string)*: The minimum version of the node that can handle the request
Any handling node with a lower version will fail the request.
** *`preference` (Optional, string)*: Nodes and shards used for the search.
By default, Elasticsearch selects from eligible nodes and shards using adaptive replica selection, accounting for allocation awareness. Valid values are:
`_only_local` to run the search only on shards on the local node;
`_local` to, if possible, run the search on shards on the local node, or if not, select shards using the default method;
`_only_nodes:<node-id>,<node-id>` to run the search on only the specified nodes IDs, where, if suitable shards exist on more than one selected node, use shards on those nodes using the default method, or if none of the specified nodes are available, select shards from any available node using the default method;
`_prefer_nodes:<node-id>,<node-id>` to if possible, run the search on the specified nodes IDs, or if not, select shards using the default method;
`_shards:<shard>,<shard>` to run the search only on the specified shards;
`<custom-string>` (any string that does not start with `_`) to route searches with the same `<custom-string>` to the same shards in the same order.
** *`pre_filter_shard_size` (Optional, number)*: Defines a threshold that enforces a pre-filter roundtrip to prefilter search shards based on query rewriting if the number of shards the search request expands to exceeds the threshold.
This filter roundtrip can limit the number of shards significantly if for instance a shard can not match any documents based on its rewrite method (if date filters are mandatory to match but the shard bounds and the query are disjoint).
When unspecified, the pre-filter phase is executed if any of these conditions is met:
the request targets more than 128 shards;
the request targets one or more read-only index;
the primary sort of the query targets an indexed field.
** *`request_cache` (Optional, boolean)*: If `true`, the caching of search results is enabled for requests where `size` is `0`.
Defaults to index level settings.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`scroll` (Optional, string | -1 | 0)*: Period to retain the search context for scrolling. See Scroll search results.
By default, this value cannot exceed `1d` (24 hours).
You can change this limit using the `search.max_keep_alive` cluster-level setting.
** *`search_type` (Optional, Enum("query_then_fetch" | "dfs_query_then_fetch"))*: How distributed term frequencies are calculated for relevance scoring.
** *`suggest_field` (Optional, string)*: Specifies which field to use for suggestions.
** *`suggest_mode` (Optional, Enum("missing" | "popular" | "always"))*: Specifies the suggest mode.
This parameter can only be used when the `suggest_field` and `suggest_text` query string parameters are specified.
** *`suggest_size` (Optional, number)*: Number of suggestions to return.
This parameter can only be used when the `suggest_field` and `suggest_text` query string parameters are specified.
** *`suggest_text` (Optional, string)*: The source text for which the suggestions should be returned.
This parameter can only be used when the `suggest_field` and `suggest_text` query string parameters are specified.
** *`typed_keys` (Optional, boolean)*: If `true`, aggregation and suggester names are be prefixed by their respective types in the response.
** *`rest_total_hits_as_int` (Optional, boolean)*: Indicates whether `hits.total` should be rendered as an integer or an object in the rest search response.
** *`_source_excludes` (Optional, string | string[])*: A list of source fields to exclude from the response.
You can also use this parameter to exclude fields from the subset specified in `_source_includes` query parameter.
If the `_source` parameter is `false`, this parameter is ignored.
** *`_source_includes` (Optional, string | string[])*: A list of source fields to include in the response.
If this parameter is specified, only these source fields are returned.
You can exclude fields from this subset using the `_source_excludes` query parameter.
If the `_source` parameter is `false`, this parameter is ignored.
** *`q` (Optional, string)*: Query in the Lucene query string syntax using query parameter search.
Query parameter searches do not support the full Elasticsearch Query DSL but are handy for testing.
** *`force_synthetic_source` (Optional, boolean)*: Should this request force synthetic _source?
Use this to test if the mapping supports synthetic _source and to get a sense of the worst case performance.
Fetches with this enabled will be slower the enabling synthetic source natively in the index.

[discrete]
=== search_mvt
Search a vector tile.

Search a vector tile for geospatial values.

{ref}/search-vector-tile-api.html[Endpoint documentation]
[source,ts]
----
client.searchMvt({ index, field, zoom, x, y })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of data streams, indices, or aliases to search
** *`field` (string)*: Field containing geospatial data to return
** *`zoom` (number)*: Zoom level for the vector tile to search
** *`x` (number)*: X coordinate for the vector tile to search
** *`y` (number)*: Y coordinate for the vector tile to search
** *`aggs` (Optional, Record<string, { aggregations, meta, adjacency_matrix, auto_date_histogram, avg, avg_bucket, boxplot, bucket_script, bucket_selector, bucket_sort, bucket_count_ks_test, bucket_correlation, cardinality, categorize_text, children, composite, cumulative_cardinality, cumulative_sum, date_histogram, date_range, derivative, diversified_sampler, extended_stats, extended_stats_bucket, frequent_item_sets, filter, filters, geo_bounds, geo_centroid, geo_distance, geohash_grid, geo_line, geotile_grid, geohex_grid, global, histogram, ip_range, ip_prefix, inference, line, matrix_stats, max, max_bucket, median_absolute_deviation, min, min_bucket, missing, moving_avg, moving_percentiles, moving_fn, multi_terms, nested, normalize, parent, percentile_ranks, percentiles, percentiles_bucket, range, rare_terms, rate, reverse_nested, random_sampler, sampler, scripted_metric, serial_diff, significant_terms, significant_text, stats, stats_bucket, string_stats, sum, sum_bucket, terms, time_series, top_hits, t_test, top_metrics, value_count, weighted_avg, variable_width_histogram }>)*: Sub-aggregations for the geotile_grid.

Supports the following aggregation types:
- avg
- cardinality
- max
- min
- sum
** *`buffer` (Optional, number)*: Size, in pixels, of a clipping buffer outside the tile. This allows renderers
to avoid outline artifacts from geometries that extend past the extent of the tile.
** *`exact_bounds` (Optional, boolean)*: If false, the meta layer’s feature is the bounding box of the tile.
If true, the meta layer’s feature is a bounding box resulting from a
geo_bounds aggregation. The aggregation runs on <field> values that intersect
the <zoom>/<x>/<y> tile with wrap_longitude set to false. The resulting
bounding box may be larger than the vector tile.
** *`extent` (Optional, number)*: Size, in pixels, of a side of the tile. Vector tiles are square with equal sides.
** *`fields` (Optional, string | string[])*: Fields to return in the `hits` layer. Supports wildcards (`*`).
This parameter does not support fields with array values. Fields with array
values may return inconsistent results.
** *`grid_agg` (Optional, Enum("geotile" | "geohex"))*: Aggregation used to create a grid for the `field`.
** *`grid_precision` (Optional, number)*: Additional zoom levels available through the aggs layer. For example, if <zoom> is 7
and grid_precision is 8, you can zoom in up to level 15. Accepts 0-8. If 0, results
don’t include the aggs layer.
** *`grid_type` (Optional, Enum("grid" | "point" | "centroid"))*: Determines the geometry type for features in the aggs layer. In the aggs layer,
each feature represents a geotile_grid cell. If 'grid' each feature is a Polygon
of the cells bounding box. If 'point' each feature is a Point that is the centroid
of the cell.
** *`query` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Query DSL used to filter documents for the search.
** *`runtime_mappings` (Optional, Record<string, { fields, fetch_fields, format, input_field, target_field, target_index, script, type }>)*: Defines one or more runtime fields in the search request. These fields take
precedence over mapped fields with the same name.
** *`size` (Optional, number)*: Maximum number of features to return in the hits layer. Accepts 0-10000.
If 0, results don’t include the hits layer.
** *`sort` (Optional, string | { _score, _doc, _geo_distance, _script } | string | { _score, _doc, _geo_distance, _script }[])*: Sorts features in the hits layer. By default, the API calculates a bounding
box for each feature. It sorts features based on this box’s diagonal length,
from longest to shortest.
** *`track_total_hits` (Optional, boolean | number)*: Number of hits matching the query to count accurately. If `true`, the exact number
of hits is returned at the cost of some performance. If `false`, the response does
not include the total number of hits matching the query.
** *`with_labels` (Optional, boolean)*: If `true`, the hits and aggs layers will contain additional point features representing
suggested label positions for the original features.

[discrete]
=== search_shards
Get the search shards.

Get the indices and shards that a search request would be run against.
This information can be useful for working out issues or planning optimizations with routing and shard preferences.
When filtered aliases are used, the filter is returned as part of the indices section.

{ref}/search-shards.html[Endpoint documentation]
[source,ts]
----
client.searchShards({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: Returns the indices and shards that a search request would be executed against.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
For example, a request targeting `foo*,bar*` returns an error if an index starts with `foo` but no index starts with `bar`.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`local` (Optional, boolean)*: If `true`, the request retrieves information from the local node only.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.

[discrete]
=== search_template
Run a search with a search template.

{ref}/search-template.html[Endpoint documentation]
[source,ts]
----
client.searchTemplate({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices,
and aliases to search. Supports wildcards (*).
** *`explain` (Optional, boolean)*: If `true`, returns detailed information about score calculation as part of each hit.
** *`id` (Optional, string)*: ID of the search template to use. If no source is specified,
this parameter is required.
** *`params` (Optional, Record<string, User-defined value>)*: Key-value pairs used to replace Mustache variables in the template.
The key is the variable name.
The value is the variable value.
** *`profile` (Optional, boolean)*: If `true`, the query execution is profiled.
** *`source` (Optional, string)*: An inline search template. Supports the same parameters as the search API's
request body. Also supports Mustache variables. If no id is specified, this
parameter is required.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
For example, a request targeting `foo*,bar*` returns an error if an index starts with `foo` but no index starts with `bar`.
** *`ccs_minimize_roundtrips` (Optional, boolean)*: If `true`, network round-trips are minimized for cross-cluster search requests.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_throttled` (Optional, boolean)*: If `true`, specified concrete, expanded, or aliased indices are not included in the response when throttled.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`scroll` (Optional, string | -1 | 0)*: Specifies how long a consistent view of the index
should be maintained for scrolled search.
** *`search_type` (Optional, Enum("query_then_fetch" | "dfs_query_then_fetch"))*: The type of the search operation.
** *`rest_total_hits_as_int` (Optional, boolean)*: If true, hits.total are rendered as an integer in the response.
** *`typed_keys` (Optional, boolean)*: If `true`, the response prefixes aggregation and suggester names with their respective types.

[discrete]
=== terms_enum
Get terms in an index.

Discover terms that match a partial string in an index.
This "terms enum" API is designed for low-latency look-ups used in auto-complete scenarios.

If the `complete` property in the response is false, the returned terms set may be incomplete and should be treated as approximate.
This can occur due to a few reasons, such as a request timeout or a node error.

NOTE: The terms enum API may return terms from deleted documents. Deleted documents are initially only marked as deleted. It is not until their segments are merged that documents are actually deleted. Until that happens, the terms enum API will return terms from these documents.

{ref}/search-terms-enum.html[Endpoint documentation]
[source,ts]
----
client.termsEnum({ index, field })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: List of data streams, indices, and index aliases to search. Wildcard (*) expressions are supported.
** *`field` (string)*: The string to match at the start of indexed terms. If not provided, all terms in the field are considered.
** *`size` (Optional, number)*: How many matching terms to return.
** *`timeout` (Optional, string | -1 | 0)*: The maximum length of time to spend collecting results. Defaults to "1s" (one second). If the timeout is exceeded the complete flag set to false in the response and the results may be partial or empty.
** *`case_insensitive` (Optional, boolean)*: When true the provided search string is matched against index terms without case sensitivity.
** *`index_filter` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Allows to filter an index shard if the provided query rewrites to match_none.
** *`string` (Optional, string)*: The string after which terms in the index should be returned. Allows for a form of pagination if the last result from one request is passed as the search_after parameter for a subsequent request.
** *`search_after` (Optional, string)*

[discrete]
=== termvectors
Get term vector information.

Get information and statistics about terms in the fields of a particular document.

{ref}/docs-termvectors.html[Endpoint documentation]
[source,ts]
----
client.termvectors({ index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: Name of the index that contains the document.
** *`id` (Optional, string)*: Unique identifier of the document.
** *`doc` (Optional, object)*: An artificial document (a document not present in the index) for which you want to retrieve term vectors.
** *`filter` (Optional, { max_doc_freq, max_num_terms, max_term_freq, max_word_length, min_doc_freq, min_term_freq, min_word_length })*: Filter terms based on their tf-idf scores.
** *`per_field_analyzer` (Optional, Record<string, string>)*: Overrides the default per-field analyzer.
** *`fields` (Optional, string | string[])*: List or wildcard expressions of fields to include in the statistics.
Used as the default list unless a specific field list is provided in the `completion_fields` or `fielddata_fields` parameters.
** *`field_statistics` (Optional, boolean)*: If `true`, the response includes the document count, sum of document frequencies, and sum of total term frequencies.
** *`offsets` (Optional, boolean)*: If `true`, the response includes term offsets.
** *`payloads` (Optional, boolean)*: If `true`, the response includes term payloads.
** *`positions` (Optional, boolean)*: If `true`, the response includes term positions.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`realtime` (Optional, boolean)*: If true, the request is real-time as opposed to near-real-time.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`term_statistics` (Optional, boolean)*: If `true`, the response includes term frequency and document frequency.
** *`version` (Optional, number)*: If `true`, returns the document version as part of a hit.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type.

[discrete]
=== update
Update a document.
Updates a document by running a script or passing a partial document.

{ref}/docs-update.html[Endpoint documentation]
[source,ts]
----
client.update({ id, index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Document ID
** *`index` (string)*: The name of the index
** *`detect_noop` (Optional, boolean)*: Set to false to disable setting 'result' in the response
to 'noop' if no change to the document occurred.
** *`doc` (Optional, object)*: A partial update to an existing document.
** *`doc_as_upsert` (Optional, boolean)*: Set to true to use the contents of 'doc' as the value of 'upsert'
** *`script` (Optional, { source, id, params, lang, options })*: Script to execute to update the document.
** *`scripted_upsert` (Optional, boolean)*: Set to true to execute the script whether or not the document exists.
** *`_source` (Optional, boolean | { excludes, includes })*: Set to false to disable source retrieval. You can also specify a comma-separated
list of the fields you want to retrieve.
** *`upsert` (Optional, object)*: If the document does not already exist, the contents of 'upsert' are inserted as a
new document. If the document exists, the 'script' is executed.
** *`if_primary_term` (Optional, number)*: Only perform the operation if the document has this primary term.
** *`if_seq_no` (Optional, number)*: Only perform the operation if the document has this sequence number.
** *`lang` (Optional, string)*: The script language.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If 'true', Elasticsearch refreshes the affected shards to make this operation
visible to search, if 'wait_for' then wait for a refresh to make this operation
visible to search, if 'false' do nothing with refreshes.
** *`require_alias` (Optional, boolean)*: If true, the destination must be an index alias.
** *`retry_on_conflict` (Optional, number)*: Specify how many times should the operation be retried when a conflict occurs.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for dynamic mapping updates and active shards.
This guarantees Elasticsearch waits for at least the timeout before failing.
The actual wait time could be longer, particularly when multiple waits occur.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operations.
Set to 'all' or any positive integer up to the total number of shards in the index
(number_of_replicas+1). Defaults to 1 meaning the primary shard.
** *`_source_excludes` (Optional, string | string[])*: Specify the source fields you want to exclude.
** *`_source_includes` (Optional, string | string[])*: Specify the source fields you want to retrieve.

[discrete]
=== update_by_query
Update documents.
Updates documents that match the specified query.
If no query is specified, performs an update on every document in the data stream or index without modifying the source, which is useful for picking up mapping changes.

{ref}/docs-update-by-query.html[Endpoint documentation]
[source,ts]
----
client.updateByQuery({ index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of data streams, indices, and aliases to search.
Supports wildcards (`*`).
To search all data streams or indices, omit this parameter or use `*` or `_all`.
** *`max_docs` (Optional, number)*: The maximum number of documents to update.
** *`query` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Specifies the documents to update using the Query DSL.
** *`script` (Optional, { source, id, params, lang, options })*: The script to run to update the document source or metadata when updating.
** *`slice` (Optional, { field, id, max })*: Slice the request manually using the provided slice ID and total number of slices.
** *`conflicts` (Optional, Enum("abort" | "proceed"))*: What to do if update by query hits version conflicts: `abort` or `proceed`.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
For example, a request targeting `foo*,bar*` returns an error if an index starts with `foo` but no index starts with `bar`.
** *`analyzer` (Optional, string)*: Analyzer to use for the query string.
** *`analyze_wildcard` (Optional, boolean)*: If `true`, wildcard and prefix queries are analyzed.
** *`default_operator` (Optional, Enum("and" | "or"))*: The default operator for query string query: `AND` or `OR`.
** *`df` (Optional, string)*: Field to use as default where no field prefix is given in the query string.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`from` (Optional, number)*: Starting offset (default: 0)
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`lenient` (Optional, boolean)*: If `true`, format-based query failures (such as providing text to a numeric field) in the query string will be ignored.
** *`pipeline` (Optional, string)*: ID of the pipeline to use to preprocess incoming documents.
If the index has a default ingest pipeline specified, then setting the value to `_none` disables the default ingest pipeline for this request.
If a final pipeline is configured it will always run, regardless of the value of this parameter.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`q` (Optional, string)*: Query in the Lucene query string syntax.
** *`refresh` (Optional, boolean)*: If `true`, Elasticsearch refreshes affected shards to make the operation visible to search.
** *`request_cache` (Optional, boolean)*: If `true`, the request cache is used for this request.
** *`requests_per_second` (Optional, float)*: The throttle for this request in sub-requests per second.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`scroll` (Optional, string | -1 | 0)*: Period to retain the search context for scrolling.
** *`scroll_size` (Optional, number)*: Size of the scroll request that powers the operation.
** *`search_timeout` (Optional, string | -1 | 0)*: Explicit timeout for each search request.
** *`search_type` (Optional, Enum("query_then_fetch" | "dfs_query_then_fetch"))*: The type of the search operation. Available options: `query_then_fetch`, `dfs_query_then_fetch`.
** *`slices` (Optional, number | Enum("auto"))*: The number of slices this task should be divided into.
** *`sort` (Optional, string[])*: A list of <field>:<direction> pairs.
** *`stats` (Optional, string[])*: Specific `tag` of the request for logging and statistical purposes.
** *`terminate_after` (Optional, number)*: Maximum number of documents to collect for each shard.
If a query reaches this limit, Elasticsearch terminates the query early.
Elasticsearch collects documents before sorting.
Use with caution.
Elasticsearch applies this parameter to each shard handling the request.
When possible, let Elasticsearch perform early termination automatically.
Avoid specifying this parameter for requests that target data streams with backing indices across multiple data tiers.
** *`timeout` (Optional, string | -1 | 0)*: Period each update request waits for the following operations: dynamic mapping updates, waiting for active shards.
** *`version` (Optional, boolean)*: If `true`, returns the document version as part of a hit.
** *`version_type` (Optional, boolean)*: Should the document increment the version number (internal) on hit or not (reindex)
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to `all` or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).
** *`wait_for_completion` (Optional, boolean)*: If `true`, the request blocks until the operation is complete.

[discrete]
=== update_by_query_rethrottle
Throttle an update by query operation.

Change the number of requests per second for a particular update by query operation.
Rethrottling that speeds up the query takes effect immediately but rethrotting that slows down the query takes effect after completing the current batch to prevent scroll timeouts.

{ref}/docs-update-by-query.html[Endpoint documentation]
[source,ts]
----
client.updateByQueryRethrottle({ task_id })
----
[discrete]
==== Arguments

* *Request (object):*
** *`task_id` (string)*: The ID for the task.
** *`requests_per_second` (Optional, float)*: The throttle for this request in sub-requests per second.

[discrete]
=== async_search
[discrete]
==== delete
Delete an async search.

If the asynchronous search is still running, it is cancelled.
Otherwise, the saved search results are deleted.
If the Elasticsearch security features are enabled, the deletion of a specific async search is restricted to: the authenticated user that submitted the original search request; users that have the `cancel_task` cluster privilege.

{ref}/async-search.html[Endpoint documentation]
[source,ts]
----
client.asyncSearch.delete({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: A unique identifier for the async search.

[discrete]
==== get
Get async search results.

Retrieve the results of a previously submitted asynchronous search request.
If the Elasticsearch security features are enabled, access to the results of a specific async search is restricted to the user or API key that submitted it.

{ref}/async-search.html[Endpoint documentation]
[source,ts]
----
client.asyncSearch.get({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: A unique identifier for the async search.
** *`keep_alive` (Optional, string | -1 | 0)*: Specifies how long the async search should be available in the cluster.
When not specified, the `keep_alive` set with the corresponding submit async request will be used.
Otherwise, it is possible to override the value and extend the validity of the request.
When this period expires, the search, if still running, is cancelled.
If the search is completed, its saved results are deleted.
** *`typed_keys` (Optional, boolean)*: Specify whether aggregation and suggester names should be prefixed by their respective types in the response
** *`wait_for_completion_timeout` (Optional, string | -1 | 0)*: Specifies to wait for the search to be completed up until the provided timeout.
Final results will be returned if available before the timeout expires, otherwise the currently available results will be returned once the timeout expires.
By default no timeout is set meaning that the currently available results will be returned without any additional wait.

[discrete]
==== status
Get the async search status.

Get the status of a previously submitted async search request given its identifier, without retrieving search results.
If the Elasticsearch security features are enabled, use of this API is restricted to the `monitoring_user` role.

{ref}/async-search.html[Endpoint documentation]
[source,ts]
----
client.asyncSearch.status({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: A unique identifier for the async search.

[discrete]
==== submit
Run an async search.

When the primary sort of the results is an indexed field, shards get sorted based on minimum and maximum value that they hold for that field. Partial results become available following the sort criteria that was requested.

Warning: Asynchronous search does not support scroll or search requests that include only the suggest section.

By default, Elasticsearch does not allow you to store an async search response larger than 10Mb and an attempt to do this results in an error.
The maximum allowed size for a stored async search response can be set by changing the `search.max_async_search_response_size` cluster level setting.

{ref}/async-search.html[Endpoint documentation]
[source,ts]
----
client.asyncSearch.submit({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: A list of index names to search; use `_all` or empty string to perform the operation on all indices
** *`aggregations` (Optional, Record<string, { aggregations, meta, adjacency_matrix, auto_date_histogram, avg, avg_bucket, boxplot, bucket_script, bucket_selector, bucket_sort, bucket_count_ks_test, bucket_correlation, cardinality, categorize_text, children, composite, cumulative_cardinality, cumulative_sum, date_histogram, date_range, derivative, diversified_sampler, extended_stats, extended_stats_bucket, frequent_item_sets, filter, filters, geo_bounds, geo_centroid, geo_distance, geohash_grid, geo_line, geotile_grid, geohex_grid, global, histogram, ip_range, ip_prefix, inference, line, matrix_stats, max, max_bucket, median_absolute_deviation, min, min_bucket, missing, moving_avg, moving_percentiles, moving_fn, multi_terms, nested, normalize, parent, percentile_ranks, percentiles, percentiles_bucket, range, rare_terms, rate, reverse_nested, random_sampler, sampler, scripted_metric, serial_diff, significant_terms, significant_text, stats, stats_bucket, string_stats, sum, sum_bucket, terms, time_series, top_hits, t_test, top_metrics, value_count, weighted_avg, variable_width_histogram }>)*
** *`collapse` (Optional, { field, inner_hits, max_concurrent_group_searches, collapse })*
** *`explain` (Optional, boolean)*: If true, returns detailed information about score computation as part of a hit.
** *`ext` (Optional, Record<string, User-defined value>)*: Configuration of search extensions defined by Elasticsearch plugins.
** *`from` (Optional, number)*: Starting document offset. By default, you cannot page through more than 10,000
hits using the from and size parameters. To page through more hits, use the
search_after parameter.
** *`highlight` (Optional, { encoder, fields })*
** *`track_total_hits` (Optional, boolean | number)*: Number of hits matching the query to count accurately. If true, the exact
number of hits is returned at the cost of some performance. If false, the
response does not include the total number of hits matching the query.
Defaults to 10,000 hits.
** *`indices_boost` (Optional, Record<string, number>[])*: Boosts the _score of documents from specified indices.
** *`docvalue_fields` (Optional, { field, format, include_unmapped }[])*: Array of wildcard (*) patterns. The request returns doc values for field
names matching these patterns in the hits.fields property of the response.
** *`knn` (Optional, { field, query_vector, query_vector_builder, k, num_candidates, boost, filter, similarity, inner_hits } | { field, query_vector, query_vector_builder, k, num_candidates, boost, filter, similarity, inner_hits }[])*: Defines the approximate kNN search to run.
** *`min_score` (Optional, number)*: Minimum _score for matching documents. Documents with a lower _score are
not included in the search results.
** *`post_filter` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*
** *`profile` (Optional, boolean)*
** *`query` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Defines the search definition using the Query DSL.
** *`rescore` (Optional, { window_size, query, learning_to_rank } | { window_size, query, learning_to_rank }[])*
** *`script_fields` (Optional, Record<string, { script, ignore_failure }>)*: Retrieve a script evaluation (based on different fields) for each hit.
** *`search_after` (Optional, number | number | string | boolean | null | User-defined value[])*
** *`size` (Optional, number)*: The number of hits to return. By default, you cannot page through more
than 10,000 hits using the from and size parameters. To page through more
hits, use the search_after parameter.
** *`slice` (Optional, { field, id, max })*
** *`sort` (Optional, string | { _score, _doc, _geo_distance, _script } | string | { _score, _doc, _geo_distance, _script }[])*
** *`_source` (Optional, boolean | { excludes, includes })*: Indicates which source fields are returned for matching documents. These
fields are returned in the hits._source property of the search response.
** *`fields` (Optional, { field, format, include_unmapped }[])*: Array of wildcard (*) patterns. The request returns values for field names
matching these patterns in the hits.fields property of the response.
** *`suggest` (Optional, { text })*
** *`terminate_after` (Optional, number)*: Maximum number of documents to collect for each shard. If a query reaches this
limit, Elasticsearch terminates the query early. Elasticsearch collects documents
before sorting. Defaults to 0, which does not terminate query execution early.
** *`timeout` (Optional, string)*: Specifies the period of time to wait for a response from each shard. If no response
is received before the timeout expires, the request fails and returns an error.
Defaults to no timeout.
** *`track_scores` (Optional, boolean)*: If true, calculate and return document scores, even if the scores are not used for sorting.
** *`version` (Optional, boolean)*: If true, returns document version as part of a hit.
** *`seq_no_primary_term` (Optional, boolean)*: If true, returns sequence number and primary term of the last modification
of each hit. See Optimistic concurrency control.
** *`stored_fields` (Optional, string | string[])*: List of stored fields to return as part of a hit. If no fields are specified,
no stored fields are included in the response. If this field is specified, the _source
parameter defaults to false. You can pass _source: true to return both source fields
and stored fields in the search response.
** *`pit` (Optional, { id, keep_alive })*: Limits the search to a point in time (PIT). If you provide a PIT, you
cannot specify an <index> in the request path.
** *`runtime_mappings` (Optional, Record<string, { fields, fetch_fields, format, input_field, target_field, target_index, script, type }>)*: Defines one or more runtime fields in the search request. These fields take
precedence over mapped fields with the same name.
** *`stats` (Optional, string[])*: Stats groups to associate with the search. Each group maintains a statistics
aggregation for its associated searches. You can retrieve these stats using
the indices stats API.
** *`wait_for_completion_timeout` (Optional, string | -1 | 0)*: Blocks and waits until the search is completed up to a certain timeout.
When the async search completes within the timeout, the response won’t include the ID as the results are not stored in the cluster.
** *`keep_on_completion` (Optional, boolean)*: If `true`, results are stored for later retrieval when the search completes within the `wait_for_completion_timeout`.
** *`keep_alive` (Optional, string | -1 | 0)*: Specifies how long the async search needs to be available.
Ongoing async searches and any saved search results are deleted after this period.
** *`allow_no_indices` (Optional, boolean)*: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes `_all` string or when no indices have been specified)
** *`allow_partial_search_results` (Optional, boolean)*: Indicate if an error should be returned if there is a partial search failure or timeout
** *`analyzer` (Optional, string)*: The analyzer to use for the query string
** *`analyze_wildcard` (Optional, boolean)*: Specify whether wildcard and prefix queries should be analyzed (default: false)
** *`batched_reduce_size` (Optional, number)*: Affects how often partial results become available, which happens whenever shard results are reduced.
A partial reduction is performed every time the coordinating node has received a certain number of new shard responses (5 by default).
** *`ccs_minimize_roundtrips` (Optional, boolean)*: The default value is the only supported value.
** *`default_operator` (Optional, Enum("and" | "or"))*: The default operator for query string query (AND or OR)
** *`df` (Optional, string)*: The field to use as default where no field prefix is given in the query string
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Whether to expand wildcard expression to concrete indices that are open, closed or both.
** *`ignore_throttled` (Optional, boolean)*: Whether specified concrete, expanded or aliased indices should be ignored when throttled
** *`ignore_unavailable` (Optional, boolean)*: Whether specified concrete indices should be ignored when unavailable (missing or closed)
** *`lenient` (Optional, boolean)*: Specify whether format-based query failures (such as providing text to a numeric field) should be ignored
** *`max_concurrent_shard_requests` (Optional, number)*: The number of concurrent shard requests per node this search executes concurrently. This value should be used to limit the impact of the search on the cluster in order to limit the number of concurrent shard requests
** *`min_compatible_shard_node` (Optional, string)*
** *`preference` (Optional, string)*: Specify the node or shard the operation should be performed on (default: random)
** *`pre_filter_shard_size` (Optional, number)*: The default value cannot be changed, which enforces the execution of a pre-filter roundtrip to retrieve statistics from each shard so that the ones that surely don’t hold any document matching the query get skipped.
** *`request_cache` (Optional, boolean)*: Specify if request cache should be used for this request or not, defaults to true
** *`routing` (Optional, string)*: A list of specific routing values
** *`scroll` (Optional, string | -1 | 0)*
** *`search_type` (Optional, Enum("query_then_fetch" | "dfs_query_then_fetch"))*: Search operation type
** *`suggest_field` (Optional, string)*: Specifies which field to use for suggestions.
** *`suggest_mode` (Optional, Enum("missing" | "popular" | "always"))*: Specify suggest mode
** *`suggest_size` (Optional, number)*: How many suggestions to return in response
** *`suggest_text` (Optional, string)*: The source text for which the suggestions should be returned.
** *`typed_keys` (Optional, boolean)*: Specify whether aggregation and suggester names should be prefixed by their respective types in the response
** *`rest_total_hits_as_int` (Optional, boolean)*
** *`_source_excludes` (Optional, string | string[])*: A list of fields to exclude from the returned _source field
** *`_source_includes` (Optional, string | string[])*: A list of fields to extract and return from the _source field
** *`q` (Optional, string)*: Query in the Lucene query string syntax

[discrete]
=== autoscaling
[discrete]
==== delete_autoscaling_policy
Delete an autoscaling policy.

NOTE: This feature is designed for indirect use by Elasticsearch Service, Elastic Cloud Enterprise, and Elastic Cloud on Kubernetes. Direct use is not supported.

{ref}/autoscaling-delete-autoscaling-policy.html[Endpoint documentation]
[source,ts]
----
client.autoscaling.deleteAutoscalingPolicy({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: the name of the autoscaling policy

[discrete]
==== get_autoscaling_capacity
Get the autoscaling capacity.

NOTE: This feature is designed for indirect use by Elasticsearch Service, Elastic Cloud Enterprise, and Elastic Cloud on Kubernetes. Direct use is not supported.

This API gets the current autoscaling capacity based on the configured autoscaling policy.
It will return information to size the cluster appropriately to the current workload.

The `required_capacity` is calculated as the maximum of the `required_capacity` result of all individual deciders that are enabled for the policy.

The operator should verify that the `current_nodes` match the operator’s knowledge of the cluster to avoid making autoscaling decisions based on stale or incomplete information.

The response contains decider-specific information you can use to diagnose how and why autoscaling determined a certain capacity was required.
This information is provided for diagnosis only.
Do not use this information to make autoscaling decisions.

{ref}/autoscaling-get-autoscaling-capacity.html[Endpoint documentation]
[source,ts]
----
client.autoscaling.getAutoscalingCapacity()
----


[discrete]
==== get_autoscaling_policy
Get an autoscaling policy.

NOTE: This feature is designed for indirect use by Elasticsearch Service, Elastic Cloud Enterprise, and Elastic Cloud on Kubernetes. Direct use is not supported.

{ref}/autoscaling-get-autoscaling-capacity.html[Endpoint documentation]
[source,ts]
----
client.autoscaling.getAutoscalingPolicy({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: the name of the autoscaling policy

[discrete]
==== put_autoscaling_policy
Create or update an autoscaling policy.

NOTE: This feature is designed for indirect use by Elasticsearch Service, Elastic Cloud Enterprise, and Elastic Cloud on Kubernetes. Direct use is not supported.

{ref}/autoscaling-put-autoscaling-policy.html[Endpoint documentation]
[source,ts]
----
client.autoscaling.putAutoscalingPolicy({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: the name of the autoscaling policy
** *`policy` (Optional, { roles, deciders })*

[discrete]
=== cat
[discrete]
==== aliases
Get aliases.
Retrieves the cluster’s index aliases, including filter and routing information.
The API does not return data stream aliases.

CAT APIs are only intended for human consumption using the command line or the Kibana console. They are not intended for use by applications. For application consumption, use the aliases API.

{ref}/cat-alias.html[Endpoint documentation]
[source,ts]
----
client.cat.aliases({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string | string[])*: A list of aliases to retrieve. Supports wildcards (`*`).  To retrieve all aliases, omit this parameter or use `*` or `_all`.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Whether to expand wildcard expression to concrete indices that are open, closed or both.

[discrete]
==== allocation
Provides a snapshot of the number of shards allocated to each data node and their disk space.
IMPORTANT: cat APIs are only intended for human consumption using the command line or Kibana console. They are not intended for use by applications.

{ref}/cat-allocation.html[Endpoint documentation]
[source,ts]
----
client.cat.allocation({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`node_id` (Optional, string | string[])*: List of node identifiers or names used to limit the returned information.
** *`bytes` (Optional, Enum("b" | "kb" | "mb" | "gb" | "tb" | "pb"))*: The unit used to display byte values.

[discrete]
==== component_templates
Get component templates.
Returns information about component templates in a cluster.
Component templates are building blocks for constructing index templates that specify index mappings, settings, and aliases.

CAT APIs are only intended for human consumption using the command line or Kibana console.
They are not intended for use by applications. For application consumption, use the get component template API.

{ref}/cat-component-templates.html[Endpoint documentation]
[source,ts]
----
client.cat.componentTemplates({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string)*: The name of the component template. Accepts wildcard expressions. If omitted, all component templates are returned.

[discrete]
==== count
Get a document count.
Provides quick access to a document count for a data stream, an index, or an entire cluster.
The document count only includes live documents, not deleted documents which have not yet been removed by the merge process.

CAT APIs are only intended for human consumption using the command line or Kibana console.
They are not intended for use by applications. For application consumption, use the count API.

{ref}/cat-count.html[Endpoint documentation]
[source,ts]
----
client.cat.count({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit the request.
Supports wildcards (`*`). To target all data streams and indices, omit this parameter or use `*` or `_all`.

[discrete]
==== fielddata
Returns the amount of heap memory currently used by the field data cache on every data node in the cluster.
IMPORTANT: cat APIs are only intended for human consumption using the command line or Kibana console.
They are not intended for use by applications. For application consumption, use the nodes stats API.

{ref}/cat-fielddata.html[Endpoint documentation]
[source,ts]
----
client.cat.fielddata({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`fields` (Optional, string | string[])*: List of fields used to limit returned information.
To retrieve all fields, omit this parameter.
** *`bytes` (Optional, Enum("b" | "kb" | "mb" | "gb" | "tb" | "pb"))*: The unit used to display byte values.

[discrete]
==== health
Returns the health status of a cluster, similar to the cluster health API.
IMPORTANT: cat APIs are only intended for human consumption using the command line or Kibana console.
They are not intended for use by applications. For application consumption, use the cluster health API.
This API is often used to check malfunctioning clusters.
To help you track cluster health alongside log files and alerting systems, the API returns timestamps in two formats:
`HH:MM:SS`, which is human-readable but includes no date information;
`Unix epoch time`, which is machine-sortable and includes date information.
The latter format is useful for cluster recoveries that take multiple days.
You can use the cat health API to verify cluster health across multiple nodes.
You also can use the API to track the recovery of a large cluster over a longer period of time.

{ref}/cat-health.html[Endpoint documentation]
[source,ts]
----
client.cat.health({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`time` (Optional, Enum("nanos" | "micros" | "ms" | "s" | "m" | "h" | "d"))*: The unit used to display time values.
** *`ts` (Optional, boolean)*: If true, returns `HH:MM:SS` and Unix epoch timestamps.

[discrete]
==== help
Get CAT help.
Returns help for the CAT APIs.

{ref}/cat.html[Endpoint documentation]
[source,ts]
----
client.cat.help()
----


[discrete]
==== indices
Get index information.
Returns high-level information about indices in a cluster, including backing indices for data streams.

Use this request to get the following information for each index in a cluster:
- shard count
- document count
- deleted document count
- primary store size
- total store size of all shards, including shard replicas

These metrics are retrieved directly from Lucene, which Elasticsearch uses internally to power indexing and search. As a result, all document counts include hidden nested documents.
To get an accurate count of Elasticsearch documents, use the cat count or count APIs.

CAT APIs are only intended for human consumption using the command line or Kibana console.
They are not intended for use by applications. For application consumption, use an index endpoint.

{ref}/cat-indices.html[Endpoint documentation]
[source,ts]
----
client.cat.indices({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit the request.
Supports wildcards (`*`). To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`bytes` (Optional, Enum("b" | "kb" | "mb" | "gb" | "tb" | "pb"))*: The unit used to display byte values.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: The type of index that wildcard patterns can match.
** *`health` (Optional, Enum("green" | "yellow" | "red"))*: The health status used to limit returned indices. By default, the response includes indices of any health status.
** *`include_unloaded_segments` (Optional, boolean)*: If true, the response includes information from segments that are not loaded into memory.
** *`pri` (Optional, boolean)*: If true, the response only includes information from primary shards.
** *`time` (Optional, Enum("nanos" | "micros" | "ms" | "s" | "m" | "h" | "d"))*: The unit used to display time values.

[discrete]
==== master
Returns information about the master node, including the ID, bound IP address, and name.
IMPORTANT: cat APIs are only intended for human consumption using the command line or Kibana console. They are not intended for use by applications. For application consumption, use the nodes info API.

{ref}/cat-master.html[Endpoint documentation]
[source,ts]
----
client.cat.master()
----


[discrete]
==== ml_data_frame_analytics
Get data frame analytics jobs.
Returns configuration and usage information about data frame analytics jobs.

CAT APIs are only intended for human consumption using the Kibana
console or command line. They are not intended for use by applications. For
application consumption, use the get data frame analytics jobs statistics API.

{ref}/cat-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.cat.mlDataFrameAnalytics({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: The ID of the data frame analytics to fetch
** *`allow_no_match` (Optional, boolean)*: Whether to ignore if a wildcard expression matches no configs. (This includes `_all` string or when no configs have been specified)
** *`bytes` (Optional, Enum("b" | "kb" | "mb" | "gb" | "tb" | "pb"))*: The unit in which to display byte values
** *`h` (Optional, Enum("assignment_explanation" | "create_time" | "description" | "dest_index" | "failure_reason" | "id" | "model_memory_limit" | "node.address" | "node.ephemeral_id" | "node.id" | "node.name" | "progress" | "source_index" | "state" | "type" | "version") | Enum("assignment_explanation" | "create_time" | "description" | "dest_index" | "failure_reason" | "id" | "model_memory_limit" | "node.address" | "node.ephemeral_id" | "node.id" | "node.name" | "progress" | "source_index" | "state" | "type" | "version")[])*: List of column names to display.
** *`s` (Optional, Enum("assignment_explanation" | "create_time" | "description" | "dest_index" | "failure_reason" | "id" | "model_memory_limit" | "node.address" | "node.ephemeral_id" | "node.id" | "node.name" | "progress" | "source_index" | "state" | "type" | "version") | Enum("assignment_explanation" | "create_time" | "description" | "dest_index" | "failure_reason" | "id" | "model_memory_limit" | "node.address" | "node.ephemeral_id" | "node.id" | "node.name" | "progress" | "source_index" | "state" | "type" | "version")[])*: List of column names or column aliases used to sort the
response.
** *`time` (Optional, string | -1 | 0)*: Unit used to display time values.

[discrete]
==== ml_datafeeds
Get datafeeds.
Returns configuration and usage information about datafeeds.
This API returns a maximum of 10,000 datafeeds.
If the Elasticsearch security features are enabled, you must have `monitor_ml`, `monitor`, `manage_ml`, or `manage`
cluster privileges to use this API.

CAT APIs are only intended for human consumption using the Kibana
console or command line. They are not intended for use by applications. For
application consumption, use the get datafeed statistics API.

{ref}/cat-datafeeds.html[Endpoint documentation]
[source,ts]
----
client.cat.mlDatafeeds({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (Optional, string)*: A numerical character string that uniquely identifies the datafeed.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

* Contains wildcard expressions and there are no datafeeds that match.
* Contains the `_all` string or no identifiers and there are no matches.
* Contains wildcard expressions and there are only partial matches.

If `true`, the API returns an empty datafeeds array when there are no matches and the subset of results when
there are partial matches. If `false`, the API returns a 404 status code when there are no matches or only
partial matches.
** *`h` (Optional, Enum("ae" | "bc" | "id" | "na" | "ne" | "ni" | "nn" | "sba" | "sc" | "seah" | "st" | "s") | Enum("ae" | "bc" | "id" | "na" | "ne" | "ni" | "nn" | "sba" | "sc" | "seah" | "st" | "s")[])*: List of column names to display.
** *`s` (Optional, Enum("ae" | "bc" | "id" | "na" | "ne" | "ni" | "nn" | "sba" | "sc" | "seah" | "st" | "s") | Enum("ae" | "bc" | "id" | "na" | "ne" | "ni" | "nn" | "sba" | "sc" | "seah" | "st" | "s")[])*: List of column names or column aliases used to sort the response.
** *`time` (Optional, Enum("nanos" | "micros" | "ms" | "s" | "m" | "h" | "d"))*: The unit used to display time values.

[discrete]
==== ml_jobs
Get anomaly detection jobs.
Returns configuration and usage information for anomaly detection jobs.
This API returns a maximum of 10,000 jobs.
If the Elasticsearch security features are enabled, you must have `monitor_ml`,
`monitor`, `manage_ml`, or `manage` cluster privileges to use this API.

CAT APIs are only intended for human consumption using the Kibana
console or command line. They are not intended for use by applications. For
application consumption, use the get anomaly detection job statistics API.

{ref}/cat-anomaly-detectors.html[Endpoint documentation]
[source,ts]
----
client.cat.mlJobs({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (Optional, string)*: Identifier for the anomaly detection job.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

* Contains wildcard expressions and there are no jobs that match.
* Contains the `_all` string or no identifiers and there are no matches.
* Contains wildcard expressions and there are only partial matches.

If `true`, the API returns an empty jobs array when there are no matches and the subset of results when there
are partial matches. If `false`, the API returns a 404 status code when there are no matches or only partial
matches.
** *`bytes` (Optional, Enum("b" | "kb" | "mb" | "gb" | "tb" | "pb"))*: The unit used to display byte values.
** *`h` (Optional, Enum("assignment_explanation" | "buckets.count" | "buckets.time.exp_avg" | "buckets.time.exp_avg_hour" | "buckets.time.max" | "buckets.time.min" | "buckets.time.total" | "data.buckets" | "data.earliest_record" | "data.empty_buckets" | "data.input_bytes" | "data.input_fields" | "data.input_records" | "data.invalid_dates" | "data.last" | "data.last_empty_bucket" | "data.last_sparse_bucket" | "data.latest_record" | "data.missing_fields" | "data.out_of_order_timestamps" | "data.processed_fields" | "data.processed_records" | "data.sparse_buckets" | "forecasts.memory.avg" | "forecasts.memory.max" | "forecasts.memory.min" | "forecasts.memory.total" | "forecasts.records.avg" | "forecasts.records.max" | "forecasts.records.min" | "forecasts.records.total" | "forecasts.time.avg" | "forecasts.time.max" | "forecasts.time.min" | "forecasts.time.total" | "forecasts.total" | "id" | "model.bucket_allocation_failures" | "model.by_fields" | "model.bytes" | "model.bytes_exceeded" | "model.categorization_status" | "model.categorized_doc_count" | "model.dead_category_count" | "model.failed_category_count" | "model.frequent_category_count" | "model.log_time" | "model.memory_limit" | "model.memory_status" | "model.over_fields" | "model.partition_fields" | "model.rare_category_count" | "model.timestamp" | "model.total_category_count" | "node.address" | "node.ephemeral_id" | "node.id" | "node.name" | "opened_time" | "state") | Enum("assignment_explanation" | "buckets.count" | "buckets.time.exp_avg" | "buckets.time.exp_avg_hour" | "buckets.time.max" | "buckets.time.min" | "buckets.time.total" | "data.buckets" | "data.earliest_record" | "data.empty_buckets" | "data.input_bytes" | "data.input_fields" | "data.input_records" | "data.invalid_dates" | "data.last" | "data.last_empty_bucket" | "data.last_sparse_bucket" | "data.latest_record" | "data.missing_fields" | "data.out_of_order_timestamps" | "data.processed_fields" | "data.processed_records" | "data.sparse_buckets" | "forecasts.memory.avg" | "forecasts.memory.max" | "forecasts.memory.min" | "forecasts.memory.total" | "forecasts.records.avg" | "forecasts.records.max" | "forecasts.records.min" | "forecasts.records.total" | "forecasts.time.avg" | "forecasts.time.max" | "forecasts.time.min" | "forecasts.time.total" | "forecasts.total" | "id" | "model.bucket_allocation_failures" | "model.by_fields" | "model.bytes" | "model.bytes_exceeded" | "model.categorization_status" | "model.categorized_doc_count" | "model.dead_category_count" | "model.failed_category_count" | "model.frequent_category_count" | "model.log_time" | "model.memory_limit" | "model.memory_status" | "model.over_fields" | "model.partition_fields" | "model.rare_category_count" | "model.timestamp" | "model.total_category_count" | "node.address" | "node.ephemeral_id" | "node.id" | "node.name" | "opened_time" | "state")[])*: List of column names to display.
** *`s` (Optional, Enum("assignment_explanation" | "buckets.count" | "buckets.time.exp_avg" | "buckets.time.exp_avg_hour" | "buckets.time.max" | "buckets.time.min" | "buckets.time.total" | "data.buckets" | "data.earliest_record" | "data.empty_buckets" | "data.input_bytes" | "data.input_fields" | "data.input_records" | "data.invalid_dates" | "data.last" | "data.last_empty_bucket" | "data.last_sparse_bucket" | "data.latest_record" | "data.missing_fields" | "data.out_of_order_timestamps" | "data.processed_fields" | "data.processed_records" | "data.sparse_buckets" | "forecasts.memory.avg" | "forecasts.memory.max" | "forecasts.memory.min" | "forecasts.memory.total" | "forecasts.records.avg" | "forecasts.records.max" | "forecasts.records.min" | "forecasts.records.total" | "forecasts.time.avg" | "forecasts.time.max" | "forecasts.time.min" | "forecasts.time.total" | "forecasts.total" | "id" | "model.bucket_allocation_failures" | "model.by_fields" | "model.bytes" | "model.bytes_exceeded" | "model.categorization_status" | "model.categorized_doc_count" | "model.dead_category_count" | "model.failed_category_count" | "model.frequent_category_count" | "model.log_time" | "model.memory_limit" | "model.memory_status" | "model.over_fields" | "model.partition_fields" | "model.rare_category_count" | "model.timestamp" | "model.total_category_count" | "node.address" | "node.ephemeral_id" | "node.id" | "node.name" | "opened_time" | "state") | Enum("assignment_explanation" | "buckets.count" | "buckets.time.exp_avg" | "buckets.time.exp_avg_hour" | "buckets.time.max" | "buckets.time.min" | "buckets.time.total" | "data.buckets" | "data.earliest_record" | "data.empty_buckets" | "data.input_bytes" | "data.input_fields" | "data.input_records" | "data.invalid_dates" | "data.last" | "data.last_empty_bucket" | "data.last_sparse_bucket" | "data.latest_record" | "data.missing_fields" | "data.out_of_order_timestamps" | "data.processed_fields" | "data.processed_records" | "data.sparse_buckets" | "forecasts.memory.avg" | "forecasts.memory.max" | "forecasts.memory.min" | "forecasts.memory.total" | "forecasts.records.avg" | "forecasts.records.max" | "forecasts.records.min" | "forecasts.records.total" | "forecasts.time.avg" | "forecasts.time.max" | "forecasts.time.min" | "forecasts.time.total" | "forecasts.total" | "id" | "model.bucket_allocation_failures" | "model.by_fields" | "model.bytes" | "model.bytes_exceeded" | "model.categorization_status" | "model.categorized_doc_count" | "model.dead_category_count" | "model.failed_category_count" | "model.frequent_category_count" | "model.log_time" | "model.memory_limit" | "model.memory_status" | "model.over_fields" | "model.partition_fields" | "model.rare_category_count" | "model.timestamp" | "model.total_category_count" | "node.address" | "node.ephemeral_id" | "node.id" | "node.name" | "opened_time" | "state")[])*: List of column names or column aliases used to sort the response.
** *`time` (Optional, Enum("nanos" | "micros" | "ms" | "s" | "m" | "h" | "d"))*: The unit used to display time values.

[discrete]
==== ml_trained_models
Get trained models.
Returns configuration and usage information about inference trained models.

CAT APIs are only intended for human consumption using the Kibana
console or command line. They are not intended for use by applications. For
application consumption, use the get trained models statistics API.

{ref}/cat-trained-model.html[Endpoint documentation]
[source,ts]
----
client.cat.mlTrainedModels({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (Optional, string)*: A unique identifier for the trained model.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request: contains wildcard expressions and there are no models that match; contains the `_all` string or no identifiers and there are no matches; contains wildcard expressions and there are only partial matches.
If `true`, the API returns an empty array when there are no matches and the subset of results when there are partial matches.
If `false`, the API returns a 404 status code when there are no matches or only partial matches.
** *`bytes` (Optional, Enum("b" | "kb" | "mb" | "gb" | "tb" | "pb"))*: The unit used to display byte values.
** *`h` (Optional, Enum("create_time" | "created_by" | "data_frame_analytics_id" | "description" | "heap_size" | "id" | "ingest.count" | "ingest.current" | "ingest.failed" | "ingest.pipelines" | "ingest.time" | "license" | "operations" | "version") | Enum("create_time" | "created_by" | "data_frame_analytics_id" | "description" | "heap_size" | "id" | "ingest.count" | "ingest.current" | "ingest.failed" | "ingest.pipelines" | "ingest.time" | "license" | "operations" | "version")[])*: A list of column names to display.
** *`s` (Optional, Enum("create_time" | "created_by" | "data_frame_analytics_id" | "description" | "heap_size" | "id" | "ingest.count" | "ingest.current" | "ingest.failed" | "ingest.pipelines" | "ingest.time" | "license" | "operations" | "version") | Enum("create_time" | "created_by" | "data_frame_analytics_id" | "description" | "heap_size" | "id" | "ingest.count" | "ingest.current" | "ingest.failed" | "ingest.pipelines" | "ingest.time" | "license" | "operations" | "version")[])*: A list of column names or aliases used to sort the response.
** *`from` (Optional, number)*: Skips the specified number of transforms.
** *`size` (Optional, number)*: The maximum number of transforms to display.

[discrete]
==== nodeattrs
Returns information about custom node attributes.
IMPORTANT: cat APIs are only intended for human consumption using the command line or Kibana console. They are not intended for use by applications. For application consumption, use the nodes info API.

{ref}/cat-nodeattrs.html[Endpoint documentation]
[source,ts]
----
client.cat.nodeattrs()
----


[discrete]
==== nodes
Returns information about the nodes in a cluster.
IMPORTANT: cat APIs are only intended for human consumption using the command line or Kibana console. They are not intended for use by applications. For application consumption, use the nodes info API.

{ref}/cat-nodes.html[Endpoint documentation]
[source,ts]
----
client.cat.nodes({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`bytes` (Optional, Enum("b" | "kb" | "mb" | "gb" | "tb" | "pb"))*: The unit used to display byte values.
** *`full_id` (Optional, boolean | string)*: If `true`, return the full node ID. If `false`, return the shortened node ID.
** *`include_unloaded_segments` (Optional, boolean)*: If true, the response includes information from segments that are not loaded into memory.

[discrete]
==== pending_tasks
Returns cluster-level changes that have not yet been executed.
IMPORTANT: cat APIs are only intended for human consumption using the command line or Kibana console. They are not intended for use by applications. For application consumption, use the pending cluster tasks API.

{ref}/cat-pending-tasks.html[Endpoint documentation]
[source,ts]
----
client.cat.pendingTasks()
----


[discrete]
==== plugins
Returns a list of plugins running on each node of a cluster.
IMPORTANT: cat APIs are only intended for human consumption using the command line or Kibana console. They are not intended for use by applications. For application consumption, use the nodes info API.

{ref}/cat-plugins.html[Endpoint documentation]
[source,ts]
----
client.cat.plugins()
----


[discrete]
==== recovery
Returns information about ongoing and completed shard recoveries.
Shard recovery is the process of initializing a shard copy, such as restoring a primary shard from a snapshot or syncing a replica shard from a primary shard. When a shard recovery completes, the recovered shard is available for search and indexing.
For data streams, the API returns information about the stream’s backing indices.
IMPORTANT: cat APIs are only intended for human consumption using the command line or Kibana console. They are not intended for use by applications. For application consumption, use the index recovery API.

{ref}/cat-recovery.html[Endpoint documentation]
[source,ts]
----
client.cat.recovery({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: A list of data streams, indices, and aliases used to limit the request.
Supports wildcards (`*`). To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`active_only` (Optional, boolean)*: If `true`, the response only includes ongoing shard recoveries.
** *`bytes` (Optional, Enum("b" | "kb" | "mb" | "gb" | "tb" | "pb"))*: The unit used to display byte values.
** *`detailed` (Optional, boolean)*: If `true`, the response includes detailed information about shard recoveries.

[discrete]
==== repositories
Returns the snapshot repositories for a cluster.
IMPORTANT: cat APIs are only intended for human consumption using the command line or Kibana console. They are not intended for use by applications. For application consumption, use the get snapshot repository API.

{ref}/cat-repositories.html[Endpoint documentation]
[source,ts]
----
client.cat.repositories()
----


[discrete]
==== segments
Returns low-level information about the Lucene segments in index shards.
For data streams, the API returns information about the backing indices.
IMPORTANT: cat APIs are only intended for human consumption using the command line or Kibana console. They are not intended for use by applications. For application consumption, use the index segments API.

{ref}/cat-segments.html[Endpoint documentation]
[source,ts]
----
client.cat.segments({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: A list of data streams, indices, and aliases used to limit the request.
Supports wildcards (`*`).
To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`bytes` (Optional, Enum("b" | "kb" | "mb" | "gb" | "tb" | "pb"))*: The unit used to display byte values.

[discrete]
==== shards
Returns information about the shards in a cluster.
For data streams, the API returns information about the backing indices.
IMPORTANT: cat APIs are only intended for human consumption using the command line or Kibana console. They are not intended for use by applications.

{ref}/cat-shards.html[Endpoint documentation]
[source,ts]
----
client.cat.shards({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: A list of data streams, indices, and aliases used to limit the request.
Supports wildcards (`*`).
To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`bytes` (Optional, Enum("b" | "kb" | "mb" | "gb" | "tb" | "pb"))*: The unit used to display byte values.

[discrete]
==== snapshots
Returns information about the snapshots stored in one or more repositories.
A snapshot is a backup of an index or running Elasticsearch cluster.
IMPORTANT: cat APIs are only intended for human consumption using the command line or Kibana console. They are not intended for use by applications. For application consumption, use the get snapshot API.

{ref}/cat-snapshots.html[Endpoint documentation]
[source,ts]
----
client.cat.snapshots({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`repository` (Optional, string | string[])*: A list of snapshot repositories used to limit the request.
Accepts wildcard expressions.
`_all` returns all repositories.
If any repository fails during the request, Elasticsearch returns an error.
** *`ignore_unavailable` (Optional, boolean)*: If `true`, the response does not include information from unavailable snapshots.

[discrete]
==== tasks
Returns information about tasks currently executing in the cluster.
IMPORTANT: cat APIs are only intended for human consumption using the command line or Kibana console. They are not intended for use by applications. For application consumption, use the task management API.

{ref}/tasks.html[Endpoint documentation]
[source,ts]
----
client.cat.tasks({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`actions` (Optional, string[])*: The task action names, which are used to limit the response.
** *`detailed` (Optional, boolean)*: If `true`, the response includes detailed information about shard recoveries.
** *`node_id` (Optional, string[])*: Unique node identifiers, which are used to limit the response.
** *`parent_task_id` (Optional, string)*: The parent task identifier, which is used to limit the response.

[discrete]
==== templates
Returns information about index templates in a cluster.
You can use index templates to apply index settings and field mappings to new indices at creation.
IMPORTANT: cat APIs are only intended for human consumption using the command line or Kibana console. They are not intended for use by applications. For application consumption, use the get index template API.

{ref}/cat-templates.html[Endpoint documentation]
[source,ts]
----
client.cat.templates({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string)*: The name of the template to return.
Accepts wildcard expressions. If omitted, all templates are returned.

[discrete]
==== thread_pool
Returns thread pool statistics for each node in a cluster.
Returned information includes all built-in thread pools and custom thread pools.
IMPORTANT: cat APIs are only intended for human consumption using the command line or Kibana console. They are not intended for use by applications. For application consumption, use the nodes info API.

{ref}/cat-thread-pool.html[Endpoint documentation]
[source,ts]
----
client.cat.threadPool({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`thread_pool_patterns` (Optional, string | string[])*: A list of thread pool names used to limit the request.
Accepts wildcard expressions.
** *`time` (Optional, Enum("nanos" | "micros" | "ms" | "s" | "m" | "h" | "d"))*: The unit used to display time values.

[discrete]
==== transforms
Get transforms.
Returns configuration and usage information about transforms.

CAT APIs are only intended for human consumption using the Kibana
console or command line. They are not intended for use by applications. For
application consumption, use the get transform statistics API.

{ref}/cat-transforms.html[Endpoint documentation]
[source,ts]
----
client.cat.transforms({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (Optional, string)*: A transform identifier or a wildcard expression.
If you do not specify one of these options, the API returns information for all transforms.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request: contains wildcard expressions and there are no transforms that match; contains the `_all` string or no identifiers and there are no matches; contains wildcard expressions and there are only partial matches.
If `true`, it returns an empty transforms array when there are no matches and the subset of results when there are partial matches.
If `false`, the request returns a 404 status code when there are no matches or only partial matches.
** *`from` (Optional, number)*: Skips the specified number of transforms.
** *`h` (Optional, Enum("changes_last_detection_time" | "checkpoint" | "checkpoint_duration_time_exp_avg" | "checkpoint_progress" | "create_time" | "delete_time" | "description" | "dest_index" | "documents_deleted" | "documents_indexed" | "docs_per_second" | "documents_processed" | "frequency" | "id" | "index_failure" | "index_time" | "index_total" | "indexed_documents_exp_avg" | "last_search_time" | "max_page_search_size" | "pages_processed" | "pipeline" | "processed_documents_exp_avg" | "processing_time" | "reason" | "search_failure" | "search_time" | "search_total" | "source_index" | "state" | "transform_type" | "trigger_count" | "version") | Enum("changes_last_detection_time" | "checkpoint" | "checkpoint_duration_time_exp_avg" | "checkpoint_progress" | "create_time" | "delete_time" | "description" | "dest_index" | "documents_deleted" | "documents_indexed" | "docs_per_second" | "documents_processed" | "frequency" | "id" | "index_failure" | "index_time" | "index_total" | "indexed_documents_exp_avg" | "last_search_time" | "max_page_search_size" | "pages_processed" | "pipeline" | "processed_documents_exp_avg" | "processing_time" | "reason" | "search_failure" | "search_time" | "search_total" | "source_index" | "state" | "transform_type" | "trigger_count" | "version")[])*: List of column names to display.
** *`s` (Optional, Enum("changes_last_detection_time" | "checkpoint" | "checkpoint_duration_time_exp_avg" | "checkpoint_progress" | "create_time" | "delete_time" | "description" | "dest_index" | "documents_deleted" | "documents_indexed" | "docs_per_second" | "documents_processed" | "frequency" | "id" | "index_failure" | "index_time" | "index_total" | "indexed_documents_exp_avg" | "last_search_time" | "max_page_search_size" | "pages_processed" | "pipeline" | "processed_documents_exp_avg" | "processing_time" | "reason" | "search_failure" | "search_time" | "search_total" | "source_index" | "state" | "transform_type" | "trigger_count" | "version") | Enum("changes_last_detection_time" | "checkpoint" | "checkpoint_duration_time_exp_avg" | "checkpoint_progress" | "create_time" | "delete_time" | "description" | "dest_index" | "documents_deleted" | "documents_indexed" | "docs_per_second" | "documents_processed" | "frequency" | "id" | "index_failure" | "index_time" | "index_total" | "indexed_documents_exp_avg" | "last_search_time" | "max_page_search_size" | "pages_processed" | "pipeline" | "processed_documents_exp_avg" | "processing_time" | "reason" | "search_failure" | "search_time" | "search_total" | "source_index" | "state" | "transform_type" | "trigger_count" | "version")[])*: List of column names or column aliases used to sort the response.
** *`time` (Optional, Enum("nanos" | "micros" | "ms" | "s" | "m" | "h" | "d"))*: The unit used to display time values.
** *`size` (Optional, number)*: The maximum number of transforms to obtain.

[discrete]
=== ccr
[discrete]
==== delete_auto_follow_pattern
Deletes auto-follow patterns.

{ref}/ccr-delete-auto-follow-pattern.html[Endpoint documentation]
[source,ts]
----
client.ccr.deleteAutoFollowPattern({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the auto follow pattern.

[discrete]
==== follow
Creates a new follower index configured to follow the referenced leader index.

{ref}/ccr-put-follow.html[Endpoint documentation]
[source,ts]
----
client.ccr.follow({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: The name of the follower index
** *`leader_index` (Optional, string)*
** *`max_outstanding_read_requests` (Optional, number)*
** *`max_outstanding_write_requests` (Optional, number)*
** *`max_read_request_operation_count` (Optional, number)*
** *`max_read_request_size` (Optional, string)*
** *`max_retry_delay` (Optional, string | -1 | 0)*
** *`max_write_buffer_count` (Optional, number)*
** *`max_write_buffer_size` (Optional, string)*
** *`max_write_request_operation_count` (Optional, number)*
** *`max_write_request_size` (Optional, string)*
** *`read_poll_timeout` (Optional, string | -1 | 0)*
** *`remote_cluster` (Optional, string)*
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: Sets the number of shard copies that must be active before returning. Defaults to 0. Set to `all` for all shard copies, otherwise set to any non-negative value less than or equal to the total number of copies for the shard (number of replicas + 1)

[discrete]
==== follow_info
Retrieves information about all follower indices, including parameters and status for each follower index

{ref}/ccr-get-follow-info.html[Endpoint documentation]
[source,ts]
----
client.ccr.followInfo({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: A list of index patterns; use `_all` to perform the operation on all indices

[discrete]
==== follow_stats
Retrieves follower stats. return shard-level stats about the following tasks associated with each shard for the specified indices.

{ref}/ccr-get-follow-stats.html[Endpoint documentation]
[source,ts]
----
client.ccr.followStats({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: A list of index patterns; use `_all` to perform the operation on all indices

[discrete]
==== forget_follower
Removes the follower retention leases from the leader.

{ref}/ccr-post-forget-follower.html[Endpoint documentation]
[source,ts]
----
client.ccr.forgetFollower({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: the name of the leader index for which specified follower retention leases should be removed
** *`follower_cluster` (Optional, string)*
** *`follower_index` (Optional, string)*
** *`follower_index_uuid` (Optional, string)*
** *`leader_remote_cluster` (Optional, string)*

[discrete]
==== get_auto_follow_pattern
Gets configured auto-follow patterns. Returns the specified auto-follow pattern collection.

{ref}/ccr-get-auto-follow-pattern.html[Endpoint documentation]
[source,ts]
----
client.ccr.getAutoFollowPattern({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string)*: Specifies the auto-follow pattern collection that you want to retrieve. If you do not specify a name, the API returns information for all collections.

[discrete]
==== pause_auto_follow_pattern
Pauses an auto-follow pattern

{ref}/ccr-pause-auto-follow-pattern.html[Endpoint documentation]
[source,ts]
----
client.ccr.pauseAutoFollowPattern({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the auto follow pattern that should pause discovering new indices to follow.

[discrete]
==== pause_follow
Pauses a follower index. The follower index will not fetch any additional operations from the leader index.

{ref}/ccr-post-pause-follow.html[Endpoint documentation]
[source,ts]
----
client.ccr.pauseFollow({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: The name of the follower index that should pause following its leader index.

[discrete]
==== put_auto_follow_pattern
Creates a new named collection of auto-follow patterns against a specified remote cluster. Newly created indices on the remote cluster matching any of the specified patterns will be automatically configured as follower indices.

{ref}/ccr-put-auto-follow-pattern.html[Endpoint documentation]
[source,ts]
----
client.ccr.putAutoFollowPattern({ name, remote_cluster })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the collection of auto-follow patterns.
** *`remote_cluster` (string)*: The remote cluster containing the leader indices to match against.
** *`follow_index_pattern` (Optional, string)*: The name of follower index. The template {{leader_index}} can be used to derive the name of the follower index from the name of the leader index. When following a data stream, use {{leader_index}}; CCR does not support changes to the names of a follower data stream’s backing indices.
** *`leader_index_patterns` (Optional, string[])*: An array of simple index patterns to match against indices in the remote cluster specified by the remote_cluster field.
** *`leader_index_exclusion_patterns` (Optional, string[])*: An array of simple index patterns that can be used to exclude indices from being auto-followed. Indices in the remote cluster whose names are matching one or more leader_index_patterns and one or more leader_index_exclusion_patterns won’t be followed.
** *`max_outstanding_read_requests` (Optional, number)*: The maximum number of outstanding reads requests from the remote cluster.
** *`settings` (Optional, Record<string, User-defined value>)*: Settings to override from the leader index. Note that certain settings can not be overrode (e.g., index.number_of_shards).
** *`max_outstanding_write_requests` (Optional, number)*: The maximum number of outstanding reads requests from the remote cluster.
** *`read_poll_timeout` (Optional, string | -1 | 0)*: The maximum time to wait for new operations on the remote cluster when the follower index is synchronized with the leader index. When the timeout has elapsed, the poll for operations will return to the follower so that it can update some statistics. Then the follower will immediately attempt to read from the leader again.
** *`max_read_request_operation_count` (Optional, number)*: The maximum number of operations to pull per read from the remote cluster.
** *`max_read_request_size` (Optional, number | string)*: The maximum size in bytes of per read of a batch of operations pulled from the remote cluster.
** *`max_retry_delay` (Optional, string | -1 | 0)*: The maximum time to wait before retrying an operation that failed exceptionally. An exponential backoff strategy is employed when retrying.
** *`max_write_buffer_count` (Optional, number)*: The maximum number of operations that can be queued for writing. When this limit is reached, reads from the remote cluster will be deferred until the number of queued operations goes below the limit.
** *`max_write_buffer_size` (Optional, number | string)*: The maximum total bytes of operations that can be queued for writing. When this limit is reached, reads from the remote cluster will be deferred until the total bytes of queued operations goes below the limit.
** *`max_write_request_operation_count` (Optional, number)*: The maximum number of operations per bulk write request executed on the follower.
** *`max_write_request_size` (Optional, number | string)*: The maximum total bytes of operations per bulk write request executed on the follower.

[discrete]
==== resume_auto_follow_pattern
Resumes an auto-follow pattern that has been paused

{ref}/ccr-resume-auto-follow-pattern.html[Endpoint documentation]
[source,ts]
----
client.ccr.resumeAutoFollowPattern({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the auto follow pattern to resume discovering new indices to follow.

[discrete]
==== resume_follow
Resumes a follower index that has been paused

{ref}/ccr-post-resume-follow.html[Endpoint documentation]
[source,ts]
----
client.ccr.resumeFollow({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: The name of the follow index to resume following.
** *`max_outstanding_read_requests` (Optional, number)*
** *`max_outstanding_write_requests` (Optional, number)*
** *`max_read_request_operation_count` (Optional, number)*
** *`max_read_request_size` (Optional, string)*
** *`max_retry_delay` (Optional, string | -1 | 0)*
** *`max_write_buffer_count` (Optional, number)*
** *`max_write_buffer_size` (Optional, string)*
** *`max_write_request_operation_count` (Optional, number)*
** *`max_write_request_size` (Optional, string)*
** *`read_poll_timeout` (Optional, string | -1 | 0)*

[discrete]
==== stats
Gets all stats related to cross-cluster replication.

{ref}/ccr-get-stats.html[Endpoint documentation]
[source,ts]
----
client.ccr.stats()
----


[discrete]
==== unfollow
Stops the following task associated with a follower index and removes index metadata and settings associated with cross-cluster replication.

{ref}/ccr-post-unfollow.html[Endpoint documentation]
[source,ts]
----
client.ccr.unfollow({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: The name of the follower index that should be turned into a regular index.

[discrete]
=== cluster
[discrete]
==== allocation_explain
Provides explanations for shard allocations in the cluster.

{ref}/cluster-allocation-explain.html[Endpoint documentation]
[source,ts]
----
client.cluster.allocationExplain({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`current_node` (Optional, string)*: Specifies the node ID or the name of the node to only explain a shard that is currently located on the specified node.
** *`index` (Optional, string)*: Specifies the name of the index that you would like an explanation for.
** *`primary` (Optional, boolean)*: If true, returns explanation for the primary shard for the given shard ID.
** *`shard` (Optional, number)*: Specifies the ID of the shard that you would like an explanation for.
** *`include_disk_info` (Optional, boolean)*: If true, returns information about disk usage and shard sizes.
** *`include_yes_decisions` (Optional, boolean)*: If true, returns YES decisions in explanation.

[discrete]
==== delete_component_template
Delete component templates.
Deletes component templates.
Component templates are building blocks for constructing index templates that specify index mappings, settings, and aliases.

{ref}/indices-component-template.html[Endpoint documentation]
[source,ts]
----
client.cluster.deleteComponentTemplate({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: List or wildcard expression of component template names used to limit the request.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== delete_voting_config_exclusions
Clears cluster voting config exclusions.

{ref}/voting-config-exclusions.html[Endpoint documentation]
[source,ts]
----
client.cluster.deleteVotingConfigExclusions({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`wait_for_removal` (Optional, boolean)*: Specifies whether to wait for all excluded nodes to be removed from the
cluster before clearing the voting configuration exclusions list.
Defaults to true, meaning that all excluded nodes must be removed from
the cluster before this API takes any action. If set to false then the
voting configuration exclusions list is cleared even if some excluded
nodes are still in the cluster.

[discrete]
==== exists_component_template
Check component templates.
Returns information about whether a particular component template exists.

{ref}/indices-component-template.html[Endpoint documentation]
[source,ts]
----
client.cluster.existsComponentTemplate({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: List of component template names used to limit the request.
Wildcard (*) expressions are supported.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is
received before the timeout expires, the request fails and returns an
error.
** *`local` (Optional, boolean)*: If true, the request retrieves information from the local node only.
Defaults to false, which means information is retrieved from the master node.

[discrete]
==== get_component_template
Get component templates.
Retrieves information about component templates.

{ref}/indices-component-template.html[Endpoint documentation]
[source,ts]
----
client.cluster.getComponentTemplate({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string)*: List of component template names used to limit the request.
Wildcard (`*`) expressions are supported.
** *`flat_settings` (Optional, boolean)*: If `true`, returns settings in flat format.
** *`include_defaults` (Optional, boolean)*: Return all default configurations for the component template (default: false)
** *`local` (Optional, boolean)*: If `true`, the request retrieves information from the local node only.
If `false`, information is retrieved from the master node.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== get_settings
Returns cluster-wide settings.
By default, it returns only settings that have been explicitly defined.

{ref}/cluster-get-settings.html[Endpoint documentation]
[source,ts]
----
client.cluster.getSettings({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`flat_settings` (Optional, boolean)*: If `true`, returns settings in flat format.
** *`include_defaults` (Optional, boolean)*: If `true`, returns default cluster settings from the local node.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== health
The cluster health API returns a simple status on the health of the cluster. You can also use the API to get the health status of only specified data streams and indices. For data streams, the API retrieves the health status of the stream’s backing indices.
The cluster health status is: green, yellow or red. On the shard level, a red status indicates that the specific shard is not allocated in the cluster, yellow means that the primary shard is allocated but replicas are not, and green means that all shards are allocated. The index level status is controlled by the worst shard status. The cluster status is controlled by the worst index status.

{ref}/cluster-health.html[Endpoint documentation]
[source,ts]
----
client.cluster.health({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and index aliases used to limit the request. Wildcard expressions (`*`) are supported. To target all data streams and indices in a cluster, omit this parameter or use _all or `*`.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Whether to expand wildcard expression to concrete indices that are open, closed or both.
** *`level` (Optional, Enum("cluster" | "indices" | "shards"))*: Can be one of cluster, indices or shards. Controls the details level of the health information returned.
** *`local` (Optional, boolean)*: If true, the request retrieves information from the local node only. Defaults to false, which means information is retrieved from the master node.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: A number controlling to how many active shards to wait for, all to wait for all shards in the cluster to be active, or 0 to not wait.
** *`wait_for_events` (Optional, Enum("immediate" | "urgent" | "high" | "normal" | "low" | "languid"))*: Can be one of immediate, urgent, high, normal, low, languid. Wait until all currently queued events with the given priority are processed.
** *`wait_for_nodes` (Optional, string | number)*: The request waits until the specified number N of nodes is available. It also accepts >=N, <=N, >N and <N. Alternatively, it is possible to use ge(N), le(N), gt(N) and lt(N) notation.
** *`wait_for_no_initializing_shards` (Optional, boolean)*: A boolean value which controls whether to wait (until the timeout provided) for the cluster to have no shard initializations. Defaults to false, which means it will not wait for initializing shards.
** *`wait_for_no_relocating_shards` (Optional, boolean)*: A boolean value which controls whether to wait (until the timeout provided) for the cluster to have no shard relocations. Defaults to false, which means it will not wait for relocating shards.
** *`wait_for_status` (Optional, Enum("green" | "yellow" | "red"))*: One of green, yellow or red. Will wait (until the timeout provided) until the status of the cluster changes to the one provided or better, i.e. green > yellow > red. By default, will not wait for any status.

[discrete]
==== info
Get cluster info.
Returns basic information about the cluster.

{ref}/cluster-info.html[Endpoint documentation]
[source,ts]
----
client.cluster.info({ target })
----

[discrete]
==== Arguments

* *Request (object):*
** *`target` (Enum("_all" | "http" | "ingest" | "thread_pool" | "script") | Enum("_all" | "http" | "ingest" | "thread_pool" | "script")[])*: Limits the information returned to the specific target. Supports a list, such as http,ingest.

[discrete]
==== pending_tasks
Returns cluster-level changes (such as create index, update mapping, allocate or fail shard) that have not yet been executed.
NOTE: This API returns a list of any pending updates to the cluster state.
These are distinct from the tasks reported by the Task Management API which include periodic tasks and tasks initiated by the user, such as node stats, search queries, or create index requests.
However, if a user-initiated task such as a create index command causes a cluster state update, the activity of this task might be reported by both task api and pending cluster tasks API.

{ref}/cluster-pending.html[Endpoint documentation]
[source,ts]
----
client.cluster.pendingTasks({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`local` (Optional, boolean)*: If `true`, the request retrieves information from the local node only.
If `false`, information is retrieved from the master node.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== post_voting_config_exclusions
Updates the cluster voting config exclusions by node ids or node names.

{ref}/voting-config-exclusions.html[Endpoint documentation]
[source,ts]
----
client.cluster.postVotingConfigExclusions({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`node_names` (Optional, string | string[])*: A list of the names of the nodes to exclude from the
voting configuration. If specified, you may not also specify node_ids.
** *`node_ids` (Optional, string | string[])*: A list of the persistent ids of the nodes to exclude
from the voting configuration. If specified, you may not also specify node_names.
** *`timeout` (Optional, string | -1 | 0)*: When adding a voting configuration exclusion, the API waits for the
specified nodes to be excluded from the voting configuration before
returning. If the timeout expires before the appropriate condition
is satisfied, the request fails and returns an error.

[discrete]
==== put_component_template
Create or update a component template.
Creates or updates a component template.
Component templates are building blocks for constructing index templates that specify index mappings, settings, and aliases.

An index template can be composed of multiple component templates.
To use a component template, specify it in an index template’s `composed_of` list.
Component templates are only applied to new data streams and indices as part of a matching index template.

Settings and mappings specified directly in the index template or the create index request override any settings or mappings specified in a component template.

Component templates are only used during index creation.
For data streams, this includes data stream creation and the creation of a stream’s backing indices.
Changes to component templates do not affect existing indices, including a stream’s backing indices.

You can use C-style `/* *\/` block comments in component templates.
You can include comments anywhere in the request body except before the opening curly bracket.

{ref}/indices-component-template.html[Endpoint documentation]
[source,ts]
----
client.cluster.putComponentTemplate({ name, template })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Name of the component template to create.
Elasticsearch includes the following built-in component templates: `logs-mappings`; `logs-settings`; `metrics-mappings`; `metrics-settings`;`synthetics-mapping`; `synthetics-settings`.
Elastic Agent uses these templates to configure backing indices for its data streams.
If you use Elastic Agent and want to overwrite one of these templates, set the `version` for your replacement template higher than the current version.
If you don’t use Elastic Agent and want to disable all built-in component and index templates, set `stack.templates.enabled` to `false` using the cluster update settings API.
** *`template` ({ aliases, mappings, settings, defaults, data_stream, lifecycle })*: The template to be applied which includes mappings, settings, or aliases configuration.
** *`version` (Optional, number)*: Version number used to manage component templates externally.
This number isn't automatically generated or incremented by Elasticsearch.
To unset a version, replace the template without specifying a version.
** *`_meta` (Optional, Record<string, User-defined value>)*: Optional user metadata about the component template.
May have any contents. This map is not automatically generated by Elasticsearch.
This information is stored in the cluster state, so keeping it short is preferable.
To unset `_meta`, replace the template without specifying this information.
** *`deprecated` (Optional, boolean)*: Marks this index template as deprecated. When creating or updating a non-deprecated index template
that uses deprecated components, Elasticsearch will emit a deprecation warning.
** *`create` (Optional, boolean)*: If `true`, this request cannot replace or update existing component templates.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== put_settings
Updates the cluster settings.

{ref}/cluster-update-settings.html[Endpoint documentation]
[source,ts]
----
client.cluster.putSettings({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`persistent` (Optional, Record<string, User-defined value>)*
** *`transient` (Optional, Record<string, User-defined value>)*
** *`flat_settings` (Optional, boolean)*: Return settings in flat format (default: false)
** *`master_timeout` (Optional, string | -1 | 0)*: Explicit operation timeout for connection to master node
** *`timeout` (Optional, string | -1 | 0)*: Explicit operation timeout

[discrete]
==== remote_info
The cluster remote info API allows you to retrieve all of the configured
remote cluster information. It returns connection and endpoint information
keyed by the configured remote cluster alias.

{ref}/cluster-remote-info.html[Endpoint documentation]
[source,ts]
----
client.cluster.remoteInfo()
----


[discrete]
==== reroute
Allows to manually change the allocation of individual shards in the cluster.

{ref}/cluster-reroute.html[Endpoint documentation]
[source,ts]
----
client.cluster.reroute({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`commands` (Optional, { cancel, move, allocate_replica, allocate_stale_primary, allocate_empty_primary }[])*: Defines the commands to perform.
** *`dry_run` (Optional, boolean)*: If true, then the request simulates the operation only and returns the resulting state.
** *`explain` (Optional, boolean)*: If true, then the response contains an explanation of why the commands can or cannot be executed.
** *`metric` (Optional, string | string[])*: Limits the information returned to the specified metrics.
** *`retry_failed` (Optional, boolean)*: If true, then retries allocation of shards that are blocked due to too many subsequent allocation failures.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== state
Returns a comprehensive information about the state of the cluster.

{ref}/cluster-state.html[Endpoint documentation]
[source,ts]
----
client.cluster.state({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`metric` (Optional, string | string[])*: Limit the information returned to the specified metrics
** *`index` (Optional, string | string[])*: A list of index names; use `_all` or empty string to perform the operation on all indices
** *`allow_no_indices` (Optional, boolean)*: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes `_all` string or when no indices have been specified)
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Whether to expand wildcard expression to concrete indices that are open, closed or both.
** *`flat_settings` (Optional, boolean)*: Return settings in flat format (default: false)
** *`ignore_unavailable` (Optional, boolean)*: Whether specified concrete indices should be ignored when unavailable (missing or closed)
** *`local` (Optional, boolean)*: Return local information, do not retrieve the state from master node (default: false)
** *`master_timeout` (Optional, string | -1 | 0)*: Specify timeout for connection to master
** *`wait_for_metadata_version` (Optional, number)*: Wait for the metadata version to be equal or greater than the specified metadata version
** *`wait_for_timeout` (Optional, string | -1 | 0)*: The maximum time to wait for wait_for_metadata_version before timing out

[discrete]
==== stats
Returns cluster statistics.
It returns basic index metrics (shard numbers, store size, memory usage) and information about the current nodes that form the cluster (number, roles, os, jvm versions, memory usage, cpu and installed plugins).

{ref}/cluster-stats.html[Endpoint documentation]
[source,ts]
----
client.cluster.stats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`node_id` (Optional, string | string[])*: List of node filters used to limit returned information. Defaults to all nodes in the cluster.
** *`include_remotes` (Optional, boolean)*: Include remote cluster data into the response
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for each node to respond.
If a node does not respond before its timeout expires, the response does not include its stats.
However, timed out nodes are included in the response’s `_nodes.failed` property. Defaults to no timeout.

[discrete]
=== connector
[discrete]
==== check_in
Check in a connector.

Update the `last_seen` field in the connector and set it to the current timestamp.

{ref}/check-in-connector-api.html[Endpoint documentation]
[source,ts]
----
client.connector.checkIn({ connector_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (string)*: The unique identifier of the connector to be checked in

[discrete]
==== delete
Delete a connector.

Removes a connector and associated sync jobs.
This is a destructive action that is not recoverable.
NOTE: This action doesn’t delete any API keys, ingest pipelines, or data indices associated with the connector.
These need to be removed manually.

{ref}/delete-connector-api.html[Endpoint documentation]
[source,ts]
----
client.connector.delete({ connector_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (string)*: The unique identifier of the connector to be deleted
** *`delete_sync_jobs` (Optional, boolean)*: A flag indicating if associated sync jobs should be also removed. Defaults to false.

[discrete]
==== get
Get a connector.

Get the details about a connector.

{ref}/get-connector-api.html[Endpoint documentation]
[source,ts]
----
client.connector.get({ connector_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (string)*: The unique identifier of the connector

[discrete]
==== list
Get all connectors.

Get information about all connectors.

{ref}/list-connector-api.html[Endpoint documentation]
[source,ts]
----
client.connector.list({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`from` (Optional, number)*: Starting offset (default: 0)
** *`size` (Optional, number)*: Specifies a max number of results to get
** *`index_name` (Optional, string | string[])*: A list of connector index names to fetch connector documents for
** *`connector_name` (Optional, string | string[])*: A list of connector names to fetch connector documents for
** *`service_type` (Optional, string | string[])*: A list of connector service types to fetch connector documents for
** *`query` (Optional, string)*: A wildcard query string that filters connectors with matching name, description or index name

[discrete]
==== post
Create a connector.

Connectors are Elasticsearch integrations that bring content from third-party data sources, which can be deployed on Elastic Cloud or hosted on your own infrastructure.
Elastic managed connectors (Native connectors) are a managed service on Elastic Cloud.
Self-managed connectors (Connector clients) are self-managed on your infrastructure.

{ref}/create-connector-api.html[Endpoint documentation]
[source,ts]
----
client.connector.post({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`description` (Optional, string)*
** *`index_name` (Optional, string)*
** *`is_native` (Optional, boolean)*
** *`language` (Optional, string)*
** *`name` (Optional, string)*
** *`service_type` (Optional, string)*

[discrete]
==== put
Create or update a connector.

{ref}/create-connector-api.html[Endpoint documentation]
[source,ts]
----
client.connector.put({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (Optional, string)*: The unique identifier of the connector to be created or updated. ID is auto-generated if not provided.
** *`description` (Optional, string)*
** *`index_name` (Optional, string)*
** *`is_native` (Optional, boolean)*
** *`language` (Optional, string)*
** *`name` (Optional, string)*
** *`service_type` (Optional, string)*

[discrete]
==== sync_job_cancel
Cancel a connector sync job.

Cancel a connector sync job, which sets the status to cancelling and updates `cancellation_requested_at` to the current time.
The connector service is then responsible for setting the status of connector sync jobs to cancelled.

{ref}/cancel-connector-sync-job-api.html[Endpoint documentation]
[source,ts]
----
client.connector.syncJobCancel({ connector_sync_job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_sync_job_id` (string)*: The unique identifier of the connector sync job

[discrete]
==== sync_job_check_in
Checks in a connector sync job (refreshes 'last_seen').

{ref}/check-in-connector-sync-job-api.html[Endpoint documentation]
[source,ts]
----
client.connector.syncJobCheckIn()
----


[discrete]
==== sync_job_claim
Claims a connector sync job.
[source,ts]
----
client.connector.syncJobClaim()
----


[discrete]
==== sync_job_delete
Delete a connector sync job.

Remove a connector sync job and its associated data.
This is a destructive action that is not recoverable.

{ref}/delete-connector-sync-job-api.html[Endpoint documentation]
[source,ts]
----
client.connector.syncJobDelete({ connector_sync_job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_sync_job_id` (string)*: The unique identifier of the connector sync job to be deleted

[discrete]
==== sync_job_error
Sets an error for a connector sync job.

{ref}/set-connector-sync-job-error-api.html[Endpoint documentation]
[source,ts]
----
client.connector.syncJobError()
----


[discrete]
==== sync_job_get
Get a connector sync job.

{ref}/get-connector-sync-job-api.html[Endpoint documentation]
[source,ts]
----
client.connector.syncJobGet({ connector_sync_job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_sync_job_id` (string)*: The unique identifier of the connector sync job

[discrete]
==== sync_job_list
Get all connector sync jobs.

Get information about all stored connector sync jobs listed by their creation date in ascending order.

{ref}/list-connector-sync-jobs-api.html[Endpoint documentation]
[source,ts]
----
client.connector.syncJobList({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`from` (Optional, number)*: Starting offset (default: 0)
** *`size` (Optional, number)*: Specifies a max number of results to get
** *`status` (Optional, Enum("canceling" | "canceled" | "completed" | "error" | "in_progress" | "pending" | "suspended"))*: A sync job status to fetch connector sync jobs for
** *`connector_id` (Optional, string)*: A connector id to fetch connector sync jobs for
** *`job_type` (Optional, Enum("full" | "incremental" | "access_control") | Enum("full" | "incremental" | "access_control")[])*: A list of job types to fetch the sync jobs for

[discrete]
==== sync_job_post
Create a connector sync job.

Create a connector sync job document in the internal index and initialize its counters and timestamps with default values.

{ref}/create-connector-sync-job-api.html[Endpoint documentation]
[source,ts]
----
client.connector.syncJobPost({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: The id of the associated connector
** *`job_type` (Optional, Enum("full" | "incremental" | "access_control"))*
** *`trigger_method` (Optional, Enum("on_demand" | "scheduled"))*

[discrete]
==== sync_job_update_stats
Updates the stats fields in the connector sync job document.

{ref}/set-connector-sync-job-stats-api.html[Endpoint documentation]
[source,ts]
----
client.connector.syncJobUpdateStats()
----


[discrete]
==== update_active_filtering
Activate the connector draft filter.

Activates the valid draft filtering for a connector.

{ref}/update-connector-filtering-api.html[Endpoint documentation]
[source,ts]
----
client.connector.updateActiveFiltering({ connector_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (string)*: The unique identifier of the connector to be updated

[discrete]
==== update_api_key_id
Update the connector API key ID.

Update the `api_key_id` and `api_key_secret_id` fields of a connector.
You can specify the ID of the API key used for authorization and the ID of the connector secret where the API key is stored.
The connector secret ID is required only for Elastic managed (native) connectors.
Self-managed connectors (connector clients) do not use this field.

{ref}/update-connector-api-key-id-api.html[Endpoint documentation]
[source,ts]
----
client.connector.updateApiKeyId({ connector_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (string)*: The unique identifier of the connector to be updated
** *`api_key_id` (Optional, string)*
** *`api_key_secret_id` (Optional, string)*

[discrete]
==== update_configuration
Update the connector configuration.

Update the configuration field in the connector document.

{ref}/update-connector-configuration-api.html[Endpoint documentation]
[source,ts]
----
client.connector.updateConfiguration({ connector_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (string)*: The unique identifier of the connector to be updated
** *`configuration` (Optional, Record<string, { category, default_value, depends_on, display, label, options, order, placeholder, required, sensitive, tooltip, type, ui_restrictions, validations, value }>)*
** *`values` (Optional, Record<string, User-defined value>)*

[discrete]
==== update_error
Update the connector error field.

Set the error field for the connector.
If the error provided in the request body is non-null, the connector’s status is updated to error.
Otherwise, if the error is reset to null, the connector status is updated to connected.

{ref}/update-connector-error-api.html[Endpoint documentation]
[source,ts]
----
client.connector.updateError({ connector_id, error })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (string)*: The unique identifier of the connector to be updated
** *`error` (T | null)*

[discrete]
==== update_features
Updates the connector features in the connector document.

{ref}/update-connector-features-api.html[Endpoint documentation]
[source,ts]
----
client.connector.updateFeatures()
----


[discrete]
==== update_filtering
Update the connector filtering.

Update the draft filtering configuration of a connector and marks the draft validation state as edited.
The filtering draft is activated once validated by the running Elastic connector service.
The filtering property is used to configure sync rules (both basic and advanced) for a connector.

{ref}/update-connector-filtering-api.html[Endpoint documentation]
[source,ts]
----
client.connector.updateFiltering({ connector_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (string)*: The unique identifier of the connector to be updated
** *`filtering` (Optional, { active, domain, draft }[])*
** *`rules` (Optional, { created_at, field, id, order, policy, rule, updated_at, value }[])*
** *`advanced_snippet` (Optional, { created_at, updated_at, value })*

[discrete]
==== update_filtering_validation
Update the connector draft filtering validation.

Update the draft filtering validation info for a connector.
[source,ts]
----
client.connector.updateFilteringValidation({ connector_id, validation })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (string)*: The unique identifier of the connector to be updated
** *`validation` ({ errors, state })*

[discrete]
==== update_index_name
Update the connector index name.

Update the `index_name` field of a connector, specifying the index where the data ingested by the connector is stored.

{ref}/update-connector-index-name-api.html[Endpoint documentation]
[source,ts]
----
client.connector.updateIndexName({ connector_id, index_name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (string)*: The unique identifier of the connector to be updated
** *`index_name` (T | null)*

[discrete]
==== update_name
Update the connector name and description.

{ref}/update-connector-name-description-api.html[Endpoint documentation]
[source,ts]
----
client.connector.updateName({ connector_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (string)*: The unique identifier of the connector to be updated
** *`name` (Optional, string)*
** *`description` (Optional, string)*

[discrete]
==== update_native
Update the connector is_native flag.
[source,ts]
----
client.connector.updateNative({ connector_id, is_native })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (string)*: The unique identifier of the connector to be updated
** *`is_native` (boolean)*

[discrete]
==== update_pipeline
Update the connector pipeline.

When you create a new connector, the configuration of an ingest pipeline is populated with default settings.

{ref}/update-connector-pipeline-api.html[Endpoint documentation]
[source,ts]
----
client.connector.updatePipeline({ connector_id, pipeline })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (string)*: The unique identifier of the connector to be updated
** *`pipeline` ({ extract_binary_content, name, reduce_whitespace, run_ml_inference })*

[discrete]
==== update_scheduling
Update the connector scheduling.

{ref}/update-connector-scheduling-api.html[Endpoint documentation]
[source,ts]
----
client.connector.updateScheduling({ connector_id, scheduling })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (string)*: The unique identifier of the connector to be updated
** *`scheduling` ({ access_control, full, incremental })*

[discrete]
==== update_service_type
Update the connector service type.

{ref}/update-connector-service-type-api.html[Endpoint documentation]
[source,ts]
----
client.connector.updateServiceType({ connector_id, service_type })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (string)*: The unique identifier of the connector to be updated
** *`service_type` (string)*

[discrete]
==== update_status
Update the connector status.

{ref}/update-connector-status-api.html[Endpoint documentation]
[source,ts]
----
client.connector.updateStatus({ connector_id, status })
----

[discrete]
==== Arguments

* *Request (object):*
** *`connector_id` (string)*: The unique identifier of the connector to be updated
** *`status` (Enum("created" | "needs_configuration" | "configured" | "connected" | "error"))*

[discrete]
=== dangling_indices
[discrete]
==== delete_dangling_index
Delete a dangling index.

If Elasticsearch encounters index data that is absent from the current cluster state, those indices are considered to be dangling.
For example, this can happen if you delete more than `cluster.indices.tombstones.size` indices while an Elasticsearch node is offline.

{ref}/modules-gateway-dangling-indices.html[Endpoint documentation]
[source,ts]
----
client.danglingIndices.deleteDanglingIndex({ index_uuid, accept_data_loss })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index_uuid` (string)*: The UUID of the index to delete. Use the get dangling indices API to find the UUID.
** *`accept_data_loss` (boolean)*: This parameter must be set to true to acknowledge that it will no longer be possible to recove data from the dangling index.
** *`master_timeout` (Optional, string | -1 | 0)*: Specify timeout for connection to master
** *`timeout` (Optional, string | -1 | 0)*: Explicit operation timeout

[discrete]
==== import_dangling_index
Import a dangling index.

If Elasticsearch encounters index data that is absent from the current cluster state, those indices are considered to be dangling.
For example, this can happen if you delete more than `cluster.indices.tombstones.size` indices while an Elasticsearch node is offline.

{ref}/modules-gateway-dangling-indices.html[Endpoint documentation]
[source,ts]
----
client.danglingIndices.importDanglingIndex({ index_uuid, accept_data_loss })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index_uuid` (string)*: The UUID of the index to import. Use the get dangling indices API to locate the UUID.
** *`accept_data_loss` (boolean)*: This parameter must be set to true to import a dangling index.
Because Elasticsearch cannot know where the dangling index data came from or determine which shard copies are fresh and which are stale, it cannot guarantee that the imported data represents the latest state of the index when it was last in the cluster.
** *`master_timeout` (Optional, string | -1 | 0)*: Specify timeout for connection to master
** *`timeout` (Optional, string | -1 | 0)*: Explicit operation timeout

[discrete]
==== list_dangling_indices
Get the dangling indices.

If Elasticsearch encounters index data that is absent from the current cluster state, those indices are considered to be dangling.
For example, this can happen if you delete more than `cluster.indices.tombstones.size` indices while an Elasticsearch node is offline.

Use this API to list dangling indices, which you can then import or delete.

{ref}/modules-gateway-dangling-indices.html[Endpoint documentation]
[source,ts]
----
client.danglingIndices.listDanglingIndices()
----


[discrete]
=== enrich
[discrete]
==== delete_policy
Delete an enrich policy.
Deletes an existing enrich policy and its enrich index.

{ref}/delete-enrich-policy-api.html[Endpoint documentation]
[source,ts]
----
client.enrich.deletePolicy({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Enrich policy to delete.

[discrete]
==== execute_policy
Creates the enrich index for an existing enrich policy.

{ref}/execute-enrich-policy-api.html[Endpoint documentation]
[source,ts]
----
client.enrich.executePolicy({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Enrich policy to execute.
** *`wait_for_completion` (Optional, boolean)*: If `true`, the request blocks other enrich policy execution requests until complete.

[discrete]
==== get_policy
Get an enrich policy.
Returns information about an enrich policy.

{ref}/get-enrich-policy-api.html[Endpoint documentation]
[source,ts]
----
client.enrich.getPolicy({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string | string[])*: List of enrich policy names used to limit the request.
To return information for all enrich policies, omit this parameter.

[discrete]
==== put_policy
Create an enrich policy.
Creates an enrich policy.

{ref}/put-enrich-policy-api.html[Endpoint documentation]
[source,ts]
----
client.enrich.putPolicy({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Name of the enrich policy to create or update.
** *`geo_match` (Optional, { enrich_fields, indices, match_field, query, name, elasticsearch_version })*: Matches enrich data to incoming documents based on a `geo_shape` query.
** *`match` (Optional, { enrich_fields, indices, match_field, query, name, elasticsearch_version })*: Matches enrich data to incoming documents based on a `term` query.
** *`range` (Optional, { enrich_fields, indices, match_field, query, name, elasticsearch_version })*: Matches a number, date, or IP address in incoming documents to a range in the enrich index based on a `term` query.

[discrete]
==== stats
Get enrich stats.
Returns enrich coordinator statistics and information about enrich policies that are currently executing.

{ref}/enrich-stats-api.html[Endpoint documentation]
[source,ts]
----
client.enrich.stats()
----


[discrete]
=== eql
[discrete]
==== delete
Deletes an async EQL search or a stored synchronous EQL search.
The API also deletes results for the search.

{ref}/eql-search-api.html[Endpoint documentation]
[source,ts]
----
client.eql.delete({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the search to delete.
A search ID is provided in the EQL search API's response for an async search.
A search ID is also provided if the request’s `keep_on_completion` parameter is `true`.

[discrete]
==== get
Returns the current status and available results for an async EQL search or a stored synchronous EQL search.

{ref}/get-async-eql-search-api.html[Endpoint documentation]
[source,ts]
----
client.eql.get({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the search.
** *`keep_alive` (Optional, string | -1 | 0)*: Period for which the search and its results are stored on the cluster.
Defaults to the keep_alive value set by the search’s EQL search API request.
** *`wait_for_completion_timeout` (Optional, string | -1 | 0)*: Timeout duration to wait for the request to finish.
Defaults to no timeout, meaning the request waits for complete search results.

[discrete]
==== get_status
Returns the current status for an async EQL search or a stored synchronous EQL search without returning results.

{ref}/get-async-eql-status-api.html[Endpoint documentation]
[source,ts]
----
client.eql.getStatus({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the search.

[discrete]
==== search
Returns results matching a query expressed in Event Query Language (EQL)

{ref}/eql-search-api.html[Endpoint documentation]
[source,ts]
----
client.eql.search({ index, query })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: The name of the index to scope the operation
** *`query` (string)*: EQL query you wish to run.
** *`case_sensitive` (Optional, boolean)*
** *`event_category_field` (Optional, string)*: Field containing the event classification, such as process, file, or network.
** *`tiebreaker_field` (Optional, string)*: Field used to sort hits with the same timestamp in ascending order
** *`timestamp_field` (Optional, string)*: Field containing event timestamp. Default "@timestamp"
** *`fetch_size` (Optional, number)*: Maximum number of events to search at a time for sequence queries.
** *`filter` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type } | { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type }[])*: Query, written in Query DSL, used to filter the events on which the EQL query runs.
** *`keep_alive` (Optional, string | -1 | 0)*
** *`keep_on_completion` (Optional, boolean)*
** *`wait_for_completion_timeout` (Optional, string | -1 | 0)*
** *`size` (Optional, number)*: For basic queries, the maximum number of matching events to return. Defaults to 10
** *`fields` (Optional, { field, format, include_unmapped } | { field, format, include_unmapped }[])*: Array of wildcard (*) patterns. The response returns values for field names matching these patterns in the fields property of each hit.
** *`result_position` (Optional, Enum("tail" | "head"))*
** *`runtime_mappings` (Optional, Record<string, { fields, fetch_fields, format, input_field, target_field, target_index, script, type }>)*
** *`allow_no_indices` (Optional, boolean)*
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*
** *`ignore_unavailable` (Optional, boolean)*: If true, missing or closed indices are not included in the response.

[discrete]
=== esql
[discrete]
==== async_query
Executes an ESQL request asynchronously

{ref}/esql-async-query-api.html[Endpoint documentation]
[source,ts]
----
client.esql.asyncQuery()
----


[discrete]
==== async_query_get
Retrieves the results of a previously submitted async query request given its ID.

{ref}/esql-async-query-get-api.html[Endpoint documentation]
[source,ts]
----
client.esql.asyncQueryGet()
----


[discrete]
==== query
Executes an ES|QL request

{ref}/esql-rest.html[Endpoint documentation]
[source,ts]
----
client.esql.query({ query })
----

[discrete]
==== Arguments

* *Request (object):*
** *`query` (string)*: The ES|QL query API accepts an ES|QL query string in the query parameter, runs it, and returns the results.
** *`columnar` (Optional, boolean)*: By default, ES|QL returns results as rows. For example, FROM returns each individual document as one row. For the JSON, YAML, CBOR and smile formats, ES|QL can return the results in a columnar fashion where one row represents all the values of a certain column in the results.
** *`filter` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Specify a Query DSL query in the filter parameter to filter the set of documents that an ES|QL query runs on.
** *`locale` (Optional, string)*
** *`params` (Optional, number | number | string | boolean | null | User-defined value[])*: To avoid any attempts of hacking or code injection, extract the values in a separate list of parameters. Use question mark placeholders (?) in the query string for each of the parameters.
** *`profile` (Optional, boolean)*: If provided and `true` the response will include an extra `profile` object
with information on how the query was executed. This information is for human debugging
and its format can change at any time but it can give some insight into the performance
of each part of the query.
** *`tables` (Optional, Record<string, Record<string, { integer, keyword, long, double }>>)*: Tables to use with the LOOKUP operation. The top level key is the table
name and the next level key is the column name.
** *`format` (Optional, Enum("csv" | "json" | "tsv" | "txt" | "yaml" | "cbor" | "smile" | "arrow"))*: A short version of the Accept header, e.g. json, yaml.
** *`delimiter` (Optional, string)*: The character to use between values within a CSV row. Only valid for the CSV format.
** *`drop_null_columns` (Optional, boolean)*: Should columns that are entirely `null` be removed from the `columns` and `values` portion of the results?
Defaults to `false`. If `true` then the response will include an extra section under the name `all_columns` which has the name of all columns.

[discrete]
=== features
[discrete]
==== get_features
Gets a list of features which can be included in snapshots using the feature_states field when creating a snapshot

{ref}/get-features-api.html[Endpoint documentation]
[source,ts]
----
client.features.getFeatures()
----


[discrete]
==== reset_features
Resets the internal state of features, usually by deleting system indices

{ref}/modules-snapshots.html[Endpoint documentation]
[source,ts]
----
client.features.resetFeatures()
----


[discrete]
=== fleet
[discrete]
==== global_checkpoints
Returns the current global checkpoints for an index. This API is design for internal use by the fleet server project.

{ref}/get-global-checkpoints.html[Endpoint documentation]
[source,ts]
----
client.fleet.globalCheckpoints({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string)*: A single index or index alias that resolves to a single index.
** *`wait_for_advance` (Optional, boolean)*: A boolean value which controls whether to wait (until the timeout) for the global checkpoints
to advance past the provided `checkpoints`.
** *`wait_for_index` (Optional, boolean)*: A boolean value which controls whether to wait (until the timeout) for the target index to exist
and all primary shards be active. Can only be true when `wait_for_advance` is true.
** *`checkpoints` (Optional, number[])*: A comma separated list of previous global checkpoints. When used in combination with `wait_for_advance`,
the API will only return once the global checkpoints advances past the checkpoints. Providing an empty list
will cause Elasticsearch to immediately return the current global checkpoints.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a global checkpoints to advance past `checkpoints`.

[discrete]
==== msearch
Executes several [fleet searches](https://www.elastic.co/guide/en/elasticsearch/reference/current/fleet-search.html) with a single API request.
The API follows the same structure as the [multi search](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-multi-search.html) API. However, similar to the fleet search API, it
supports the wait_for_checkpoints parameter.
[source,ts]
----
client.fleet.msearch({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string)*: A single target to search. If the target is an index alias, it must resolve to a single index.
** *`searches` (Optional, { allow_no_indices, expand_wildcards, ignore_unavailable, index, preference, request_cache, routing, search_type, ccs_minimize_roundtrips, allow_partial_search_results, ignore_throttled } | { aggregations, collapse, query, explain, ext, stored_fields, docvalue_fields, knn, from, highlight, indices_boost, min_score, post_filter, profile, rescore, script_fields, search_after, size, sort, _source, fields, terminate_after, stats, timeout, track_scores, track_total_hits, version, runtime_mappings, seq_no_primary_term, pit, suggest }[])*
** *`allow_no_indices` (Optional, boolean)*: If false, the request returns an error if any wildcard expression, index alias, or _all value targets only missing or closed indices. This behavior applies even if the request targets other open indices. For example, a request targeting foo*,bar* returns an error if an index starts with foo but no index starts with bar.
** *`ccs_minimize_roundtrips` (Optional, boolean)*: If true, network roundtrips between the coordinating node and remote clusters are minimized for cross-cluster search requests.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard expressions can match. If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
** *`ignore_throttled` (Optional, boolean)*: If true, concrete, expanded or aliased indices are ignored when frozen.
** *`ignore_unavailable` (Optional, boolean)*: If true, missing or closed indices are not included in the response.
** *`max_concurrent_searches` (Optional, number)*: Maximum number of concurrent searches the multi search API can execute.
** *`max_concurrent_shard_requests` (Optional, number)*: Maximum number of concurrent shard requests that each sub-search request executes per node.
** *`pre_filter_shard_size` (Optional, number)*: Defines a threshold that enforces a pre-filter roundtrip to prefilter search shards based on query rewriting if the number of shards the search request expands to exceeds the threshold. This filter roundtrip can limit the number of shards significantly if for instance a shard can not match any documents based on its rewrite method i.e., if date filters are mandatory to match but the shard bounds and the query are disjoint.
** *`search_type` (Optional, Enum("query_then_fetch" | "dfs_query_then_fetch"))*: Indicates whether global term and document frequencies should be used when scoring returned documents.
** *`rest_total_hits_as_int` (Optional, boolean)*: If true, hits.total are returned as an integer in the response. Defaults to false, which returns an object.
** *`typed_keys` (Optional, boolean)*: Specifies whether aggregation and suggester names should be prefixed by their respective types in the response.
** *`wait_for_checkpoints` (Optional, number[])*: A comma separated list of checkpoints. When configured, the search API will only be executed on a shard
after the relevant checkpoint has become visible for search. Defaults to an empty list which will cause
Elasticsearch to immediately execute the search.
** *`allow_partial_search_results` (Optional, boolean)*: If true, returns partial results if there are shard request timeouts or [shard failures](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-replication.html#shard-failures). If false, returns
an error with no partial results. Defaults to the configured cluster setting `search.default_allow_partial_results`
which is true by default.

[discrete]
==== search
The purpose of the fleet search api is to provide a search api where the search will only be executed
after provided checkpoint has been processed and is visible for searches inside of Elasticsearch.
[source,ts]
----
client.fleet.search({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string)*: A single target to search. If the target is an index alias, it must resolve to a single index.
** *`aggregations` (Optional, Record<string, { aggregations, meta, adjacency_matrix, auto_date_histogram, avg, avg_bucket, boxplot, bucket_script, bucket_selector, bucket_sort, bucket_count_ks_test, bucket_correlation, cardinality, categorize_text, children, composite, cumulative_cardinality, cumulative_sum, date_histogram, date_range, derivative, diversified_sampler, extended_stats, extended_stats_bucket, frequent_item_sets, filter, filters, geo_bounds, geo_centroid, geo_distance, geohash_grid, geo_line, geotile_grid, geohex_grid, global, histogram, ip_range, ip_prefix, inference, line, matrix_stats, max, max_bucket, median_absolute_deviation, min, min_bucket, missing, moving_avg, moving_percentiles, moving_fn, multi_terms, nested, normalize, parent, percentile_ranks, percentiles, percentiles_bucket, range, rare_terms, rate, reverse_nested, random_sampler, sampler, scripted_metric, serial_diff, significant_terms, significant_text, stats, stats_bucket, string_stats, sum, sum_bucket, terms, time_series, top_hits, t_test, top_metrics, value_count, weighted_avg, variable_width_histogram }>)*
** *`collapse` (Optional, { field, inner_hits, max_concurrent_group_searches, collapse })*
** *`explain` (Optional, boolean)*: If true, returns detailed information about score computation as part of a hit.
** *`ext` (Optional, Record<string, User-defined value>)*: Configuration of search extensions defined by Elasticsearch plugins.
** *`from` (Optional, number)*: Starting document offset. By default, you cannot page through more than 10,000
hits using the from and size parameters. To page through more hits, use the
search_after parameter.
** *`highlight` (Optional, { encoder, fields })*
** *`track_total_hits` (Optional, boolean | number)*: Number of hits matching the query to count accurately. If true, the exact
number of hits is returned at the cost of some performance. If false, the
response does not include the total number of hits matching the query.
Defaults to 10,000 hits.
** *`indices_boost` (Optional, Record<string, number>[])*: Boosts the _score of documents from specified indices.
** *`docvalue_fields` (Optional, { field, format, include_unmapped }[])*: Array of wildcard (*) patterns. The request returns doc values for field
names matching these patterns in the hits.fields property of the response.
** *`min_score` (Optional, number)*: Minimum _score for matching documents. Documents with a lower _score are
not included in the search results.
** *`post_filter` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*
** *`profile` (Optional, boolean)*
** *`query` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Defines the search definition using the Query DSL.
** *`rescore` (Optional, { window_size, query, learning_to_rank } | { window_size, query, learning_to_rank }[])*
** *`script_fields` (Optional, Record<string, { script, ignore_failure }>)*: Retrieve a script evaluation (based on different fields) for each hit.
** *`search_after` (Optional, number | number | string | boolean | null | User-defined value[])*
** *`size` (Optional, number)*: The number of hits to return. By default, you cannot page through more
than 10,000 hits using the from and size parameters. To page through more
hits, use the search_after parameter.
** *`slice` (Optional, { field, id, max })*
** *`sort` (Optional, string | { _score, _doc, _geo_distance, _script } | string | { _score, _doc, _geo_distance, _script }[])*
** *`_source` (Optional, boolean | { excludes, includes })*: Indicates which source fields are returned for matching documents. These
fields are returned in the hits._source property of the search response.
** *`fields` (Optional, { field, format, include_unmapped }[])*: Array of wildcard (*) patterns. The request returns values for field names
matching these patterns in the hits.fields property of the response.
** *`suggest` (Optional, { text })*
** *`terminate_after` (Optional, number)*: Maximum number of documents to collect for each shard. If a query reaches this
limit, Elasticsearch terminates the query early. Elasticsearch collects documents
before sorting. Defaults to 0, which does not terminate query execution early.
** *`timeout` (Optional, string)*: Specifies the period of time to wait for a response from each shard. If no response
is received before the timeout expires, the request fails and returns an error.
Defaults to no timeout.
** *`track_scores` (Optional, boolean)*: If true, calculate and return document scores, even if the scores are not used for sorting.
** *`version` (Optional, boolean)*: If true, returns document version as part of a hit.
** *`seq_no_primary_term` (Optional, boolean)*: If true, returns sequence number and primary term of the last modification
of each hit. See Optimistic concurrency control.
** *`stored_fields` (Optional, string | string[])*: List of stored fields to return as part of a hit. If no fields are specified,
no stored fields are included in the response. If this field is specified, the _source
parameter defaults to false. You can pass _source: true to return both source fields
and stored fields in the search response.
** *`pit` (Optional, { id, keep_alive })*: Limits the search to a point in time (PIT). If you provide a PIT, you
cannot specify an <index> in the request path.
** *`runtime_mappings` (Optional, Record<string, { fields, fetch_fields, format, input_field, target_field, target_index, script, type }>)*: Defines one or more runtime fields in the search request. These fields take
precedence over mapped fields with the same name.
** *`stats` (Optional, string[])*: Stats groups to associate with the search. Each group maintains a statistics
aggregation for its associated searches. You can retrieve these stats using
the indices stats API.
** *`allow_no_indices` (Optional, boolean)*
** *`analyzer` (Optional, string)*
** *`analyze_wildcard` (Optional, boolean)*
** *`batched_reduce_size` (Optional, number)*
** *`ccs_minimize_roundtrips` (Optional, boolean)*
** *`default_operator` (Optional, Enum("and" | "or"))*
** *`df` (Optional, string)*
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*
** *`ignore_throttled` (Optional, boolean)*
** *`ignore_unavailable` (Optional, boolean)*
** *`lenient` (Optional, boolean)*
** *`max_concurrent_shard_requests` (Optional, number)*
** *`min_compatible_shard_node` (Optional, string)*
** *`preference` (Optional, string)*
** *`pre_filter_shard_size` (Optional, number)*
** *`request_cache` (Optional, boolean)*
** *`routing` (Optional, string)*
** *`scroll` (Optional, string | -1 | 0)*
** *`search_type` (Optional, Enum("query_then_fetch" | "dfs_query_then_fetch"))*
** *`suggest_field` (Optional, string)*: Specifies which field to use for suggestions.
** *`suggest_mode` (Optional, Enum("missing" | "popular" | "always"))*
** *`suggest_size` (Optional, number)*
** *`suggest_text` (Optional, string)*: The source text for which the suggestions should be returned.
** *`typed_keys` (Optional, boolean)*
** *`rest_total_hits_as_int` (Optional, boolean)*
** *`_source_excludes` (Optional, string | string[])*
** *`_source_includes` (Optional, string | string[])*
** *`q` (Optional, string)*
** *`wait_for_checkpoints` (Optional, number[])*: A comma separated list of checkpoints. When configured, the search API will only be executed on a shard
after the relevant checkpoint has become visible for search. Defaults to an empty list which will cause
Elasticsearch to immediately execute the search.
** *`allow_partial_search_results` (Optional, boolean)*: If true, returns partial results if there are shard request timeouts or [shard failures](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-replication.html#shard-failures). If false, returns
an error with no partial results. Defaults to the configured cluster setting `search.default_allow_partial_results`
which is true by default.

[discrete]
=== graph
[discrete]
==== explore
Extracts and summarizes information about the documents and terms in an Elasticsearch data stream or index.

{ref}/graph-explore-api.html[Endpoint documentation]
[source,ts]
----
client.graph.explore({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: Name of the index.
** *`connections` (Optional, { connections, query, vertices })*: Specifies or more fields from which you want to extract terms that are associated with the specified vertices.
** *`controls` (Optional, { sample_diversity, sample_size, timeout, use_significance })*: Direct the Graph API how to build the graph.
** *`query` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: A seed query that identifies the documents of interest. Can be any valid Elasticsearch query.
** *`vertices` (Optional, { exclude, field, include, min_doc_count, shard_min_doc_count, size }[])*: Specifies one or more fields that contain the terms you want to include in the graph as vertices.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`timeout` (Optional, string | -1 | 0)*: Specifies the period of time to wait for a response from each shard.
If no response is received before the timeout expires, the request fails and returns an error.
Defaults to no timeout.

[discrete]
=== ilm
[discrete]
==== delete_lifecycle
Deletes the specified lifecycle policy definition. You cannot delete policies that are currently in use. If the policy is being used to manage any indices, the request fails and returns an error.

{ref}/ilm-delete-lifecycle.html[Endpoint documentation]
[source,ts]
----
client.ilm.deleteLifecycle({ policy })
----

[discrete]
==== Arguments

* *Request (object):*
** *`policy` (string)*: Identifier for the policy.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== explain_lifecycle
Retrieves information about the index’s current lifecycle state, such as the currently executing phase, action, and step. Shows when the index entered each one, the definition of the running phase, and information about any failures.

{ref}/ilm-explain-lifecycle.html[Endpoint documentation]
[source,ts]
----
client.ilm.explainLifecycle({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: List of data streams, indices, and aliases to target. Supports wildcards (`*`).
To target all data streams and indices, use `*` or `_all`.
** *`only_errors` (Optional, boolean)*: Filters the returned indices to only indices that are managed by ILM and are in an error state, either due to an encountering an error while executing the policy, or attempting to use a policy that does not exist.
** *`only_managed` (Optional, boolean)*: Filters the returned indices to only indices that are managed by ILM.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== get_lifecycle
Retrieves a lifecycle policy.

{ref}/ilm-get-lifecycle.html[Endpoint documentation]
[source,ts]
----
client.ilm.getLifecycle({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`policy` (Optional, string)*: Identifier for the policy.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== get_status
Retrieves the current index lifecycle management (ILM) status.

{ref}/ilm-get-status.html[Endpoint documentation]
[source,ts]
----
client.ilm.getStatus()
----


[discrete]
==== migrate_to_data_tiers
Switches the indices, ILM policies, and legacy, composable and component templates from using custom node attributes and
attribute-based allocation filters to using data tiers, and optionally deletes one legacy index template.+
Using node roles enables ILM to automatically move the indices between data tiers.

{ref}/ilm-migrate-to-data-tiers.html[Endpoint documentation]
[source,ts]
----
client.ilm.migrateToDataTiers({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`legacy_template_to_delete` (Optional, string)*
** *`node_attribute` (Optional, string)*
** *`dry_run` (Optional, boolean)*: If true, simulates the migration from node attributes based allocation filters to data tiers, but does not perform the migration.
This provides a way to retrieve the indices and ILM policies that need to be migrated.

[discrete]
==== move_to_step
Manually moves an index into the specified step and executes that step.

{ref}/ilm-move-to-step.html[Endpoint documentation]
[source,ts]
----
client.ilm.moveToStep({ index, current_step, next_step })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: The name of the index whose lifecycle step is to change
** *`current_step` ({ action, name, phase })*
** *`next_step` ({ action, name, phase })*

[discrete]
==== put_lifecycle
Creates a lifecycle policy. If the specified policy exists, the policy is replaced and the policy version is incremented.

{ref}/ilm-put-lifecycle.html[Endpoint documentation]
[source,ts]
----
client.ilm.putLifecycle({ policy })
----

[discrete]
==== Arguments

* *Request (object):*
** *`policy` (string)*: Identifier for the policy.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== remove_policy
Removes the assigned lifecycle policy and stops managing the specified index

{ref}/ilm-remove-policy.html[Endpoint documentation]
[source,ts]
----
client.ilm.removePolicy({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: The name of the index to remove policy on

[discrete]
==== retry
Retries executing the policy for an index that is in the ERROR step.

{ref}/ilm-retry-policy.html[Endpoint documentation]
[source,ts]
----
client.ilm.retry({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: The name of the indices (comma-separated) whose failed lifecycle step is to be retry

[discrete]
==== start
Start the index lifecycle management (ILM) plugin.

{ref}/ilm-start.html[Endpoint documentation]
[source,ts]
----
client.ilm.start({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`master_timeout` (Optional, string | -1 | 0)*
** *`timeout` (Optional, string | -1 | 0)*

[discrete]
==== stop
Halts all lifecycle management operations and stops the index lifecycle management (ILM) plugin

{ref}/ilm-stop.html[Endpoint documentation]
[source,ts]
----
client.ilm.stop({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`master_timeout` (Optional, string | -1 | 0)*
** *`timeout` (Optional, string | -1 | 0)*

[discrete]
=== indices
[discrete]
==== add_block
Add an index block.
Limits the operations allowed on an index by blocking specific operation types.

{ref}/index-modules-blocks.html[Endpoint documentation]
[source,ts]
----
client.indices.addBlock({ index, block })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: A comma separated list of indices to add a block to
** *`block` (Enum("metadata" | "read" | "read_only" | "write"))*: The block to add (one of read, write, read_only or metadata)
** *`allow_no_indices` (Optional, boolean)*: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes `_all` string or when no indices have been specified)
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Whether to expand wildcard expression to concrete indices that are open, closed or both.
** *`ignore_unavailable` (Optional, boolean)*: Whether specified concrete indices should be ignored when unavailable (missing or closed)
** *`master_timeout` (Optional, string | -1 | 0)*: Specify timeout for connection to master
** *`timeout` (Optional, string | -1 | 0)*: Explicit operation timeout

[discrete]
==== analyze
Get tokens from text analysis.
The analyze API performs [analysis](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html) on a text string and returns the resulting tokens.

{ref}/indices-analyze.html[Endpoint documentation]
[source,ts]
----
client.indices.analyze({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string)*: Index used to derive the analyzer.
If specified, the `analyzer` or field parameter overrides this value.
If no index is specified or the index does not have a default analyzer, the analyze API uses the standard analyzer.
** *`analyzer` (Optional, string)*: The name of the analyzer that should be applied to the provided `text`.
This could be a built-in analyzer, or an analyzer that’s been configured in the index.
** *`attributes` (Optional, string[])*: Array of token attributes used to filter the output of the `explain` parameter.
** *`char_filter` (Optional, string | { type, escaped_tags } | { type, mappings, mappings_path } | { type, flags, pattern, replacement } | { type, mode, name } | { type, normalize_kana, normalize_kanji }[])*: Array of character filters used to preprocess characters before the tokenizer.
** *`explain` (Optional, boolean)*: If `true`, the response includes token attributes and additional details.
** *`field` (Optional, string)*: Field used to derive the analyzer.
To use this parameter, you must specify an index.
If specified, the `analyzer` parameter overrides this value.
** *`filter` (Optional, string | { type, preserve_original } | { type, common_words, common_words_path, ignore_case, query_mode } | { type, filter, script } | { type, delimiter, encoding } | { type, max_gram, min_gram, side, preserve_original } | { type, articles, articles_path, articles_case } | { type, max_output_size, separator } | { type, dedup, dictionary, locale, longest_only } | { type } | { type, mode, types } | { type, keep_words, keep_words_case, keep_words_path } | { type, ignore_case, keywords, keywords_path, keywords_pattern } | { type } | { type, max, min } | { type, consume_all_tokens, max_token_count } | { type, language } | { type, filters, preserve_original } | { type, max_gram, min_gram, preserve_original } | { type, stoptags } | { type, patterns, preserve_original } | { type, all, flags, pattern, replacement } | { type } | { type, script } | { type } | { type } | { type, filler_token, max_shingle_size, min_shingle_size, output_unigrams, output_unigrams_if_no_shingles, token_separator } | { type, language } | { type, rules, rules_path } | { type, language } | { type, ignore_case, remove_trailing, stopwords, stopwords_path } | { type, expand, format, lenient, synonyms, synonyms_path, synonyms_set, tokenizer, updateable } | { type, expand, format, lenient, synonyms, synonyms_path, synonyms_set, tokenizer, updateable } | { type } | { type, length } | { type, only_on_same_position } | { type } | { type, adjust_offsets, catenate_all, catenate_numbers, catenate_words, generate_number_parts, generate_word_parts, ignore_keywords, preserve_original, protected_words, protected_words_path, split_on_case_change, split_on_numerics, stem_english_possessive, type_table, type_table_path } | { type, catenate_all, catenate_numbers, catenate_words, generate_number_parts, generate_word_parts, preserve_original, protected_words, protected_words_path, split_on_case_change, split_on_numerics, stem_english_possessive, type_table, type_table_path } | { type, minimum_length } | { type, use_romaji } | { type, stoptags } | { type, alternate, case_first, case_level, country, decomposition, hiragana_quaternary_mode, language, numeric, rules, strength, variable_top, variant } | { type, unicode_set_filter } | { type, name } | { type, dir, id } | { type, encoder, languageset, max_code_len, name_type, replace, rule_type } | { type }[])*: Array of token filters used to apply after the tokenizer.
** *`normalizer` (Optional, string)*: Normalizer to use to convert text into a single token.
** *`text` (Optional, string | string[])*: Text to analyze.
If an array of strings is provided, it is analyzed as a multi-value field.
** *`tokenizer` (Optional, string | { type, tokenize_on_chars, max_token_length } | { type, max_token_length } | { type, custom_token_chars, max_gram, min_gram, token_chars } | { type, buffer_size } | { type } | { type } | { type, custom_token_chars, max_gram, min_gram, token_chars } | { type, buffer_size, delimiter, replacement, reverse, skip } | { type, flags, group, pattern } | { type, pattern } | { type, pattern } | { type, max_token_length } | { type } | { type, max_token_length } | { type, max_token_length } | { type, rule_files } | { type, discard_punctuation, mode, nbest_cost, nbest_examples, user_dictionary, user_dictionary_rules, discard_compound_token } | { type, decompound_mode, discard_punctuation, user_dictionary, user_dictionary_rules })*: Tokenizer to use to convert text into tokens.

[discrete]
==== clear_cache
Clears the caches of one or more indices.
For data streams, the API clears the caches of the stream’s backing indices.

{ref}/indices-clearcache.html[Endpoint documentation]
[source,ts]
----
client.indices.clearCache({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit the request.
Supports wildcards (`*`).
To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`fielddata` (Optional, boolean)*: If `true`, clears the fields cache.
Use the `fields` parameter to clear the cache of specific fields only.
** *`fields` (Optional, string | string[])*: List of field names used to limit the `fielddata` parameter.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`query` (Optional, boolean)*: If `true`, clears the query cache.
** *`request` (Optional, boolean)*: If `true`, clears the request cache.

[discrete]
==== clone
Clones an existing index.

{ref}/indices-clone-index.html[Endpoint documentation]
[source,ts]
----
client.indices.clone({ index, target })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: Name of the source index to clone.
** *`target` (string)*: Name of the target index to create.
** *`aliases` (Optional, Record<string, { filter, index_routing, is_hidden, is_write_index, routing, search_routing }>)*: Aliases for the resulting index.
** *`settings` (Optional, Record<string, User-defined value>)*: Configuration options for the target index.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to `all` or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).

[discrete]
==== close
Closes an index.

{ref}/indices-close.html[Endpoint documentation]
[source,ts]
----
client.indices.close({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List or wildcard expression of index names used to limit the request.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to `all` or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).

[discrete]
==== create
Create an index.
Creates a new index.

{ref}/indices-create-index.html[Endpoint documentation]
[source,ts]
----
client.indices.create({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: Name of the index you wish to create.
** *`aliases` (Optional, Record<string, { filter, index_routing, is_hidden, is_write_index, routing, search_routing }>)*: Aliases for the index.
** *`mappings` (Optional, { all_field, date_detection, dynamic, dynamic_date_formats, dynamic_templates, _field_names, index_field, _meta, numeric_detection, properties, _routing, _size, _source, runtime, enabled, subobjects, _data_stream_timestamp })*: Mapping for fields in the index. If specified, this mapping can include:
- Field names
- Field data types
- Mapping parameters
** *`settings` (Optional, { index, mode, routing_path, soft_deletes, sort, number_of_shards, number_of_replicas, number_of_routing_shards, check_on_startup, codec, routing_partition_size, load_fixed_bitset_filters_eagerly, hidden, auto_expand_replicas, merge, search, refresh_interval, max_result_window, max_inner_result_window, max_rescore_window, max_docvalue_fields_search, max_script_fields, max_ngram_diff, max_shingle_diff, blocks, max_refresh_listeners, analyze, highlight, max_terms_count, max_regex_length, routing, gc_deletes, default_pipeline, final_pipeline, lifecycle, provided_name, creation_date, creation_date_string, uuid, version, verified_before_close, format, max_slices_per_scroll, translog, query_string, priority, top_metrics_max_size, analysis, settings, time_series, queries, similarity, mapping, indexing.slowlog, indexing_pressure, store })*: Configuration options for the index.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to `all` or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).

[discrete]
==== create_data_stream
Create a data stream.
Creates a data stream.
You must have a matching index template with data stream enabled.

{ref}/data-streams.html[Endpoint documentation]
[source,ts]
----
client.indices.createDataStream({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Name of the data stream, which must meet the following criteria:
Lowercase only;
Cannot include `\`, `/`, `*`, `?`, `"`, `<`, `>`, `|`, `,`, `#`, `:`, or a space character;
Cannot start with `-`, `_`, `+`, or `.ds-`;
Cannot be `.` or `..`;
Cannot be longer than 255 bytes. Multi-byte characters count towards this limit faster.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== data_streams_stats
Get data stream stats.
Retrieves statistics for one or more data streams.

{ref}/data-streams.html[Endpoint documentation]
[source,ts]
----
client.indices.dataStreamsStats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string)*: List of data streams used to limit the request.
Wildcard expressions (`*`) are supported.
To target all data streams in a cluster, omit this parameter or use `*`.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of data stream that wildcard patterns can match.
Supports a list of values, such as `open,hidden`.

[discrete]
==== delete
Delete indices.
Deletes one or more indices.

{ref}/indices-delete-index.html[Endpoint documentation]
[source,ts]
----
client.indices.delete({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of indices to delete.
You cannot specify index aliases.
By default, this parameter does not support wildcards (`*`) or `_all`.
To use wildcards or `_all`, set the `action.destructive_requires_name` cluster setting to `false`.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== delete_alias
Delete an alias.
Removes a data stream or index from an alias.

{ref}/indices-aliases.html[Endpoint documentation]
[source,ts]
----
client.indices.deleteAlias({ index, name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of data streams or indices used to limit the request.
Supports wildcards (`*`).
** *`name` (string | string[])*: List of aliases to remove.
Supports wildcards (`*`). To remove all aliases, use `*` or `_all`.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== delete_data_lifecycle
Delete data stream lifecycles.
Removes the data stream lifecycle from a data stream, rendering it not managed by the data stream lifecycle.

{ref}/data-streams-delete-lifecycle.html[Endpoint documentation]
[source,ts]
----
client.indices.deleteDataLifecycle({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: A list of data streams of which the data stream lifecycle will be deleted; use `*` to get all data streams
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Whether wildcard expressions should get expanded to open or closed indices (default: open)
** *`master_timeout` (Optional, string | -1 | 0)*: Specify timeout for connection to master
** *`timeout` (Optional, string | -1 | 0)*: Explicit timestamp for the document

[discrete]
==== delete_data_stream
Delete data streams.
Deletes one or more data streams and their backing indices.

{ref}/data-streams.html[Endpoint documentation]
[source,ts]
----
client.indices.deleteDataStream({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: List of data streams to delete. Wildcard (`*`) expressions are supported.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of data stream that wildcard patterns can match. Supports a list of values,such as `open,hidden`.

[discrete]
==== delete_index_template
Delete an index template.
The provided <index-template> may contain multiple template names separated by a comma. If multiple template
names are specified then there is no wildcard support and the provided names should match completely with
existing templates.

{ref}/indices-delete-template.html[Endpoint documentation]
[source,ts]
----
client.indices.deleteIndexTemplate({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: List of index template names used to limit the request. Wildcard (*) expressions are supported.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== delete_template
Deletes a legacy index template.

{ref}/indices-delete-template-v1.html[Endpoint documentation]
[source,ts]
----
client.indices.deleteTemplate({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the legacy index template to delete.
Wildcard (`*`) expressions are supported.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== disk_usage
Analyzes the disk usage of each field of an index or data stream.

{ref}/indices-disk-usage.html[Endpoint documentation]
[source,ts]
----
client.indices.diskUsage({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of data streams, indices, and aliases used to limit the request.
It’s recommended to execute this API with a single index (or the latest backing index of a data stream) as the API consumes resources significantly.
** *`allow_no_indices` (Optional, boolean)*: If false, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
For example, a request targeting `foo*,bar*` returns an error if an index starts with `foo` but no index starts with `bar`.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
** *`flush` (Optional, boolean)*: If `true`, the API performs a flush before analysis.
If `false`, the response may not include uncommitted data.
** *`ignore_unavailable` (Optional, boolean)*: If `true`, missing or closed indices are not included in the response.
** *`run_expensive_tasks` (Optional, boolean)*: Analyzing field disk usage is resource-intensive.
To use the API, this parameter must be set to `true`.

[discrete]
==== downsample
Aggregates a time series (TSDS) index and stores pre-computed statistical summaries (`min`, `max`, `sum`, `value_count` and `avg`) for each metric field grouped by a configured time interval.

{ref}/indices-downsample-data-stream.html[Endpoint documentation]
[source,ts]
----
client.indices.downsample({ index, target_index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: Name of the time series index to downsample.
** *`target_index` (string)*: Name of the index to create.
** *`config` (Optional, { fixed_interval })*

[discrete]
==== exists
Check indices.
Checks if one or more indices, index aliases, or data streams exist.

{ref}/indices-exists.html[Endpoint documentation]
[source,ts]
----
client.indices.exists({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of data streams, indices, and aliases. Supports wildcards (`*`).
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`flat_settings` (Optional, boolean)*: If `true`, returns settings in flat format.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`include_defaults` (Optional, boolean)*: If `true`, return all default settings in the response.
** *`local` (Optional, boolean)*: If `true`, the request retrieves information from the local node only.

[discrete]
==== exists_alias
Check aliases.
Checks if one or more data stream or index aliases exist.

{ref}/indices-aliases.html[Endpoint documentation]
[source,ts]
----
client.indices.existsAlias({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: List of aliases to check. Supports wildcards (`*`).
** *`index` (Optional, string | string[])*: List of data streams or indices used to limit the request. Supports wildcards (`*`).
To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, requests that include a missing data stream or index in the target indices or data streams return an error.
** *`local` (Optional, boolean)*: If `true`, the request retrieves information from the local node only.

[discrete]
==== exists_index_template
Returns information about whether a particular index template exists.

{ref}/index-templates.html[Endpoint documentation]
[source,ts]
----
client.indices.existsIndexTemplate({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: List of index template names used to limit the request. Wildcard (*) expressions are supported.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== exists_template
Check existence of index templates.
Returns information about whether a particular index template exists.

{ref}/indices-template-exists-v1.html[Endpoint documentation]
[source,ts]
----
client.indices.existsTemplate({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: The comma separated names of the index templates
** *`flat_settings` (Optional, boolean)*: Return settings in flat format (default: false)
** *`local` (Optional, boolean)*: Return local information, do not retrieve the state from master node (default: false)
** *`master_timeout` (Optional, string | -1 | 0)*: Explicit operation timeout for connection to master node

[discrete]
==== explain_data_lifecycle
Get the status for a data stream lifecycle.
Retrieves information about an index or data stream’s current data stream lifecycle status, such as time since index creation, time since rollover, the lifecycle configuration managing the index, or any errors encountered during lifecycle execution.

{ref}/data-streams-explain-lifecycle.html[Endpoint documentation]
[source,ts]
----
client.indices.explainDataLifecycle({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: The name of the index to explain
** *`include_defaults` (Optional, boolean)*: indicates if the API should return the default values the system uses for the index's lifecycle
** *`master_timeout` (Optional, string | -1 | 0)*: Specify timeout for connection to master

[discrete]
==== field_usage_stats
Returns field usage information for each shard and field of an index.

{ref}/field-usage-stats.html[Endpoint documentation]
[source,ts]
----
client.indices.fieldUsageStats({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List or wildcard expression of index names used to limit the request.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
For example, a request targeting `foo*,bar*` returns an error if an index starts with `foo` but no index starts with `bar`.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
** *`ignore_unavailable` (Optional, boolean)*: If `true`, missing or closed indices are not included in the response.
** *`fields` (Optional, string | string[])*: List or wildcard expressions of fields to include in the statistics.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to all or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).

[discrete]
==== flush
Flushes one or more data streams or indices.

{ref}/indices-flush.html[Endpoint documentation]
[source,ts]
----
client.indices.flush({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases to flush.
Supports wildcards (`*`).
To flush all data streams and indices, omit this parameter or use `*` or `_all`.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`force` (Optional, boolean)*: If `true`, the request forces a flush even if there are no changes to commit to the index.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`wait_if_ongoing` (Optional, boolean)*: If `true`, the flush operation blocks until execution when another flush operation is running.
If `false`, Elasticsearch returns an error if you request a flush when another flush operation is running.

[discrete]
==== forcemerge
Performs the force merge operation on one or more indices.

{ref}/indices-forcemerge.html[Endpoint documentation]
[source,ts]
----
client.indices.forcemerge({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: A list of index names; use `_all` or empty string to perform the operation on all indices
** *`allow_no_indices` (Optional, boolean)*: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes `_all` string or when no indices have been specified)
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Whether to expand wildcard expression to concrete indices that are open, closed or both.
** *`flush` (Optional, boolean)*: Specify whether the index should be flushed after performing the operation (default: true)
** *`ignore_unavailable` (Optional, boolean)*: Whether specified concrete indices should be ignored when unavailable (missing or closed)
** *`max_num_segments` (Optional, number)*: The number of segments the index should be merged into (default: dynamic)
** *`only_expunge_deletes` (Optional, boolean)*: Specify whether the operation should only expunge deleted documents
** *`wait_for_completion` (Optional, boolean)*: Should the request wait until the force merge is completed.

[discrete]
==== get
Get index information.
Returns information about one or more indices. For data streams, the API returns information about the
stream’s backing indices.

{ref}/indices-get-index.html[Endpoint documentation]
[source,ts]
----
client.indices.get({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of data streams, indices, and index aliases used to limit the request.
Wildcard expressions (*) are supported.
** *`allow_no_indices` (Optional, boolean)*: If false, the request returns an error if any wildcard expression, index alias, or _all value targets only
missing or closed indices. This behavior applies even if the request targets other open indices. For example,
a request targeting foo*,bar* returns an error if an index starts with foo but no index starts with bar.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard expressions can match. If the request can target data streams, this argument
determines whether wildcard expressions match hidden data streams. Supports a list of values,
such as open,hidden.
** *`flat_settings` (Optional, boolean)*: If true, returns settings in flat format.
** *`ignore_unavailable` (Optional, boolean)*: If false, requests that target a missing index return an error.
** *`include_defaults` (Optional, boolean)*: If true, return all default settings in the response.
** *`local` (Optional, boolean)*: If true, the request retrieves information from the local node only. Defaults to false, which means information is retrieved from the master node.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`features` (Optional, { name, description } | { name, description }[])*: Return only information on specified index features

[discrete]
==== get_alias
Get aliases.
Retrieves information for one or more data stream or index aliases.

{ref}/indices-aliases.html[Endpoint documentation]
[source,ts]
----
client.indices.getAlias({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string | string[])*: List of aliases to retrieve.
Supports wildcards (`*`).
To retrieve all aliases, omit this parameter or use `*` or `_all`.
** *`index` (Optional, string | string[])*: List of data streams or indices used to limit the request.
Supports wildcards (`*`).
To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`local` (Optional, boolean)*: If `true`, the request retrieves information from the local node only.

[discrete]
==== get_data_lifecycle
Get data stream lifecycles.
Retrieves the data stream lifecycle configuration of one or more data streams.

{ref}/data-streams-get-lifecycle.html[Endpoint documentation]
[source,ts]
----
client.indices.getDataLifecycle({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: List of data streams to limit the request.
Supports wildcards (`*`).
To target all data streams, omit this parameter or use `*` or `_all`.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of data stream that wildcard patterns can match.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`include_defaults` (Optional, boolean)*: If `true`, return all default settings in the response.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== get_data_stream
Get data streams.
Retrieves information about one or more data streams.

{ref}/data-streams.html[Endpoint documentation]
[source,ts]
----
client.indices.getDataStream({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string | string[])*: List of data stream names used to limit the request.
Wildcard (`*`) expressions are supported. If omitted, all data streams are returned.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of data stream that wildcard patterns can match.
Supports a list of values, such as `open,hidden`.
** *`include_defaults` (Optional, boolean)*: If true, returns all relevant default configurations for the index template.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`verbose` (Optional, boolean)*: Whether the maximum timestamp for each data stream should be calculated and returned.

[discrete]
==== get_field_mapping
Get mapping definitions.
Retrieves mapping definitions for one or more fields.
For data streams, the API retrieves field mappings for the stream’s backing indices.

{ref}/indices-get-field-mapping.html[Endpoint documentation]
[source,ts]
----
client.indices.getFieldMapping({ fields })
----

[discrete]
==== Arguments

* *Request (object):*
** *`fields` (string | string[])*: List or wildcard expression of fields used to limit returned information.
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit the request.
Supports wildcards (`*`).
To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`include_defaults` (Optional, boolean)*: If `true`, return all default settings in the response.
** *`local` (Optional, boolean)*: If `true`, the request retrieves information from the local node only.

[discrete]
==== get_index_template
Get index templates.
Returns information about one or more index templates.

{ref}/indices-get-template.html[Endpoint documentation]
[source,ts]
----
client.indices.getIndexTemplate({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string)*: List of index template names used to limit the request. Wildcard (*) expressions are supported.
** *`local` (Optional, boolean)*: If true, the request retrieves information from the local node only. Defaults to false, which means information is retrieved from the master node.
** *`flat_settings` (Optional, boolean)*: If true, returns settings in flat format.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`include_defaults` (Optional, boolean)*: If true, returns all relevant default configurations for the index template.

[discrete]
==== get_mapping
Get mapping definitions.
Retrieves mapping definitions for one or more indices.
For data streams, the API retrieves mappings for the stream’s backing indices.

{ref}/indices-get-mapping.html[Endpoint documentation]
[source,ts]
----
client.indices.getMapping({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit the request.
Supports wildcards (`*`).
To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`local` (Optional, boolean)*: If `true`, the request retrieves information from the local node only.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== get_settings
Get index settings.
Returns setting information for one or more indices. For data streams,
returns setting information for the stream’s backing indices.

{ref}/indices-get-settings.html[Endpoint documentation]
[source,ts]
----
client.indices.getSettings({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit
the request. Supports wildcards (`*`). To target all data streams and
indices, omit this parameter or use `*` or `_all`.
** *`name` (Optional, string | string[])*: List or wildcard expression of settings to retrieve.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index
alias, or `_all` value targets only missing or closed indices. This
behavior applies even if the request targets other open indices. For
example, a request targeting `foo*,bar*` returns an error if an index
starts with foo but no index starts with `bar`.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
** *`flat_settings` (Optional, boolean)*: If `true`, returns settings in flat format.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`include_defaults` (Optional, boolean)*: If `true`, return all default settings in the response.
** *`local` (Optional, boolean)*: If `true`, the request retrieves information from the local node only. If
`false`, information is retrieved from the master node.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is
received before the timeout expires, the request fails and returns an
error.

[discrete]
==== get_template
Get index templates.
Retrieves information about one or more index templates.

{ref}/indices-get-template-v1.html[Endpoint documentation]
[source,ts]
----
client.indices.getTemplate({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string | string[])*: List of index template names used to limit the request.
Wildcard (`*`) expressions are supported.
To return all index templates, omit this parameter or use a value of `_all` or `*`.
** *`flat_settings` (Optional, boolean)*: If `true`, returns settings in flat format.
** *`local` (Optional, boolean)*: If `true`, the request retrieves information from the local node only.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== migrate_to_data_stream
Convert an index alias to a data stream.
Converts an index alias to a data stream.
You must have a matching index template that is data stream enabled.
The alias must meet the following criteria:
The alias must have a write index;
All indices for the alias must have a `@timestamp` field mapping of a `date` or `date_nanos` field type;
The alias must not have any filters;
The alias must not use custom routing.
If successful, the request removes the alias and creates a data stream with the same name.
The indices for the alias become hidden backing indices for the stream.
The write index for the alias becomes the write index for the stream.

{ref}/data-streams.html[Endpoint documentation]
[source,ts]
----
client.indices.migrateToDataStream({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Name of the index alias to convert to a data stream.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== modify_data_stream
Update data streams.
Performs one or more data stream modification actions in a single atomic operation.

{ref}/data-streams.html[Endpoint documentation]
[source,ts]
----
client.indices.modifyDataStream({ actions })
----

[discrete]
==== Arguments

* *Request (object):*
** *`actions` ({ add_backing_index, remove_backing_index }[])*: Actions to perform.

[discrete]
==== open
Opens a closed index.
For data streams, the API opens any closed backing indices.

{ref}/indices-open-close.html[Endpoint documentation]
[source,ts]
----
client.indices.open({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of data streams, indices, and aliases used to limit the request.
Supports wildcards (`*`).
By default, you must explicitly name the indices you using to limit the request.
To limit a request using `_all`, `*`, or other wildcard expressions, change the `action.destructive_requires_name` setting to false.
You can update this setting in the `elasticsearch.yml` file or using the cluster update settings API.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to `all` or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).

[discrete]
==== promote_data_stream
Promotes a data stream from a replicated data stream managed by CCR to a regular data stream

{ref}/data-streams.html[Endpoint documentation]
[source,ts]
----
client.indices.promoteDataStream({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the data stream
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== put_alias
Create or update an alias.
Adds a data stream or index to an alias.

{ref}/indices-aliases.html[Endpoint documentation]
[source,ts]
----
client.indices.putAlias({ index, name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of data streams or indices to add.
Supports wildcards (`*`).
Wildcard patterns that match both data streams and indices return an error.
** *`name` (string)*: Alias to update.
If the alias doesn’t exist, the request creates it.
Index alias names support date math.
** *`filter` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Query used to limit documents the alias can access.
** *`index_routing` (Optional, string)*: Value used to route indexing operations to a specific shard.
If specified, this overwrites the `routing` value for indexing operations.
Data stream aliases don’t support this parameter.
** *`is_write_index` (Optional, boolean)*: If `true`, sets the write index or data stream for the alias.
If an alias points to multiple indices or data streams and `is_write_index` isn’t set, the alias rejects write requests.
If an index alias points to one index and `is_write_index` isn’t set, the index automatically acts as the write index.
Data stream aliases don’t automatically set a write data stream, even if the alias points to one data stream.
** *`routing` (Optional, string)*: Value used to route indexing and search operations to a specific shard.
Data stream aliases don’t support this parameter.
** *`search_routing` (Optional, string)*: Value used to route search operations to a specific shard.
If specified, this overwrites the `routing` value for search operations.
Data stream aliases don’t support this parameter.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== put_data_lifecycle
Update data stream lifecycles.
Update the data stream lifecycle of the specified data streams.

{ref}/data-streams-put-lifecycle.html[Endpoint documentation]
[source,ts]
----
client.indices.putDataLifecycle({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: List of data streams used to limit the request.
Supports wildcards (`*`).
To target all data streams use `*` or `_all`.
** *`data_retention` (Optional, string | -1 | 0)*: If defined, every document added to this data stream will be stored at least for this time frame.
Any time after this duration the document could be deleted.
When empty, every document in this data stream will be stored indefinitely.
** *`downsampling` (Optional, { rounds })*: If defined, every backing index will execute the configured downsampling configuration after the backing
index is not the data stream write index anymore.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of data stream that wildcard patterns can match.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `hidden`, `open`, `closed`, `none`.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is
received before the timeout expires, the request fails and returns an
error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== put_index_template
Create or update an index template.
Index templates define settings, mappings, and aliases that can be applied automatically to new indices.

{ref}/indices-put-template.html[Endpoint documentation]
[source,ts]
----
client.indices.putIndexTemplate({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Index or template name
** *`index_patterns` (Optional, string | string[])*: Name of the index template to create.
** *`composed_of` (Optional, string[])*: An ordered list of component template names.
Component templates are merged in the order specified, meaning that the last component template specified has the highest precedence.
** *`template` (Optional, { aliases, mappings, settings, lifecycle })*: Template to be applied.
It may optionally include an `aliases`, `mappings`, or `settings` configuration.
** *`data_stream` (Optional, { hidden, allow_custom_routing })*: If this object is included, the template is used to create data streams and their backing indices.
Supports an empty object.
Data streams require a matching index template with a `data_stream` object.
** *`priority` (Optional, number)*: Priority to determine index template precedence when a new data stream or index is created.
The index template with the highest priority is chosen.
If no priority is specified the template is treated as though it is of priority 0 (lowest priority).
This number is not automatically generated by Elasticsearch.
** *`version` (Optional, number)*: Version number used to manage index templates externally.
This number is not automatically generated by Elasticsearch.
** *`_meta` (Optional, Record<string, User-defined value>)*: Optional user metadata about the index template.
May have any contents.
This map is not automatically generated by Elasticsearch.
** *`allow_auto_create` (Optional, boolean)*: This setting overrides the value of the `action.auto_create_index` cluster setting.
If set to `true` in a template, then indices can be automatically created using that template even if auto-creation of indices is disabled via `actions.auto_create_index`.
If set to `false`, then indices or data streams matching the template must always be explicitly created, and may never be automatically created.
** *`ignore_missing_component_templates` (Optional, string[])*: The configuration option ignore_missing_component_templates can be used when an index template
references a component template that might not exist
** *`deprecated` (Optional, boolean)*: Marks this index template as deprecated. When creating or updating a non-deprecated index template
that uses deprecated components, Elasticsearch will emit a deprecation warning.
** *`create` (Optional, boolean)*: If `true`, this request cannot replace or update existing index templates.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`cause` (Optional, string)*: User defined reason for creating/updating the index template

[discrete]
==== put_mapping
Update field mappings.
Adds new fields to an existing data stream or index.
You can also use this API to change the search settings of existing fields.
For data streams, these changes are applied to all backing indices by default.

{ref}/indices-put-mapping.html[Endpoint documentation]
[source,ts]
----
client.indices.putMapping({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: A list of index names the mapping should be added to (supports wildcards); use `_all` or omit to add the mapping on all indices.
** *`date_detection` (Optional, boolean)*: Controls whether dynamic date detection is enabled.
** *`dynamic` (Optional, Enum("strict" | "runtime" | true | false))*: Controls whether new fields are added dynamically.
** *`dynamic_date_formats` (Optional, string[])*: If date detection is enabled then new string fields are checked
against 'dynamic_date_formats' and if the value matches then
a new date field is added instead of string.
** *`dynamic_templates` (Optional, Record<string, { mapping, runtime, match, path_match, unmatch, path_unmatch, match_mapping_type, unmatch_mapping_type, match_pattern }> | Record<string, { mapping, runtime, match, path_match, unmatch, path_unmatch, match_mapping_type, unmatch_mapping_type, match_pattern }>[])*: Specify dynamic templates for the mapping.
** *`_field_names` (Optional, { enabled })*: Control whether field names are enabled for the index.
** *`_meta` (Optional, Record<string, User-defined value>)*: A mapping type can have custom meta data associated with it. These are
not used at all by Elasticsearch, but can be used to store
application-specific metadata.
** *`numeric_detection` (Optional, boolean)*: Automatically map strings into numeric data types for all fields.
** *`properties` (Optional, Record<string, { type } | { boost, fielddata, index, null_value, type } | { type, enabled, null_value, boost, coerce, script, on_script_error, ignore_malformed, time_series_metric, analyzer, eager_global_ordinals, index, index_options, index_phrases, index_prefixes, norms, position_increment_gap, search_analyzer, search_quote_analyzer, term_vector, format, precision_step, locale } | { relations, eager_global_ordinals, type } | { boost, eager_global_ordinals, index, index_options, script, on_script_error, normalizer, norms, null_value, similarity, split_queries_on_whitespace, time_series_dimension, type } | { type, fields, meta, copy_to } | { type } | { positive_score_impact, type } | { positive_score_impact, type } | { analyzer, index, index_options, max_shingle_size, norms, search_analyzer, search_quote_analyzer, similarity, term_vector, type } | { analyzer, boost, eager_global_ordinals, fielddata, fielddata_frequency_filter, index, index_options, index_phrases, index_prefixes, norms, position_increment_gap, search_analyzer, search_quote_analyzer, similarity, term_vector, type } | { type } | { type, null_value } | { boost, format, ignore_malformed, index, null_value, precision_step, type } | { boost, fielddata, format, ignore_malformed, index, null_value, precision_step, locale, type } | { type, default_metric, metrics, time_series_metric } | { type, element_type, dims, similarity, index, index_options } | { boost, depth_limit, doc_values, eager_global_ordinals, index, index_options, null_value, similarity, split_queries_on_whitespace, type } | { enabled, include_in_parent, include_in_root, type } | { enabled, subobjects, type } | { type, meta, inference_id } | { type } | { analyzer, contexts, max_input_length, preserve_position_increments, preserve_separators, search_analyzer, type } | { value, type } | { path, type } | { ignore_malformed, type } | { boost, index, ignore_malformed, null_value, on_script_error, script, time_series_dimension, type } | { type } | { analyzer, boost, index, null_value, enable_position_increments, type } | { ignore_malformed, ignore_z_value, null_value, index, on_script_error, script, type } | { coerce, ignore_malformed, ignore_z_value, orientation, strategy, type } | { ignore_malformed, ignore_z_value, null_value, type } | { coerce, ignore_malformed, ignore_z_value, orientation, type } | { type, null_value } | { type, null_value } | { type, null_value } | { type, null_value } | { type, null_value } | { type, null_value } | { type, null_value, scaling_factor } | { type, null_value } | { type, null_value } | { format, type } | { type } | { type } | { type } | { type } | { type } | { type, norms, index_options, index, null_value, rules, language, country, variant, strength, decomposition, alternate, case_level, case_first, numeric, variable_top, hiragana_quaternary_mode }>)*: Mapping for a field. For new fields, this mapping can include:

- Field name
- Field data type
- Mapping parameters
** *`_routing` (Optional, { required })*: Enable making a routing value required on indexed documents.
** *`_source` (Optional, { compress, compress_threshold, enabled, excludes, includes, mode })*: Control whether the _source field is enabled on the index.
** *`runtime` (Optional, Record<string, { fields, fetch_fields, format, input_field, target_field, target_index, script, type }>)*: Mapping of runtime fields for the index.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.
** *`write_index_only` (Optional, boolean)*: If `true`, the mappings are applied only to the current write index for the target.

[discrete]
==== put_settings
Update index settings.
Changes dynamic index settings in real time. For data streams, index setting
changes are applied to all backing indices by default.

{ref}/indices-update-settings.html[Endpoint documentation]
[source,ts]
----
client.indices.putSettings({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit
the request. Supports wildcards (`*`). To target all data streams and
indices, omit this parameter or use `*` or `_all`.
** *`settings` (Optional, { index, mode, routing_path, soft_deletes, sort, number_of_shards, number_of_replicas, number_of_routing_shards, check_on_startup, codec, routing_partition_size, load_fixed_bitset_filters_eagerly, hidden, auto_expand_replicas, merge, search, refresh_interval, max_result_window, max_inner_result_window, max_rescore_window, max_docvalue_fields_search, max_script_fields, max_ngram_diff, max_shingle_diff, blocks, max_refresh_listeners, analyze, highlight, max_terms_count, max_regex_length, routing, gc_deletes, default_pipeline, final_pipeline, lifecycle, provided_name, creation_date, creation_date_string, uuid, version, verified_before_close, format, max_slices_per_scroll, translog, query_string, priority, top_metrics_max_size, analysis, settings, time_series, queries, similarity, mapping, indexing.slowlog, indexing_pressure, store })*
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index
alias, or `_all` value targets only missing or closed indices. This
behavior applies even if the request targets other open indices. For
example, a request targeting `foo*,bar*` returns an error if an index
starts with `foo` but no index starts with `bar`.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match. If the request can target
data streams, this argument determines whether wildcard expressions match
hidden data streams. Supports a list of values, such as
`open,hidden`.
** *`flat_settings` (Optional, boolean)*: If `true`, returns settings in flat format.
** *`ignore_unavailable` (Optional, boolean)*: If `true`, returns settings in flat format.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is
received before the timeout expires, the request fails and returns an
error.
** *`preserve_existing` (Optional, boolean)*: If `true`, existing index settings remain unchanged.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the
 timeout expires, the request fails and returns an error.

[discrete]
==== put_template
Create or update an index template.
Index templates define settings, mappings, and aliases that can be applied automatically to new indices.

{ref}/indices-templates-v1.html[Endpoint documentation]
[source,ts]
----
client.indices.putTemplate({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the template
** *`aliases` (Optional, Record<string, { filter, index_routing, is_hidden, is_write_index, routing, search_routing }>)*: Aliases for the index.
** *`index_patterns` (Optional, string | string[])*: Array of wildcard expressions used to match the names
of indices during creation.
** *`mappings` (Optional, { all_field, date_detection, dynamic, dynamic_date_formats, dynamic_templates, _field_names, index_field, _meta, numeric_detection, properties, _routing, _size, _source, runtime, enabled, subobjects, _data_stream_timestamp })*: Mapping for fields in the index.
** *`order` (Optional, number)*: Order in which Elasticsearch applies this template if index
matches multiple templates.

Templates with lower 'order' values are merged first. Templates with higher
'order' values are merged later, overriding templates with lower values.
** *`settings` (Optional, { index, mode, routing_path, soft_deletes, sort, number_of_shards, number_of_replicas, number_of_routing_shards, check_on_startup, codec, routing_partition_size, load_fixed_bitset_filters_eagerly, hidden, auto_expand_replicas, merge, search, refresh_interval, max_result_window, max_inner_result_window, max_rescore_window, max_docvalue_fields_search, max_script_fields, max_ngram_diff, max_shingle_diff, blocks, max_refresh_listeners, analyze, highlight, max_terms_count, max_regex_length, routing, gc_deletes, default_pipeline, final_pipeline, lifecycle, provided_name, creation_date, creation_date_string, uuid, version, verified_before_close, format, max_slices_per_scroll, translog, query_string, priority, top_metrics_max_size, analysis, settings, time_series, queries, similarity, mapping, indexing.slowlog, indexing_pressure, store })*: Configuration options for the index.
** *`version` (Optional, number)*: Version number used to manage index templates externally. This number
is not automatically generated by Elasticsearch.
** *`create` (Optional, boolean)*: If true, this request cannot replace or update existing index templates.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is
received before the timeout expires, the request fails and returns an error.
** *`cause` (Optional, string)*

[discrete]
==== recovery
Returns information about ongoing and completed shard recoveries for one or more indices.
For data streams, the API returns information for the stream’s backing indices.

{ref}/indices-recovery.html[Endpoint documentation]
[source,ts]
----
client.indices.recovery({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit the request.
Supports wildcards (`*`).
To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`active_only` (Optional, boolean)*: If `true`, the response only includes ongoing shard recoveries.
** *`detailed` (Optional, boolean)*: If `true`, the response includes detailed information about shard recoveries.

[discrete]
==== refresh
Refresh an index.
A refresh makes recent operations performed on one or more indices available for search.
For data streams, the API runs the refresh operation on the stream’s backing indices.

{ref}/indices-refresh.html[Endpoint documentation]
[source,ts]
----
client.indices.refresh({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit the request.
Supports wildcards (`*`).
To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.

[discrete]
==== reload_search_analyzers
Reloads an index's search analyzers and their resources.

{ref}/indices-reload-analyzers.html[Endpoint documentation]
[source,ts]
----
client.indices.reloadSearchAnalyzers({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: A list of index names to reload analyzers for
** *`allow_no_indices` (Optional, boolean)*: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes `_all` string or when no indices have been specified)
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Whether to expand wildcard expression to concrete indices that are open, closed or both.
** *`ignore_unavailable` (Optional, boolean)*: Whether specified concrete indices should be ignored when unavailable (missing or closed)

[discrete]
==== resolve_cluster
Resolves the specified index expressions to return information about each cluster, including
the local cluster, if included.
Multiple patterns and remote clusters are supported.

{ref}/indices-resolve-cluster-api.html[Endpoint documentation]
[source,ts]
----
client.indices.resolveCluster({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: Comma-separated name(s) or index pattern(s) of the indices, aliases, and data streams to resolve.
Resources on remote clusters can be specified using the `<cluster>`:`<name>` syntax.
** *`allow_no_indices` (Optional, boolean)*: If false, the request returns an error if any wildcard expression, index alias, or _all value targets only missing
or closed indices. This behavior applies even if the request targets other open indices. For example, a request
targeting foo*,bar* returns an error if an index starts with foo but no index starts with bar.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_throttled` (Optional, boolean)*: If true, concrete, expanded or aliased indices are ignored when frozen. Defaults to false.
** *`ignore_unavailable` (Optional, boolean)*: If false, the request returns an error if it targets a missing or closed index. Defaults to false.

[discrete]
==== resolve_index
Resolves the specified name(s) and/or index patterns for indices, aliases, and data streams.
Multiple patterns and remote clusters are supported.

{ref}/indices-resolve-index-api.html[Endpoint documentation]
[source,ts]
----
client.indices.resolveIndex({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: Comma-separated name(s) or index pattern(s) of the indices, aliases, and data streams to resolve.
Resources on remote clusters can be specified using the `<cluster>`:`<name>` syntax.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
For example, a request targeting `foo*,bar*` returns an error if an index starts with `foo` but no index starts with `bar`.

[discrete]
==== rollover
Roll over to a new index.
Creates a new index for a data stream or index alias.

{ref}/indices-rollover-index.html[Endpoint documentation]
[source,ts]
----
client.indices.rollover({ alias })
----

[discrete]
==== Arguments

* *Request (object):*
** *`alias` (string)*: Name of the data stream or index alias to roll over.
** *`new_index` (Optional, string)*: Name of the index to create.
Supports date math.
Data streams do not support this parameter.
** *`aliases` (Optional, Record<string, { filter, index_routing, is_hidden, is_write_index, routing, search_routing }>)*: Aliases for the target index.
Data streams do not support this parameter.
** *`conditions` (Optional, { min_age, max_age, max_age_millis, min_docs, max_docs, max_size, max_size_bytes, min_size, min_size_bytes, max_primary_shard_size, max_primary_shard_size_bytes, min_primary_shard_size, min_primary_shard_size_bytes, max_primary_shard_docs, min_primary_shard_docs })*: Conditions for the rollover.
If specified, Elasticsearch only performs the rollover if the current index satisfies these conditions.
If this parameter is not specified, Elasticsearch performs the rollover unconditionally.
If conditions are specified, at least one of them must be a `max_*` condition.
The index will rollover if any `max_*` condition is satisfied and all `min_*` conditions are satisfied.
** *`mappings` (Optional, { all_field, date_detection, dynamic, dynamic_date_formats, dynamic_templates, _field_names, index_field, _meta, numeric_detection, properties, _routing, _size, _source, runtime, enabled, subobjects, _data_stream_timestamp })*: Mapping for fields in the index.
If specified, this mapping can include field names, field data types, and mapping paramaters.
** *`settings` (Optional, Record<string, User-defined value>)*: Configuration options for the index.
Data streams do not support this parameter.
** *`dry_run` (Optional, boolean)*: If `true`, checks whether the current index satisfies the specified conditions but does not perform a rollover.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to all or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).

[discrete]
==== segments
Returns low-level information about the Lucene segments in index shards.
For data streams, the API returns information about the stream’s backing indices.

{ref}/indices-segments.html[Endpoint documentation]
[source,ts]
----
client.indices.segments({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit the request.
Supports wildcards (`*`).
To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`verbose` (Optional, boolean)*: If `true`, the request returns a verbose response.

[discrete]
==== shard_stores
Retrieves store information about replica shards in one or more indices.
For data streams, the API retrieves store information for the stream’s backing indices.

{ref}/indices-shards-stores.html[Endpoint documentation]
[source,ts]
----
client.indices.shardStores({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit the request.
** *`allow_no_indices` (Optional, boolean)*: If false, the request returns an error if any wildcard expression, index alias, or _all
value targets only missing or closed indices. This behavior applies even if the request
targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match. If the request can target data streams,
this argument determines whether wildcard expressions match hidden data streams.
** *`ignore_unavailable` (Optional, boolean)*: If true, missing or closed indices are not included in the response.
** *`status` (Optional, Enum("green" | "yellow" | "red" | "all") | Enum("green" | "yellow" | "red" | "all")[])*: List of shard health statuses used to limit the request.

[discrete]
==== shrink
Shrinks an existing index into a new index with fewer primary shards.

{ref}/indices-shrink-index.html[Endpoint documentation]
[source,ts]
----
client.indices.shrink({ index, target })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: Name of the source index to shrink.
** *`target` (string)*: Name of the target index to create.
** *`aliases` (Optional, Record<string, { filter, index_routing, is_hidden, is_write_index, routing, search_routing }>)*: The key is the alias name.
Index alias names support date math.
** *`settings` (Optional, Record<string, User-defined value>)*: Configuration options for the target index.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to `all` or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).

[discrete]
==== simulate_index_template
Simulate an index.
Returns the index configuration that would be applied to the specified index from an existing index template.

{ref}/indices-simulate-index.html[Endpoint documentation]
[source,ts]
----
client.indices.simulateIndexTemplate({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Name of the index to simulate
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`include_defaults` (Optional, boolean)*: If true, returns all relevant default configurations for the index template.

[discrete]
==== simulate_template
Simulate an index template.
Returns the index configuration that would be applied by a particular index template.

{ref}/indices-simulate-template.html[Endpoint documentation]
[source,ts]
----
client.indices.simulateTemplate({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string)*: Name of the index template to simulate. To test a template configuration before you add it to the cluster, omit
this parameter and specify the template configuration in the request body.
** *`allow_auto_create` (Optional, boolean)*: This setting overrides the value of the `action.auto_create_index` cluster setting.
If set to `true` in a template, then indices can be automatically created using that template even if auto-creation of indices is disabled via `actions.auto_create_index`.
If set to `false`, then indices or data streams matching the template must always be explicitly created, and may never be automatically created.
** *`index_patterns` (Optional, string | string[])*: Array of wildcard (`*`) expressions used to match the names of data streams and indices during creation.
** *`composed_of` (Optional, string[])*: An ordered list of component template names.
Component templates are merged in the order specified, meaning that the last component template specified has the highest precedence.
** *`template` (Optional, { aliases, mappings, settings, lifecycle })*: Template to be applied.
It may optionally include an `aliases`, `mappings`, or `settings` configuration.
** *`data_stream` (Optional, { hidden, allow_custom_routing })*: If this object is included, the template is used to create data streams and their backing indices.
Supports an empty object.
Data streams require a matching index template with a `data_stream` object.
** *`priority` (Optional, number)*: Priority to determine index template precedence when a new data stream or index is created.
The index template with the highest priority is chosen.
If no priority is specified the template is treated as though it is of priority 0 (lowest priority).
This number is not automatically generated by Elasticsearch.
** *`version` (Optional, number)*: Version number used to manage index templates externally.
This number is not automatically generated by Elasticsearch.
** *`_meta` (Optional, Record<string, User-defined value>)*: Optional user metadata about the index template.
May have any contents.
This map is not automatically generated by Elasticsearch.
** *`ignore_missing_component_templates` (Optional, string[])*: The configuration option ignore_missing_component_templates can be used when an index template
references a component template that might not exist
** *`deprecated` (Optional, boolean)*: Marks this index template as deprecated. When creating or updating a non-deprecated index template
that uses deprecated components, Elasticsearch will emit a deprecation warning.
** *`create` (Optional, boolean)*: If true, the template passed in the body is only used if no existing templates match the same index patterns. If false, the simulation uses the template with the highest priority. Note that the template is not permanently added or updated in either case; it is only used for the simulation.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`include_defaults` (Optional, boolean)*: If true, returns all relevant default configurations for the index template.

[discrete]
==== split
Splits an existing index into a new index with more primary shards.

{ref}/indices-split-index.html[Endpoint documentation]
[source,ts]
----
client.indices.split({ index, target })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: Name of the source index to split.
** *`target` (string)*: Name of the target index to create.
** *`aliases` (Optional, Record<string, { filter, index_routing, is_hidden, is_write_index, routing, search_routing }>)*: Aliases for the resulting index.
** *`settings` (Optional, Record<string, User-defined value>)*: Configuration options for the target index.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to `all` or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).

[discrete]
==== stats
Returns statistics for one or more indices.
For data streams, the API retrieves statistics for the stream’s backing indices.

{ref}/indices-stats.html[Endpoint documentation]
[source,ts]
----
client.indices.stats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`metric` (Optional, string | string[])*: Limit the information returned the specific metrics.
** *`index` (Optional, string | string[])*: A list of index names; use `_all` or empty string to perform the operation on all indices
** *`completion_fields` (Optional, string | string[])*: List or wildcard expressions of fields to include in fielddata and suggest statistics.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match. If the request can target data streams, this argument
determines whether wildcard expressions match hidden data streams. Supports a list of values,
such as `open,hidden`.
** *`fielddata_fields` (Optional, string | string[])*: List or wildcard expressions of fields to include in fielddata statistics.
** *`fields` (Optional, string | string[])*: List or wildcard expressions of fields to include in the statistics.
** *`forbid_closed_indices` (Optional, boolean)*: If true, statistics are not collected from closed indices.
** *`groups` (Optional, string | string[])*: List of search groups to include in the search statistics.
** *`include_segment_file_sizes` (Optional, boolean)*: If true, the call reports the aggregated disk usage of each one of the Lucene index files (only applies if segment stats are requested).
** *`include_unloaded_segments` (Optional, boolean)*: If true, the response includes information from segments that are not loaded into memory.
** *`level` (Optional, Enum("cluster" | "indices" | "shards"))*: Indicates whether statistics are aggregated at the cluster, index, or shard level.

[discrete]
==== unfreeze
Unfreezes an index.

{ref}/unfreeze-index-api.html[Endpoint documentation]
[source,ts]
----
client.indices.unfreeze({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: Identifier for the index.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.
** *`wait_for_active_shards` (Optional, string)*: The number of shard copies that must be active before proceeding with the operation.
Set to `all` or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).

[discrete]
==== update_aliases
Create or update an alias.
Adds a data stream or index to an alias.

{ref}/indices-aliases.html[Endpoint documentation]
[source,ts]
----
client.indices.updateAliases({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`actions` (Optional, { add_backing_index, remove_backing_index }[])*: Actions to perform.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== validate_query
Validate a query.
Validates a query without running it.

{ref}/search-validate.html[Endpoint documentation]
[source,ts]
----
client.indices.validateQuery({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases to search.
Supports wildcards (`*`).
To search all data streams or indices, omit this parameter or use `*` or `_all`.
** *`query` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Query in the Lucene query string syntax.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`all_shards` (Optional, boolean)*: If `true`, the validation is executed on all shards instead of one random shard per index.
** *`analyzer` (Optional, string)*: Analyzer to use for the query string.
This parameter can only be used when the `q` query string parameter is specified.
** *`analyze_wildcard` (Optional, boolean)*: If `true`, wildcard and prefix queries are analyzed.
** *`default_operator` (Optional, Enum("and" | "or"))*: The default operator for query string query: `AND` or `OR`.
** *`df` (Optional, string)*: Field to use as default where no field prefix is given in the query string.
This parameter can only be used when the `q` query string parameter is specified.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`explain` (Optional, boolean)*: If `true`, the response returns detailed information if an error has occurred.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`lenient` (Optional, boolean)*: If `true`, format-based query failures (such as providing text to a numeric field) in the query string will be ignored.
** *`rewrite` (Optional, boolean)*: If `true`, returns a more detailed explanation showing the actual Lucene query that will be executed.
** *`q` (Optional, string)*: Query in the Lucene query string syntax.

[discrete]
=== inference
[discrete]
==== delete
Delete an inference endpoint

{ref}/delete-inference-api.html[Endpoint documentation]
[source,ts]
----
client.inference.delete({ inference_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`inference_id` (string)*: The inference Id
** *`task_type` (Optional, Enum("sparse_embedding" | "text_embedding" | "rerank" | "completion"))*: The task type
** *`dry_run` (Optional, boolean)*: When true, the endpoint is not deleted, and a list of ingest processors which reference this endpoint is returned
** *`force` (Optional, boolean)*: When true, the inference endpoint is forcefully deleted even if it is still being used by ingest processors or semantic text fields

[discrete]
==== get
Get an inference endpoint

{ref}/get-inference-api.html[Endpoint documentation]
[source,ts]
----
client.inference.get({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`task_type` (Optional, Enum("sparse_embedding" | "text_embedding" | "rerank" | "completion"))*: The task type
** *`inference_id` (Optional, string)*: The inference Id

[discrete]
==== inference
Perform inference on the service

{ref}/post-inference-api.html[Endpoint documentation]
[source,ts]
----
client.inference.inference({ inference_id, input })
----

[discrete]
==== Arguments

* *Request (object):*
** *`inference_id` (string)*: The inference Id
** *`input` (string | string[])*: Inference input.
Either a string or an array of strings.
** *`task_type` (Optional, Enum("sparse_embedding" | "text_embedding" | "rerank" | "completion"))*: The task type
** *`query` (Optional, string)*: Query input, required for rerank task.
Not required for other tasks.
** *`task_settings` (Optional, User-defined value)*: Optional task settings
** *`timeout` (Optional, string | -1 | 0)*: Specifies the amount of time to wait for the inference request to complete.

[discrete]
==== put
Create an inference endpoint

{ref}/put-inference-api.html[Endpoint documentation]
[source,ts]
----
client.inference.put({ inference_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`inference_id` (string)*: The inference Id
** *`task_type` (Optional, Enum("sparse_embedding" | "text_embedding" | "rerank" | "completion"))*: The task type
** *`inference_config` (Optional, { service, service_settings, task_settings })*

[discrete]
==== stream_inference
Perform streaming inference
[source,ts]
----
client.inference.streamInference()
----


[discrete]
=== ingest
[discrete]
==== delete_geoip_database
Deletes a geoip database configuration.

{ref}/delete-geoip-database-api.html[Endpoint documentation]
[source,ts]
----
client.ingest.deleteGeoipDatabase({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string | string[])*: A list of geoip database configurations to delete
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== delete_pipeline
Deletes one or more existing ingest pipeline.

{ref}/delete-pipeline-api.html[Endpoint documentation]
[source,ts]
----
client.ingest.deletePipeline({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Pipeline ID or wildcard expression of pipeline IDs used to limit the request.
To delete all ingest pipelines in a cluster, use a value of `*`.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== geo_ip_stats
Gets download statistics for GeoIP2 databases used with the geoip processor.

{ref}/geoip-processor.html[Endpoint documentation]
[source,ts]
----
client.ingest.geoIpStats()
----


[discrete]
==== get_geoip_database
Returns information about one or more geoip database configurations.

{ref}/get-geoip-database-api.html[Endpoint documentation]
[source,ts]
----
client.ingest.getGeoipDatabase({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string | string[])*: List of database configuration IDs to retrieve.
Wildcard (`*`) expressions are supported.
To get all database configurations, omit this parameter or use `*`.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== get_pipeline
Returns information about one or more ingest pipelines.
This API returns a local reference of the pipeline.

{ref}/get-pipeline-api.html[Endpoint documentation]
[source,ts]
----
client.ingest.getPipeline({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: List of pipeline IDs to retrieve.
Wildcard (`*`) expressions are supported.
To get all ingest pipelines, omit this parameter or use `*`.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`summary` (Optional, boolean)*: Return pipelines without their definitions (default: false)

[discrete]
==== processor_grok
Extracts structured fields out of a single text field within a document.
You choose which field to extract matched fields from, as well as the grok pattern you expect will match.
A grok pattern is like a regular expression that supports aliased expressions that can be reused.

{ref}/grok-processor.html[Endpoint documentation]
[source,ts]
----
client.ingest.processorGrok()
----


[discrete]
==== put_geoip_database
Returns information about one or more geoip database configurations.

{ref}/put-geoip-database-api.html[Endpoint documentation]
[source,ts]
----
client.ingest.putGeoipDatabase({ id, name, maxmind })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: ID of the database configuration to create or update.
** *`name` (string)*: The provider-assigned name of the IP geolocation database to download.
** *`maxmind` ({ account_id })*: The configuration necessary to identify which IP geolocation provider to use to download the database, as well as any provider-specific configuration necessary for such downloading.
At present, the only supported provider is maxmind, and the maxmind provider requires that an account_id (string) is configured.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== put_pipeline
Creates or updates an ingest pipeline.
Changes made using this API take effect immediately.

{ref}/ingest.html[Endpoint documentation]
[source,ts]
----
client.ingest.putPipeline({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: ID of the ingest pipeline to create or update.
** *`_meta` (Optional, Record<string, User-defined value>)*: Optional metadata about the ingest pipeline. May have any contents. This map is not automatically generated by Elasticsearch.
** *`description` (Optional, string)*: Description of the ingest pipeline.
** *`on_failure` (Optional, { append, attachment, bytes, circle, community_id, convert, csv, date, date_index_name, dissect, dot_expander, drop, enrich, fail, fingerprint, foreach, ip_location, geo_grid, geoip, grok, gsub, html_strip, inference, join, json, kv, lowercase, network_direction, pipeline, redact, registered_domain, remove, rename, reroute, script, set, set_security_user, sort, split, terminate, trim, uppercase, urldecode, uri_parts, user_agent }[])*: Processors to run immediately after a processor failure. Each processor supports a processor-level `on_failure` value. If a processor without an `on_failure` value fails, Elasticsearch uses this pipeline-level parameter as a fallback. The processors in this parameter run sequentially in the order specified. Elasticsearch will not attempt to run the pipeline's remaining processors.
** *`processors` (Optional, { append, attachment, bytes, circle, community_id, convert, csv, date, date_index_name, dissect, dot_expander, drop, enrich, fail, fingerprint, foreach, ip_location, geo_grid, geoip, grok, gsub, html_strip, inference, join, json, kv, lowercase, network_direction, pipeline, redact, registered_domain, remove, rename, reroute, script, set, set_security_user, sort, split, terminate, trim, uppercase, urldecode, uri_parts, user_agent }[])*: Processors used to perform transformations on documents before indexing. Processors run sequentially in the order specified.
** *`version` (Optional, number)*: Version number used by external systems to track ingest pipelines. This parameter is intended for external systems only. Elasticsearch does not use or validate pipeline version numbers.
** *`deprecated` (Optional, boolean)*: Marks this ingest pipeline as deprecated.
When a deprecated ingest pipeline is referenced as the default or final pipeline when creating or updating a non-deprecated index template, Elasticsearch will emit a deprecation warning.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.
** *`if_version` (Optional, number)*: Required version for optimistic concurrency control for pipeline updates

[discrete]
==== simulate
Executes an ingest pipeline against a set of provided documents.

{ref}/simulate-pipeline-api.html[Endpoint documentation]
[source,ts]
----
client.ingest.simulate({ docs })
----

[discrete]
==== Arguments

* *Request (object):*
** *`docs` ({ _id, _index, _source }[])*: Sample documents to test in the pipeline.
** *`id` (Optional, string)*: Pipeline to test.
If you don’t specify a `pipeline` in the request body, this parameter is required.
** *`pipeline` (Optional, { description, on_failure, processors, version, deprecated, _meta })*: Pipeline to test.
If you don’t specify the `pipeline` request path parameter, this parameter is required.
If you specify both this and the request path parameter, the API only uses the request path parameter.
** *`verbose` (Optional, boolean)*: If `true`, the response includes output data for each processor in the executed pipeline.

[discrete]
=== license
[discrete]
==== delete
Deletes licensing information for the cluster

{ref}/delete-license.html[Endpoint documentation]
[source,ts]
----
client.license.delete()
----


[discrete]
==== get
Get license information.
Returns information about your Elastic license, including its type, its status, when it was issued, and when it expires.
For more information about the different types of licenses, refer to [Elastic Stack subscriptions](https://www.elastic.co/subscriptions).

{ref}/get-license.html[Endpoint documentation]
[source,ts]
----
client.license.get({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`accept_enterprise` (Optional, boolean)*: If `true`, this parameter returns enterprise for Enterprise license types. If `false`, this parameter returns platinum for both platinum and enterprise license types. This behavior is maintained for backwards compatibility.
This parameter is deprecated and will always be set to true in 8.x.
** *`local` (Optional, boolean)*: Specifies whether to retrieve local information. The default value is `false`, which means the information is retrieved from the master node.

[discrete]
==== get_basic_status
Retrieves information about the status of the basic license.

{ref}/get-basic-status.html[Endpoint documentation]
[source,ts]
----
client.license.getBasicStatus()
----


[discrete]
==== get_trial_status
Retrieves information about the status of the trial license.

{ref}/get-trial-status.html[Endpoint documentation]
[source,ts]
----
client.license.getTrialStatus()
----


[discrete]
==== post
Updates the license for the cluster.

{ref}/update-license.html[Endpoint documentation]
[source,ts]
----
client.license.post({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`license` (Optional, { expiry_date_in_millis, issue_date_in_millis, start_date_in_millis, issued_to, issuer, max_nodes, max_resource_units, signature, type, uid })*
** *`licenses` (Optional, { expiry_date_in_millis, issue_date_in_millis, start_date_in_millis, issued_to, issuer, max_nodes, max_resource_units, signature, type, uid }[])*: A sequence of one or more JSON documents containing the license information.
** *`acknowledge` (Optional, boolean)*: Specifies whether you acknowledge the license changes.

[discrete]
==== post_start_basic
The start basic API enables you to initiate an indefinite basic license, which gives access to all the basic features. If the basic license does not support all of the features that are available with your current license, however, you are notified in the response. You must then re-submit the API request with the acknowledge parameter set to true.
To check the status of your basic license, use the following API: [Get basic status](https://www.elastic.co/guide/en/elasticsearch/reference/current/get-basic-status.html).

{ref}/start-basic.html[Endpoint documentation]
[source,ts]
----
client.license.postStartBasic({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`acknowledge` (Optional, boolean)*: whether the user has acknowledged acknowledge messages (default: false)

[discrete]
==== post_start_trial
The start trial API enables you to start a 30-day trial, which gives access to all subscription features.

{ref}/start-trial.html[Endpoint documentation]
[source,ts]
----
client.license.postStartTrial({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`acknowledge` (Optional, boolean)*: whether the user has acknowledged acknowledge messages (default: false)
** *`type_query_string` (Optional, string)*

[discrete]
=== logstash
[discrete]
==== delete_pipeline
Deletes a pipeline used for Logstash Central Management.

{ref}/logstash-api-delete-pipeline.html[Endpoint documentation]
[source,ts]
----
client.logstash.deletePipeline({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the pipeline.

[discrete]
==== get_pipeline
Retrieves pipelines used for Logstash Central Management.

{ref}/logstash-api-get-pipeline.html[Endpoint documentation]
[source,ts]
----
client.logstash.getPipeline({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string | string[])*: List of pipeline identifiers.

[discrete]
==== put_pipeline
Creates or updates a pipeline used for Logstash Central Management.

{ref}/logstash-api-put-pipeline.html[Endpoint documentation]
[source,ts]
----
client.logstash.putPipeline({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the pipeline.
** *`pipeline` (Optional, { description, on_failure, processors, version, deprecated, _meta })*

[discrete]
=== migration
[discrete]
==== deprecations
Retrieves information about different cluster, node, and index level settings that use deprecated features that will be removed or changed in the next major version.

{ref}/migration-api-deprecation.html[Endpoint documentation]
[source,ts]
----
client.migration.deprecations({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string)*: Comma-separate list of data streams or indices to check. Wildcard (*) expressions are supported.

[discrete]
==== get_feature_upgrade_status
Find out whether system features need to be upgraded or not

{ref}/migration-api-feature-upgrade.html[Endpoint documentation]
[source,ts]
----
client.migration.getFeatureUpgradeStatus()
----


[discrete]
==== post_feature_upgrade
Begin upgrades for system features

{ref}/migration-api-feature-upgrade.html[Endpoint documentation]
[source,ts]
----
client.migration.postFeatureUpgrade()
----


[discrete]
=== ml
[discrete]
==== clear_trained_model_deployment_cache
Clear trained model deployment cache.
Cache will be cleared on all nodes where the trained model is assigned.
A trained model deployment may have an inference cache enabled.
As requests are handled by each allocated node, their responses may be cached on that individual node.
Calling this API clears the caches without restarting the deployment.

{ref}/clear-trained-model-deployment-cache.html[Endpoint documentation]
[source,ts]
----
client.ml.clearTrainedModelDeploymentCache({ model_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (string)*: The unique identifier of the trained model.

[discrete]
==== close_job
Close anomaly detection jobs.
A job can be opened and closed multiple times throughout its lifecycle. A closed job cannot receive data or perform analysis operations, but you can still explore and navigate results.
When you close a job, it runs housekeeping tasks such as pruning the model history, flushing buffers, calculating final results and persisting the model snapshots. Depending upon the size of the job, it could take several minutes to close and the equivalent time to re-open. After it is closed, the job has a minimal overhead on the cluster except for maintaining its meta data. Therefore it is a best practice to close jobs that are no longer required to process data.
If you close an anomaly detection job whose datafeed is running, the request first tries to stop the datafeed. This behavior is equivalent to calling stop datafeed API with the same timeout and force parameters as the close job request.
When a datafeed that has a specified end date stops, it automatically closes its associated job.

{ref}/ml-close-job.html[Endpoint documentation]
[source,ts]
----
client.ml.closeJob({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job. It can be a job identifier, a group name, or a wildcard expression. You can close multiple anomaly detection jobs in a single API request by using a group name, a list of jobs, or a wildcard expression. You can close all jobs by using `_all` or by specifying `*` as the job identifier.
** *`allow_no_match` (Optional, boolean)*: Refer to the description for the `allow_no_match` query parameter.
** *`force` (Optional, boolean)*: Refer to the descriptiion for the `force` query parameter.
** *`timeout` (Optional, string | -1 | 0)*: Refer to the description for the `timeout` query parameter.

[discrete]
==== delete_calendar
Delete a calendar.
Removes all scheduled events from a calendar, then deletes it.

{ref}/ml-delete-calendar.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteCalendar({ calendar_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`calendar_id` (string)*: A string that uniquely identifies a calendar.

[discrete]
==== delete_calendar_event
Delete events from a calendar.

{ref}/ml-delete-calendar-event.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteCalendarEvent({ calendar_id, event_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`calendar_id` (string)*: A string that uniquely identifies a calendar.
** *`event_id` (string)*: Identifier for the scheduled event.
You can obtain this identifier by using the get calendar events API.

[discrete]
==== delete_calendar_job
Delete anomaly jobs from a calendar.

{ref}/ml-delete-calendar-job.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteCalendarJob({ calendar_id, job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`calendar_id` (string)*: A string that uniquely identifies a calendar.
** *`job_id` (string | string[])*: An identifier for the anomaly detection jobs. It can be a job identifier, a group name, or a
list of jobs or groups.

[discrete]
==== delete_data_frame_analytics
Delete a data frame analytics job.

{ref}/delete-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteDataFrameAnalytics({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the data frame analytics job.
** *`force` (Optional, boolean)*: If `true`, it deletes a job that is not stopped; this method is quicker than stopping and deleting the job.
** *`timeout` (Optional, string | -1 | 0)*: The time to wait for the job to be deleted.

[discrete]
==== delete_datafeed
Delete a datafeed.

{ref}/ml-delete-datafeed.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteDatafeed({ datafeed_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (string)*: A numerical character string that uniquely identifies the datafeed. This
identifier can contain lowercase alphanumeric characters (a-z and 0-9),
hyphens, and underscores. It must start and end with alphanumeric
characters.
** *`force` (Optional, boolean)*: Use to forcefully delete a started datafeed; this method is quicker than
stopping and deleting the datafeed.

[discrete]
==== delete_expired_data
Delete expired ML data.
Deletes all job results, model snapshots and forecast data that have exceeded
their retention days period. Machine learning state documents that are not
associated with any job are also deleted.
You can limit the request to a single or set of anomaly detection jobs by
using a job identifier, a group name, a comma-separated list of jobs, or a
wildcard expression. You can delete expired data for all anomaly detection
jobs by using _all, by specifying * as the <job_id>, or by omitting the
<job_id>.

{ref}/ml-delete-expired-data.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteExpiredData({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (Optional, string)*: Identifier for an anomaly detection job. It can be a job identifier, a
group name, or a wildcard expression.
** *`requests_per_second` (Optional, float)*: The desired requests per second for the deletion processes. The default
behavior is no throttling.
** *`timeout` (Optional, string | -1 | 0)*: How long can the underlying delete processes run until they are canceled.

[discrete]
==== delete_filter
Delete a filter.
If an anomaly detection job references the filter, you cannot delete the
filter. You must update or delete the job before you can delete the filter.

{ref}/ml-delete-filter.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteFilter({ filter_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`filter_id` (string)*: A string that uniquely identifies a filter.

[discrete]
==== delete_forecast
Delete forecasts from a job.
By default, forecasts are retained for 14 days. You can specify a
different retention period with the `expires_in` parameter in the forecast
jobs API. The delete forecast API enables you to delete one or more
forecasts before they expire.

{ref}/ml-delete-forecast.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteForecast({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`forecast_id` (Optional, string)*: A list of forecast identifiers. If you do not specify
this optional parameter or if you specify `_all` or `*` the API deletes
all forecasts from the job.
** *`allow_no_forecasts` (Optional, boolean)*: Specifies whether an error occurs when there are no forecasts. In
particular, if this parameter is set to `false` and there are no
forecasts associated with the job, attempts to delete all forecasts
return an error.
** *`timeout` (Optional, string | -1 | 0)*: Specifies the period of time to wait for the completion of the delete
operation. When this period of time elapses, the API fails and returns an
error.

[discrete]
==== delete_job
Delete an anomaly detection job.
All job configuration, model state and results are deleted.
It is not currently possible to delete multiple jobs using wildcards or a
comma separated list. If you delete a job that has a datafeed, the request
first tries to delete the datafeed. This behavior is equivalent to calling
the delete datafeed API with the same timeout and force parameters as the
delete job request.

{ref}/ml-delete-job.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteJob({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`force` (Optional, boolean)*: Use to forcefully delete an opened job; this method is quicker than
closing and deleting the job.
** *`delete_user_annotations` (Optional, boolean)*: Specifies whether annotations that have been added by the
user should be deleted along with any auto-generated annotations when the job is
reset.
** *`wait_for_completion` (Optional, boolean)*: Specifies whether the request should return immediately or wait until the
job deletion completes.

[discrete]
==== delete_model_snapshot
Delete a model snapshot.
You cannot delete the active model snapshot. To delete that snapshot, first
revert to a different one. To identify the active model snapshot, refer to
the `model_snapshot_id` in the results from the get jobs API.

{ref}/ml-delete-snapshot.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteModelSnapshot({ job_id, snapshot_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`snapshot_id` (string)*: Identifier for the model snapshot.

[discrete]
==== delete_trained_model
Delete an unreferenced trained model.
The request deletes a trained inference model that is not referenced by an ingest pipeline.

{ref}/delete-trained-models.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteTrainedModel({ model_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (string)*: The unique identifier of the trained model.
** *`force` (Optional, boolean)*: Forcefully deletes a trained model that is referenced by ingest pipelines or has a started deployment.

[discrete]
==== delete_trained_model_alias
Delete a trained model alias.
This API deletes an existing model alias that refers to a trained model. If
the model alias is missing or refers to a model other than the one identified
by the `model_id`, this API returns an error.

{ref}/delete-trained-models-aliases.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteTrainedModelAlias({ model_alias, model_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_alias` (string)*: The model alias to delete.
** *`model_id` (string)*: The trained model ID to which the model alias refers.

[discrete]
==== estimate_model_memory
Estimate job model memory usage.
Makes an estimation of the memory usage for an anomaly detection job model.
It is based on analysis configuration details for the job and cardinality
estimates for the fields it references.

{ref}/ml-apis.html[Endpoint documentation]
[source,ts]
----
client.ml.estimateModelMemory({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`analysis_config` (Optional, { bucket_span, categorization_analyzer, categorization_field_name, categorization_filters, detectors, influencers, latency, model_prune_window, multivariate_by_fields, per_partition_categorization, summary_count_field_name })*: For a list of the properties that you can specify in the
`analysis_config` component of the body of this API.
** *`max_bucket_cardinality` (Optional, Record<string, number>)*: Estimates of the highest cardinality in a single bucket that is observed
for influencer fields over the time period that the job analyzes data.
To produce a good answer, values must be provided for all influencer
fields. Providing values for fields that are not listed as `influencers`
has no effect on the estimation.
** *`overall_cardinality` (Optional, Record<string, number>)*: Estimates of the cardinality that is observed for fields over the whole
time period that the job analyzes data. To produce a good answer, values
must be provided for fields referenced in the `by_field_name`,
`over_field_name` and `partition_field_name` of any detectors. Providing
values for other fields has no effect on the estimation. It can be
omitted from the request if no detectors have a `by_field_name`,
`over_field_name` or `partition_field_name`.

[discrete]
==== evaluate_data_frame
Evaluate data frame analytics.
The API packages together commonly used evaluation metrics for various types
of machine learning features. This has been designed for use on indexes
created by data frame analytics. Evaluation requires both a ground truth
field and an analytics result field to be present.

{ref}/evaluate-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.evaluateDataFrame({ evaluation, index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`evaluation` ({ classification, outlier_detection, regression })*: Defines the type of evaluation you want to perform.
** *`index` (string)*: Defines the `index` in which the evaluation will be performed.
** *`query` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: A query clause that retrieves a subset of data from the source index.

[discrete]
==== explain_data_frame_analytics
Explain data frame analytics config.
This API provides explanations for a data frame analytics config that either
exists already or one that has not been created yet. The following
explanations are provided:
* which fields are included or not in the analysis and why,
* how much memory is estimated to be required. The estimate can be used when deciding the appropriate value for model_memory_limit setting later on.
If you have object fields or fields that are excluded via source filtering, they are not included in the explanation.

{ref}/explain-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.explainDataFrameAnalytics({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: Identifier for the data frame analytics job. This identifier can contain
lowercase alphanumeric characters (a-z and 0-9), hyphens, and
underscores. It must start and end with alphanumeric characters.
** *`source` (Optional, { index, query, runtime_mappings, _source })*: The configuration of how to source the analysis data. It requires an
index. Optionally, query and _source may be specified.
** *`dest` (Optional, { index, results_field })*: The destination configuration, consisting of index and optionally
results_field (ml by default).
** *`analysis` (Optional, { classification, outlier_detection, regression })*: The analysis configuration, which contains the information necessary to
perform one of the following types of analysis: classification, outlier
detection, or regression.
** *`description` (Optional, string)*: A description of the job.
** *`model_memory_limit` (Optional, string)*: The approximate maximum amount of memory resources that are permitted for
analytical processing. If your `elasticsearch.yml` file contains an
`xpack.ml.max_model_memory_limit` setting, an error occurs when you try to
create data frame analytics jobs that have `model_memory_limit` values
greater than that setting.
** *`max_num_threads` (Optional, number)*: The maximum number of threads to be used by the analysis. Using more
threads may decrease the time necessary to complete the analysis at the
cost of using more CPU. Note that the process may use additional threads
for operational functionality other than the analysis itself.
** *`analyzed_fields` (Optional, { includes, excludes })*: Specify includes and/or excludes patterns to select which fields will be
included in the analysis. The patterns specified in excludes are applied
last, therefore excludes takes precedence. In other words, if the same
field is specified in both includes and excludes, then the field will not
be included in the analysis.
** *`allow_lazy_start` (Optional, boolean)*: Specifies whether this job can start when there is insufficient machine
learning node capacity for it to be immediately assigned to a node.

[discrete]
==== flush_job
Force buffered data to be processed.
The flush jobs API is only applicable when sending data for analysis using
the post data API. Depending on the content of the buffer, then it might
additionally calculate new results. Both flush and close operations are
similar, however the flush is more efficient if you are expecting to send
more data for analysis. When flushing, the job remains open and is available
to continue analyzing data. A close operation additionally prunes and
persists the model state to disk and the job must be opened again before
analyzing further data.

{ref}/ml-flush-job.html[Endpoint documentation]
[source,ts]
----
client.ml.flushJob({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`advance_time` (Optional, string | Unit)*: Refer to the description for the `advance_time` query parameter.
** *`calc_interim` (Optional, boolean)*: Refer to the description for the `calc_interim` query parameter.
** *`end` (Optional, string | Unit)*: Refer to the description for the `end` query parameter.
** *`skip_time` (Optional, string | Unit)*: Refer to the description for the `skip_time` query parameter.
** *`start` (Optional, string | Unit)*: Refer to the description for the `start` query parameter.

[discrete]
==== forecast
Predict future behavior of a time series.

Forecasts are not supported for jobs that perform population analysis; an
error occurs if you try to create a forecast for a job that has an
`over_field_name` in its configuration. Forcasts predict future behavior
based on historical data.

{ref}/ml-forecast.html[Endpoint documentation]
[source,ts]
----
client.ml.forecast({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job. The job must be open when you
create a forecast; otherwise, an error occurs.
** *`duration` (Optional, string | -1 | 0)*: Refer to the description for the `duration` query parameter.
** *`expires_in` (Optional, string | -1 | 0)*: Refer to the description for the `expires_in` query parameter.
** *`max_model_memory` (Optional, string)*: Refer to the description for the `max_model_memory` query parameter.

[discrete]
==== get_buckets
Get anomaly detection job results for buckets.
The API presents a chronological view of the records, grouped by bucket.

{ref}/ml-get-bucket.html[Endpoint documentation]
[source,ts]
----
client.ml.getBuckets({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`timestamp` (Optional, string | Unit)*: The timestamp of a single bucket result. If you do not specify this
parameter, the API returns information about all buckets.
** *`anomaly_score` (Optional, number)*: Refer to the description for the `anomaly_score` query parameter.
** *`desc` (Optional, boolean)*: Refer to the description for the `desc` query parameter.
** *`end` (Optional, string | Unit)*: Refer to the description for the `end` query parameter.
** *`exclude_interim` (Optional, boolean)*: Refer to the description for the `exclude_interim` query parameter.
** *`expand` (Optional, boolean)*: Refer to the description for the `expand` query parameter.
** *`page` (Optional, { from, size })*
** *`sort` (Optional, string)*: Refer to the desription for the `sort` query parameter.
** *`start` (Optional, string | Unit)*: Refer to the description for the `start` query parameter.
** *`from` (Optional, number)*: Skips the specified number of buckets.
** *`size` (Optional, number)*: Specifies the maximum number of buckets to obtain.

[discrete]
==== get_calendar_events
Get info about events in calendars.

{ref}/ml-get-calendar-event.html[Endpoint documentation]
[source,ts]
----
client.ml.getCalendarEvents({ calendar_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`calendar_id` (string)*: A string that uniquely identifies a calendar. You can get information for multiple calendars by using a list of ids or a wildcard expression. You can get information for all calendars by using `_all` or `*` or by omitting the calendar identifier.
** *`end` (Optional, string | Unit)*: Specifies to get events with timestamps earlier than this time.
** *`from` (Optional, number)*: Skips the specified number of events.
** *`job_id` (Optional, string)*: Specifies to get events for a specific anomaly detection job identifier or job group. It must be used with a calendar identifier of `_all` or `*`.
** *`size` (Optional, number)*: Specifies the maximum number of events to obtain.
** *`start` (Optional, string | Unit)*: Specifies to get events with timestamps after this time.

[discrete]
==== get_calendars
Get calendar configuration info.

{ref}/ml-get-calendar.html[Endpoint documentation]
[source,ts]
----
client.ml.getCalendars({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`calendar_id` (Optional, string)*: A string that uniquely identifies a calendar. You can get information for multiple calendars by using a list of ids or a wildcard expression. You can get information for all calendars by using `_all` or `*` or by omitting the calendar identifier.
** *`page` (Optional, { from, size })*: This object is supported only when you omit the calendar identifier.
** *`from` (Optional, number)*: Skips the specified number of calendars. This parameter is supported only when you omit the calendar identifier.
** *`size` (Optional, number)*: Specifies the maximum number of calendars to obtain. This parameter is supported only when you omit the calendar identifier.

[discrete]
==== get_categories
Get anomaly detection job results for categories.

{ref}/ml-get-category.html[Endpoint documentation]
[source,ts]
----
client.ml.getCategories({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`category_id` (Optional, string)*: Identifier for the category, which is unique in the job. If you specify
neither the category ID nor the partition_field_value, the API returns
information about all categories. If you specify only the
partition_field_value, it returns information about all categories for
the specified partition.
** *`page` (Optional, { from, size })*: Configures pagination.
This parameter has the `from` and `size` properties.
** *`from` (Optional, number)*: Skips the specified number of categories.
** *`partition_field_value` (Optional, string)*: Only return categories for the specified partition.
** *`size` (Optional, number)*: Specifies the maximum number of categories to obtain.

[discrete]
==== get_data_frame_analytics
Get data frame analytics job configuration info.
You can get information for multiple data frame analytics jobs in a single
API request by using a comma-separated list of data frame analytics jobs or a
wildcard expression.

{ref}/get-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.getDataFrameAnalytics({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: Identifier for the data frame analytics job. If you do not specify this
option, the API returns information for the first hundred data frame
analytics jobs.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no data frame analytics
jobs that match.
2. Contains the `_all` string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

The default value returns an empty data_frame_analytics array when there
are no matches and the subset of results when there are partial matches.
If this parameter is `false`, the request returns a 404 status code when
there are no matches or only partial matches.
** *`from` (Optional, number)*: Skips the specified number of data frame analytics jobs.
** *`size` (Optional, number)*: Specifies the maximum number of data frame analytics jobs to obtain.
** *`exclude_generated` (Optional, boolean)*: Indicates if certain fields should be removed from the configuration on
retrieval. This allows the configuration to be in an acceptable format to
be retrieved and then added to another cluster.

[discrete]
==== get_data_frame_analytics_stats
Get data frame analytics jobs usage info.

{ref}/get-dfanalytics-stats.html[Endpoint documentation]
[source,ts]
----
client.ml.getDataFrameAnalyticsStats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: Identifier for the data frame analytics job. If you do not specify this
option, the API returns information for the first hundred data frame
analytics jobs.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no data frame analytics
jobs that match.
2. Contains the `_all` string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

The default value returns an empty data_frame_analytics array when there
are no matches and the subset of results when there are partial matches.
If this parameter is `false`, the request returns a 404 status code when
there are no matches or only partial matches.
** *`from` (Optional, number)*: Skips the specified number of data frame analytics jobs.
** *`size` (Optional, number)*: Specifies the maximum number of data frame analytics jobs to obtain.
** *`verbose` (Optional, boolean)*: Defines whether the stats response should be verbose.

[discrete]
==== get_datafeed_stats
Get datafeeds usage info.
You can get statistics for multiple datafeeds in a single API request by
using a comma-separated list of datafeeds or a wildcard expression. You can
get statistics for all datafeeds by using `_all`, by specifying `*` as the
`<feed_id>`, or by omitting the `<feed_id>`. If the datafeed is stopped, the
only information you receive is the `datafeed_id` and the `state`.
This API returns a maximum of 10,000 datafeeds.

{ref}/ml-get-datafeed-stats.html[Endpoint documentation]
[source,ts]
----
client.ml.getDatafeedStats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (Optional, string | string[])*: Identifier for the datafeed. It can be a datafeed identifier or a
wildcard expression. If you do not specify one of these options, the API
returns information about all datafeeds.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no datafeeds that match.
2. Contains the `_all` string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

The default value is `true`, which returns an empty `datafeeds` array
when there are no matches and the subset of results when there are
partial matches. If this parameter is `false`, the request returns a
`404` status code when there are no matches or only partial matches.

[discrete]
==== get_datafeeds
Get datafeeds configuration info.
You can get information for multiple datafeeds in a single API request by
using a comma-separated list of datafeeds or a wildcard expression. You can
get information for all datafeeds by using `_all`, by specifying `*` as the
`<feed_id>`, or by omitting the `<feed_id>`.
This API returns a maximum of 10,000 datafeeds.

{ref}/ml-get-datafeed.html[Endpoint documentation]
[source,ts]
----
client.ml.getDatafeeds({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (Optional, string | string[])*: Identifier for the datafeed. It can be a datafeed identifier or a
wildcard expression. If you do not specify one of these options, the API
returns information about all datafeeds.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no datafeeds that match.
2. Contains the `_all` string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

The default value is `true`, which returns an empty `datafeeds` array
when there are no matches and the subset of results when there are
partial matches. If this parameter is `false`, the request returns a
`404` status code when there are no matches or only partial matches.
** *`exclude_generated` (Optional, boolean)*: Indicates if certain fields should be removed from the configuration on
retrieval. This allows the configuration to be in an acceptable format to
be retrieved and then added to another cluster.

[discrete]
==== get_filters
Get filters.
You can get a single filter or all filters.

{ref}/ml-get-filter.html[Endpoint documentation]
[source,ts]
----
client.ml.getFilters({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`filter_id` (Optional, string | string[])*: A string that uniquely identifies a filter.
** *`from` (Optional, number)*: Skips the specified number of filters.
** *`size` (Optional, number)*: Specifies the maximum number of filters to obtain.

[discrete]
==== get_influencers
Get anomaly detection job results for influencers.
Influencers are the entities that have contributed to, or are to blame for,
the anomalies. Influencer results are available only if an
`influencer_field_name` is specified in the job configuration.

{ref}/ml-get-influencer.html[Endpoint documentation]
[source,ts]
----
client.ml.getInfluencers({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`page` (Optional, { from, size })*: Configures pagination.
This parameter has the `from` and `size` properties.
** *`desc` (Optional, boolean)*: If true, the results are sorted in descending order.
** *`end` (Optional, string | Unit)*: Returns influencers with timestamps earlier than this time.
The default value means it is unset and results are not limited to
specific timestamps.
** *`exclude_interim` (Optional, boolean)*: If true, the output excludes interim results. By default, interim results
are included.
** *`influencer_score` (Optional, number)*: Returns influencers with anomaly scores greater than or equal to this
value.
** *`from` (Optional, number)*: Skips the specified number of influencers.
** *`size` (Optional, number)*: Specifies the maximum number of influencers to obtain.
** *`sort` (Optional, string)*: Specifies the sort field for the requested influencers. By default, the
influencers are sorted by the `influencer_score` value.
** *`start` (Optional, string | Unit)*: Returns influencers with timestamps after this time. The default value
means it is unset and results are not limited to specific timestamps.

[discrete]
==== get_job_stats
Get anomaly detection jobs usage info.

{ref}/ml-get-job-stats.html[Endpoint documentation]
[source,ts]
----
client.ml.getJobStats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (Optional, string)*: Identifier for the anomaly detection job. It can be a job identifier, a
group name, a list of jobs, or a wildcard expression. If
you do not specify one of these options, the API returns information for
all anomaly detection jobs.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no jobs that match.
2. Contains the _all string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

If `true`, the API returns an empty `jobs` array when
there are no matches and the subset of results when there are partial
matches. If `false`, the API returns a `404` status
code when there are no matches or only partial matches.

[discrete]
==== get_jobs
Get anomaly detection jobs configuration info.
You can get information for multiple anomaly detection jobs in a single API
request by using a group name, a comma-separated list of jobs, or a wildcard
expression. You can get information for all anomaly detection jobs by using
`_all`, by specifying `*` as the `<job_id>`, or by omitting the `<job_id>`.

{ref}/ml-get-job.html[Endpoint documentation]
[source,ts]
----
client.ml.getJobs({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (Optional, string | string[])*: Identifier for the anomaly detection job. It can be a job identifier, a
group name, or a wildcard expression. If you do not specify one of these
options, the API returns information for all anomaly detection jobs.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no jobs that match.
2. Contains the _all string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

The default value is `true`, which returns an empty `jobs` array when
there are no matches and the subset of results when there are partial
matches. If this parameter is `false`, the request returns a `404` status
code when there are no matches or only partial matches.
** *`exclude_generated` (Optional, boolean)*: Indicates if certain fields should be removed from the configuration on
retrieval. This allows the configuration to be in an acceptable format to
be retrieved and then added to another cluster.

[discrete]
==== get_memory_stats
Get machine learning memory usage info.
Get information about how machine learning jobs and trained models are using memory,
on each node, both within the JVM heap, and natively, outside of the JVM.

{ref}/get-ml-memory.html[Endpoint documentation]
[source,ts]
----
client.ml.getMemoryStats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`node_id` (Optional, string)*: The names of particular nodes in the cluster to target. For example, `nodeId1,nodeId2` or
`ml:true`
** *`human` (Optional, boolean)*: Specify this query parameter to include the fields with units in the response. Otherwise only
the `_in_bytes` sizes are returned in the response.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout
expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request
fails and returns an error.

[discrete]
==== get_model_snapshot_upgrade_stats
Get anomaly detection job model snapshot upgrade usage info.

{ref}/ml-get-job-model-snapshot-upgrade-stats.html[Endpoint documentation]
[source,ts]
----
client.ml.getModelSnapshotUpgradeStats({ job_id, snapshot_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`snapshot_id` (string)*: A numerical character string that uniquely identifies the model snapshot. You can get information for multiple
snapshots by using a list or a wildcard expression. You can get all snapshots by using `_all`,
by specifying `*` as the snapshot ID, or by omitting the snapshot ID.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

 -  Contains wildcard expressions and there are no jobs that match.
 -  Contains the _all string or no identifiers and there are no matches.
 -  Contains wildcard expressions and there are only partial matches.

The default value is true, which returns an empty jobs array when there are no matches and the subset of results
when there are partial matches. If this parameter is false, the request returns a 404 status code when there are
no matches or only partial matches.

[discrete]
==== get_model_snapshots
Get model snapshots info.

{ref}/ml-get-snapshot.html[Endpoint documentation]
[source,ts]
----
client.ml.getModelSnapshots({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`snapshot_id` (Optional, string)*: A numerical character string that uniquely identifies the model snapshot. You can get information for multiple
snapshots by using a list or a wildcard expression. You can get all snapshots by using `_all`,
by specifying `*` as the snapshot ID, or by omitting the snapshot ID.
** *`desc` (Optional, boolean)*: Refer to the description for the `desc` query parameter.
** *`end` (Optional, string | Unit)*: Refer to the description for the `end` query parameter.
** *`page` (Optional, { from, size })*
** *`sort` (Optional, string)*: Refer to the description for the `sort` query parameter.
** *`start` (Optional, string | Unit)*: Refer to the description for the `start` query parameter.
** *`from` (Optional, number)*: Skips the specified number of snapshots.
** *`size` (Optional, number)*: Specifies the maximum number of snapshots to obtain.

[discrete]
==== get_overall_buckets
Get overall bucket results.

Retrievs overall bucket results that summarize the bucket results of
multiple anomaly detection jobs.

The `overall_score` is calculated by combining the scores of all the
buckets within the overall bucket span. First, the maximum
`anomaly_score` per anomaly detection job in the overall bucket is
calculated. Then the `top_n` of those scores are averaged to result in
the `overall_score`. This means that you can fine-tune the
`overall_score` so that it is more or less sensitive to the number of
jobs that detect an anomaly at the same time. For example, if you set
`top_n` to `1`, the `overall_score` is the maximum bucket score in the
overall bucket. Alternatively, if you set `top_n` to the number of jobs,
the `overall_score` is high only when all jobs detect anomalies in that
overall bucket. If you set the `bucket_span` parameter (to a value
greater than its default), the `overall_score` is the maximum
`overall_score` of the overall buckets that have a span equal to the
jobs' largest bucket span.

{ref}/ml-get-overall-buckets.html[Endpoint documentation]
[source,ts]
----
client.ml.getOverallBuckets({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job. It can be a job identifier, a
group name, a list of jobs or groups, or a wildcard
expression.

You can summarize the bucket results for all anomaly detection jobs by
using `_all` or by specifying `*` as the `<job_id>`.
** *`allow_no_match` (Optional, boolean)*: Refer to the description for the `allow_no_match` query parameter.
** *`bucket_span` (Optional, string | -1 | 0)*: Refer to the description for the `bucket_span` query parameter.
** *`end` (Optional, string | Unit)*: Refer to the description for the `end` query parameter.
** *`exclude_interim` (Optional, boolean)*: Refer to the description for the `exclude_interim` query parameter.
** *`overall_score` (Optional, number | string)*: Refer to the description for the `overall_score` query parameter.
** *`start` (Optional, string | Unit)*: Refer to the description for the `start` query parameter.
** *`top_n` (Optional, number)*: Refer to the description for the `top_n` query parameter.

[discrete]
==== get_records
Get anomaly records for an anomaly detection job.
Records contain the detailed analytical results. They describe the anomalous
activity that has been identified in the input data based on the detector
configuration.
There can be many anomaly records depending on the characteristics and size
of the input data. In practice, there are often too many to be able to
manually process them. The machine learning features therefore perform a
sophisticated aggregation of the anomaly records into buckets.
The number of record results depends on the number of anomalies found in each
bucket, which relates to the number of time series being modeled and the
number of detectors.

{ref}/ml-get-record.html[Endpoint documentation]
[source,ts]
----
client.ml.getRecords({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`desc` (Optional, boolean)*: Refer to the description for the `desc` query parameter.
** *`end` (Optional, string | Unit)*: Refer to the description for the `end` query parameter.
** *`exclude_interim` (Optional, boolean)*: Refer to the description for the `exclude_interim` query parameter.
** *`page` (Optional, { from, size })*
** *`record_score` (Optional, number)*: Refer to the description for the `record_score` query parameter.
** *`sort` (Optional, string)*: Refer to the description for the `sort` query parameter.
** *`start` (Optional, string | Unit)*: Refer to the description for the `start` query parameter.
** *`from` (Optional, number)*: Skips the specified number of records.
** *`size` (Optional, number)*: Specifies the maximum number of records to obtain.

[discrete]
==== get_trained_models
Get trained model configuration info.

{ref}/get-trained-models.html[Endpoint documentation]
[source,ts]
----
client.ml.getTrainedModels({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (Optional, string | string[])*: The unique identifier of the trained model or a model alias.

You can get information for multiple trained models in a single API
request by using a list of model IDs or a wildcard
expression.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

- Contains wildcard expressions and there are no models that match.
- Contains the _all string or no identifiers and there are no matches.
- Contains wildcard expressions and there are only partial matches.

If true, it returns an empty array when there are no matches and the
subset of results when there are partial matches.
** *`decompress_definition` (Optional, boolean)*: Specifies whether the included model definition should be returned as a
JSON map (true) or in a custom compressed format (false).
** *`exclude_generated` (Optional, boolean)*: Indicates if certain fields should be removed from the configuration on
retrieval. This allows the configuration to be in an acceptable format to
be retrieved and then added to another cluster.
** *`from` (Optional, number)*: Skips the specified number of models.
** *`include` (Optional, Enum("definition" | "feature_importance_baseline" | "hyperparameters" | "total_feature_importance" | "definition_status"))*: A comma delimited string of optional fields to include in the response
body.
** *`size` (Optional, number)*: Specifies the maximum number of models to obtain.
** *`tags` (Optional, string | string[])*: A comma delimited string of tags. A trained model can have many tags, or
none. When supplied, only trained models that contain all the supplied
tags are returned.

[discrete]
==== get_trained_models_stats
Get trained models usage info.
You can get usage information for multiple trained
models in a single API request by using a comma-separated list of model IDs or a wildcard expression.

{ref}/get-trained-models-stats.html[Endpoint documentation]
[source,ts]
----
client.ml.getTrainedModelsStats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (Optional, string | string[])*: The unique identifier of the trained model or a model alias. It can be a
list or a wildcard expression.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

- Contains wildcard expressions and there are no models that match.
- Contains the _all string or no identifiers and there are no matches.
- Contains wildcard expressions and there are only partial matches.

If true, it returns an empty array when there are no matches and the
subset of results when there are partial matches.
** *`from` (Optional, number)*: Skips the specified number of models.
** *`size` (Optional, number)*: Specifies the maximum number of models to obtain.

[discrete]
==== infer_trained_model
Evaluate a trained model.

{ref}/infer-trained-model.html[Endpoint documentation]
[source,ts]
----
client.ml.inferTrainedModel({ model_id, docs })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (string)*: The unique identifier of the trained model.
** *`docs` (Record<string, User-defined value>[])*: An array of objects to pass to the model for inference. The objects should contain a fields matching your
configured trained model input. Typically, for NLP models, the field name is `text_field`.
Currently, for NLP models, only a single value is allowed.
** *`inference_config` (Optional, { regression, classification, text_classification, zero_shot_classification, fill_mask, ner, pass_through, text_embedding, text_expansion, question_answering })*: The inference configuration updates to apply on the API call
** *`timeout` (Optional, string | -1 | 0)*: Controls the amount of time to wait for inference results.

[discrete]
==== info
Return ML defaults and limits.
Returns defaults and limits used by machine learning.
This endpoint is designed to be used by a user interface that needs to fully
understand machine learning configurations where some options are not
specified, meaning that the defaults should be used. This endpoint may be
used to find out what those defaults are. It also provides information about
the maximum size of machine learning jobs that could run in the current
cluster configuration.

{ref}/get-ml-info.html[Endpoint documentation]
[source,ts]
----
client.ml.info()
----


[discrete]
==== open_job
Open anomaly detection jobs.
An anomaly detection job must be opened to be ready to receive and analyze
data. It can be opened and closed multiple times throughout its lifecycle.
When you open a new job, it starts with an empty model.
When you open an existing job, the most recent model state is automatically
loaded. The job is ready to resume its analysis from where it left off, once
new data is received.

{ref}/ml-open-job.html[Endpoint documentation]
[source,ts]
----
client.ml.openJob({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`timeout` (Optional, string | -1 | 0)*: Refer to the description for the `timeout` query parameter.

[discrete]
==== post_calendar_events
Add scheduled events to the calendar.

{ref}/ml-post-calendar-event.html[Endpoint documentation]
[source,ts]
----
client.ml.postCalendarEvents({ calendar_id, events })
----

[discrete]
==== Arguments

* *Request (object):*
** *`calendar_id` (string)*: A string that uniquely identifies a calendar.
** *`events` ({ calendar_id, event_id, description, end_time, start_time }[])*: A list of one of more scheduled events. The event’s start and end times can be specified as integer milliseconds since the epoch or as a string in ISO 8601 format.

[discrete]
==== post_data
Send data to an anomaly detection job for analysis.

IMPORTANT: For each job, data can be accepted from only a single connection at a time.
It is not currently possible to post data to multiple jobs using wildcards or a comma-separated list.

{ref}/ml-post-data.html[Endpoint documentation]
[source,ts]
----
client.ml.postData({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job. The job must have a state of open to receive and process the data.
** *`data` (Optional, TData[])*
** *`reset_end` (Optional, string | Unit)*: Specifies the end of the bucket resetting range.
** *`reset_start` (Optional, string | Unit)*: Specifies the start of the bucket resetting range.

[discrete]
==== preview_data_frame_analytics
Preview features used by data frame analytics.
Previews the extracted features used by a data frame analytics config.

{ref}/preview-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.previewDataFrameAnalytics({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: Identifier for the data frame analytics job.
** *`config` (Optional, { source, analysis, model_memory_limit, max_num_threads, analyzed_fields })*: A data frame analytics config as described in create data frame analytics
jobs. Note that `id` and `dest` don’t need to be provided in the context of
this API.

[discrete]
==== preview_datafeed
Preview a datafeed.
This API returns the first "page" of search results from a datafeed.
You can preview an existing datafeed or provide configuration details for a datafeed
and anomaly detection job in the API. The preview shows the structure of the data
that will be passed to the anomaly detection engine.
IMPORTANT: When Elasticsearch security features are enabled, the preview uses the credentials of the user that
called the API. However, when the datafeed starts it uses the roles of the last user that created or updated the
datafeed. To get a preview that accurately reflects the behavior of the datafeed, use the appropriate credentials.
You can also use secondary authorization headers to supply the credentials.

{ref}/ml-preview-datafeed.html[Endpoint documentation]
[source,ts]
----
client.ml.previewDatafeed({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (Optional, string)*: A numerical character string that uniquely identifies the datafeed. This identifier can contain lowercase
alphanumeric characters (a-z and 0-9), hyphens, and underscores. It must start and end with alphanumeric
characters. NOTE: If you use this path parameter, you cannot provide datafeed or anomaly detection job
configuration details in the request body.
** *`datafeed_config` (Optional, { aggregations, chunking_config, datafeed_id, delayed_data_check_config, frequency, indices, indices_options, job_id, max_empty_searches, query, query_delay, runtime_mappings, script_fields, scroll_size })*: The datafeed definition to preview.
** *`job_config` (Optional, { allow_lazy_open, analysis_config, analysis_limits, background_persist_interval, custom_settings, daily_model_snapshot_retention_after_days, data_description, datafeed_config, description, groups, job_id, job_type, model_plot_config, model_snapshot_retention_days, renormalization_window_days, results_index_name, results_retention_days })*: The configuration details for the anomaly detection job that is associated with the datafeed. If the
`datafeed_config` object does not include a `job_id` that references an existing anomaly detection job, you must
supply this `job_config` object. If you include both a `job_id` and a `job_config`, the latter information is
used. You cannot specify a `job_config` object unless you also supply a `datafeed_config` object.
** *`start` (Optional, string | Unit)*: The start time from where the datafeed preview should begin
** *`end` (Optional, string | Unit)*: The end time when the datafeed preview should stop

[discrete]
==== put_calendar
Create a calendar.

{ref}/ml-put-calendar.html[Endpoint documentation]
[source,ts]
----
client.ml.putCalendar({ calendar_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`calendar_id` (string)*: A string that uniquely identifies a calendar.
** *`job_ids` (Optional, string[])*: An array of anomaly detection job identifiers.
** *`description` (Optional, string)*: A description of the calendar.

[discrete]
==== put_calendar_job
Add anomaly detection job to calendar.

{ref}/ml-put-calendar-job.html[Endpoint documentation]
[source,ts]
----
client.ml.putCalendarJob({ calendar_id, job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`calendar_id` (string)*: A string that uniquely identifies a calendar.
** *`job_id` (string | string[])*: An identifier for the anomaly detection jobs. It can be a job identifier, a group name, or a list of jobs or groups.

[discrete]
==== put_data_frame_analytics
Create a data frame analytics job.
This API creates a data frame analytics job that performs an analysis on the
source indices and stores the outcome in a destination index.

{ref}/put-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.putDataFrameAnalytics({ id, analysis, dest, source })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the data frame analytics job. This identifier can contain
lowercase alphanumeric characters (a-z and 0-9), hyphens, and
underscores. It must start and end with alphanumeric characters.
** *`analysis` ({ classification, outlier_detection, regression })*: The analysis configuration, which contains the information necessary to
perform one of the following types of analysis: classification, outlier
detection, or regression.
** *`dest` ({ index, results_field })*: The destination configuration.
** *`source` ({ index, query, runtime_mappings, _source })*: The configuration of how to source the analysis data.
** *`allow_lazy_start` (Optional, boolean)*: Specifies whether this job can start when there is insufficient machine
learning node capacity for it to be immediately assigned to a node. If
set to `false` and a machine learning node with capacity to run the job
cannot be immediately found, the API returns an error. If set to `true`,
the API does not return an error; the job waits in the `starting` state
until sufficient machine learning node capacity is available. This
behavior is also affected by the cluster-wide
`xpack.ml.max_lazy_ml_nodes` setting.
** *`analyzed_fields` (Optional, { includes, excludes })*: Specifies `includes` and/or `excludes` patterns to select which fields
will be included in the analysis. The patterns specified in `excludes`
are applied last, therefore `excludes` takes precedence. In other words,
if the same field is specified in both `includes` and `excludes`, then
the field will not be included in the analysis. If `analyzed_fields` is
not set, only the relevant fields will be included. For example, all the
numeric fields for outlier detection.
The supported fields vary for each type of analysis. Outlier detection
requires numeric or `boolean` data to analyze. The algorithms don’t
support missing values therefore fields that have data types other than
numeric or boolean are ignored. Documents where included fields contain
missing values, null values, or an array are also ignored. Therefore the
`dest` index may contain documents that don’t have an outlier score.
Regression supports fields that are numeric, `boolean`, `text`,
`keyword`, and `ip` data types. It is also tolerant of missing values.
Fields that are supported are included in the analysis, other fields are
ignored. Documents where included fields contain an array with two or
more values are also ignored. Documents in the `dest` index that don’t
contain a results field are not included in the regression analysis.
Classification supports fields that are numeric, `boolean`, `text`,
`keyword`, and `ip` data types. It is also tolerant of missing values.
Fields that are supported are included in the analysis, other fields are
ignored. Documents where included fields contain an array with two or
more values are also ignored. Documents in the `dest` index that don’t
contain a results field are not included in the classification analysis.
Classification analysis can be improved by mapping ordinal variable
values to a single number. For example, in case of age ranges, you can
model the values as `0-14 = 0`, `15-24 = 1`, `25-34 = 2`, and so on.
** *`description` (Optional, string)*: A description of the job.
** *`max_num_threads` (Optional, number)*: The maximum number of threads to be used by the analysis. Using more
threads may decrease the time necessary to complete the analysis at the
cost of using more CPU. Note that the process may use additional threads
for operational functionality other than the analysis itself.
** *`model_memory_limit` (Optional, string)*: The approximate maximum amount of memory resources that are permitted for
analytical processing. If your `elasticsearch.yml` file contains an
`xpack.ml.max_model_memory_limit` setting, an error occurs when you try
to create data frame analytics jobs that have `model_memory_limit` values
greater than that setting.
** *`headers` (Optional, Record<string, string | string[]>)*
** *`version` (Optional, string)*

[discrete]
==== put_datafeed
Create a datafeed.
Datafeeds retrieve data from Elasticsearch for analysis by an anomaly detection job.
You can associate only one datafeed with each anomaly detection job.
The datafeed contains a query that runs at a defined interval (`frequency`).
If you are concerned about delayed data, you can add a delay (`query_delay') at each interval.
When Elasticsearch security features are enabled, your datafeed remembers which roles the user who created it had
at the time of creation and runs the query using those same roles. If you provide secondary authorization headers,
those credentials are used instead.
You must use Kibana, this API, or the create anomaly detection jobs API to create a datafeed. Do not add a datafeed
directly to the `.ml-config` index. Do not give users `write` privileges on the `.ml-config` index.

{ref}/ml-put-datafeed.html[Endpoint documentation]
[source,ts]
----
client.ml.putDatafeed({ datafeed_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (string)*: A numerical character string that uniquely identifies the datafeed.
This identifier can contain lowercase alphanumeric characters (a-z and 0-9), hyphens, and underscores.
It must start and end with alphanumeric characters.
** *`aggregations` (Optional, Record<string, { aggregations, meta, adjacency_matrix, auto_date_histogram, avg, avg_bucket, boxplot, bucket_script, bucket_selector, bucket_sort, bucket_count_ks_test, bucket_correlation, cardinality, categorize_text, children, composite, cumulative_cardinality, cumulative_sum, date_histogram, date_range, derivative, diversified_sampler, extended_stats, extended_stats_bucket, frequent_item_sets, filter, filters, geo_bounds, geo_centroid, geo_distance, geohash_grid, geo_line, geotile_grid, geohex_grid, global, histogram, ip_range, ip_prefix, inference, line, matrix_stats, max, max_bucket, median_absolute_deviation, min, min_bucket, missing, moving_avg, moving_percentiles, moving_fn, multi_terms, nested, normalize, parent, percentile_ranks, percentiles, percentiles_bucket, range, rare_terms, rate, reverse_nested, random_sampler, sampler, scripted_metric, serial_diff, significant_terms, significant_text, stats, stats_bucket, string_stats, sum, sum_bucket, terms, time_series, top_hits, t_test, top_metrics, value_count, weighted_avg, variable_width_histogram }>)*: If set, the datafeed performs aggregation searches.
Support for aggregations is limited and should be used only with low cardinality data.
** *`chunking_config` (Optional, { mode, time_span })*: Datafeeds might be required to search over long time periods, for several months or years.
This search is split into time chunks in order to ensure the load on Elasticsearch is managed.
Chunking configuration controls how the size of these time chunks are calculated;
it is an advanced configuration option.
** *`delayed_data_check_config` (Optional, { check_window, enabled })*: Specifies whether the datafeed checks for missing data and the size of the window.
The datafeed can optionally search over indices that have already been read in an effort to determine whether
any data has subsequently been added to the index. If missing data is found, it is a good indication that the
`query_delay` is set too low and the data is being indexed after the datafeed has passed that moment in time.
This check runs only on real-time datafeeds.
** *`frequency` (Optional, string | -1 | 0)*: The interval at which scheduled queries are made while the datafeed runs in real time.
The default value is either the bucket span for short bucket spans, or, for longer bucket spans, a sensible
fraction of the bucket span. When `frequency` is shorter than the bucket span, interim results for the last
(partial) bucket are written then eventually overwritten by the full bucket results. If the datafeed uses
aggregations, this value must be divisible by the interval of the date histogram aggregation.
** *`indices` (Optional, string | string[])*: An array of index names. Wildcards are supported. If any of the indices are in remote clusters, the machine
learning nodes must have the `remote_cluster_client` role.
** *`indices_options` (Optional, { allow_no_indices, expand_wildcards, ignore_unavailable, ignore_throttled })*: Specifies index expansion options that are used during search
** *`job_id` (Optional, string)*: Identifier for the anomaly detection job.
** *`max_empty_searches` (Optional, number)*: If a real-time datafeed has never seen any data (including during any initial training period), it automatically
stops and closes the associated job after this many real-time searches return no documents. In other words,
it stops after `frequency` times `max_empty_searches` of real-time operation. If not set, a datafeed with no
end time that sees no data remains started until it is explicitly stopped. By default, it is not set.
** *`query` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: The Elasticsearch query domain-specific language (DSL). This value corresponds to the query object in an
Elasticsearch search POST body. All the options that are supported by Elasticsearch can be used, as this
object is passed verbatim to Elasticsearch.
** *`query_delay` (Optional, string | -1 | 0)*: The number of seconds behind real time that data is queried. For example, if data from 10:04 a.m. might
not be searchable in Elasticsearch until 10:06 a.m., set this property to 120 seconds. The default
value is randomly selected between `60s` and `120s`. This randomness improves the query performance
when there are multiple jobs running on the same node.
** *`runtime_mappings` (Optional, Record<string, { fields, fetch_fields, format, input_field, target_field, target_index, script, type }>)*: Specifies runtime fields for the datafeed search.
** *`script_fields` (Optional, Record<string, { script, ignore_failure }>)*: Specifies scripts that evaluate custom expressions and returns script fields to the datafeed.
The detector configuration objects in a job can contain functions that use these script fields.
** *`scroll_size` (Optional, number)*: The size parameter that is used in Elasticsearch searches when the datafeed does not use aggregations.
The maximum value is the value of `index.max_result_window`, which is 10,000 by default.
** *`headers` (Optional, Record<string, string | string[]>)*
** *`allow_no_indices` (Optional, boolean)*: If true, wildcard indices expressions that resolve into no concrete indices are ignored. This includes the `_all`
string or when no indices are specified.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match. If the request can target data streams, this argument determines
whether wildcard expressions match hidden data streams. Supports a list of values.
** *`ignore_throttled` (Optional, boolean)*: If true, concrete, expanded, or aliased indices are ignored when frozen.
** *`ignore_unavailable` (Optional, boolean)*: If true, unavailable indices (missing or closed) are ignored.

[discrete]
==== put_filter
Create a filter.
A filter contains a list of strings. It can be used by one or more anomaly detection jobs.
Specifically, filters are referenced in the `custom_rules` property of detector configuration objects.

{ref}/ml-put-filter.html[Endpoint documentation]
[source,ts]
----
client.ml.putFilter({ filter_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`filter_id` (string)*: A string that uniquely identifies a filter.
** *`description` (Optional, string)*: A description of the filter.
** *`items` (Optional, string[])*: The items of the filter. A wildcard `*` can be used at the beginning or the end of an item.
Up to 10000 items are allowed in each filter.

[discrete]
==== put_job
Create an anomaly detection job.
If you include a `datafeed_config`, you must have read index privileges on the source index.

{ref}/ml-put-job.html[Endpoint documentation]
[source,ts]
----
client.ml.putJob({ job_id, analysis_config, data_description })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: The identifier for the anomaly detection job. This identifier can contain lowercase alphanumeric characters (a-z and 0-9), hyphens, and underscores. It must start and end with alphanumeric characters.
** *`analysis_config` ({ bucket_span, categorization_analyzer, categorization_field_name, categorization_filters, detectors, influencers, latency, model_prune_window, multivariate_by_fields, per_partition_categorization, summary_count_field_name })*: Specifies how to analyze the data. After you create a job, you cannot change the analysis configuration; all the properties are informational.
** *`data_description` ({ format, time_field, time_format, field_delimiter })*: Defines the format of the input data when you send data to the job by using the post data API. Note that when configure a datafeed, these properties are automatically set. When data is received via the post data API, it is not stored in Elasticsearch. Only the results for anomaly detection are retained.
** *`allow_lazy_open` (Optional, boolean)*: Advanced configuration option. Specifies whether this job can open when there is insufficient machine learning node capacity for it to be immediately assigned to a node. By default, if a machine learning node with capacity to run the job cannot immediately be found, the open anomaly detection jobs API returns an error. However, this is also subject to the cluster-wide `xpack.ml.max_lazy_ml_nodes` setting. If this option is set to true, the open anomaly detection jobs API does not return an error and the job waits in the opening state until sufficient machine learning node capacity is available.
** *`analysis_limits` (Optional, { categorization_examples_limit, model_memory_limit })*: Limits can be applied for the resources required to hold the mathematical models in memory. These limits are approximate and can be set per job. They do not control the memory used by other processes, for example the Elasticsearch Java processes.
** *`background_persist_interval` (Optional, string | -1 | 0)*: Advanced configuration option. The time between each periodic persistence of the model. The default value is a randomized value between 3 to 4 hours, which avoids all jobs persisting at exactly the same time. The smallest allowed value is 1 hour. For very large models (several GB), persistence could take 10-20 minutes, so do not set the `background_persist_interval` value too low.
** *`custom_settings` (Optional, User-defined value)*: Advanced configuration option. Contains custom meta data about the job.
** *`daily_model_snapshot_retention_after_days` (Optional, number)*: Advanced configuration option, which affects the automatic removal of old model snapshots for this job. It specifies a period of time (in days) after which only the first snapshot per day is retained. This period is relative to the timestamp of the most recent snapshot for this job. Valid values range from 0 to `model_snapshot_retention_days`.
** *`datafeed_config` (Optional, { aggregations, chunking_config, datafeed_id, delayed_data_check_config, frequency, indices, indices_options, job_id, max_empty_searches, query, query_delay, runtime_mappings, script_fields, scroll_size })*: Defines a datafeed for the anomaly detection job. If Elasticsearch security features are enabled, your datafeed remembers which roles the user who created it had at the time of creation and runs the query using those same roles. If you provide secondary authorization headers, those credentials are used instead.
** *`description` (Optional, string)*: A description of the job.
** *`groups` (Optional, string[])*: A list of job groups. A job can belong to no groups or many.
** *`model_plot_config` (Optional, { annotations_enabled, enabled, terms })*: This advanced configuration option stores model information along with the results. It provides a more detailed view into anomaly detection. If you enable model plot it can add considerable overhead to the performance of the system; it is not feasible for jobs with many entities. Model plot provides a simplified and indicative view of the model and its bounds. It does not display complex features such as multivariate correlations or multimodal data. As such, anomalies may occasionally be reported which cannot be seen in the model plot. Model plot config can be configured when the job is created or updated later. It must be disabled if performance issues are experienced.
** *`model_snapshot_retention_days` (Optional, number)*: Advanced configuration option, which affects the automatic removal of old model snapshots for this job. It specifies the maximum period of time (in days) that snapshots are retained. This period is relative to the timestamp of the most recent snapshot for this job. By default, snapshots ten days older than the newest snapshot are deleted.
** *`renormalization_window_days` (Optional, number)*: Advanced configuration option. The period over which adjustments to the score are applied, as new data is seen. The default value is the longer of 30 days or 100 bucket spans.
** *`results_index_name` (Optional, string)*: A text string that affects the name of the machine learning results index. By default, the job generates an index named `.ml-anomalies-shared`.
** *`results_retention_days` (Optional, number)*: Advanced configuration option. The period of time (in days) that results are retained. Age is calculated relative to the timestamp of the latest bucket result. If this property has a non-null value, once per day at 00:30 (server time), results that are the specified number of days older than the latest bucket result are deleted from Elasticsearch. The default value is null, which means all results are retained. Annotations generated by the system also count as results for retention purposes; they are deleted after the same number of days as results. Annotations added by users are retained forever.

[discrete]
==== put_trained_model
Create a trained model.
Enable you to supply a trained model that is not created by data frame analytics.

{ref}/put-trained-models.html[Endpoint documentation]
[source,ts]
----
client.ml.putTrainedModel({ model_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (string)*: The unique identifier of the trained model.
** *`compressed_definition` (Optional, string)*: The compressed (GZipped and Base64 encoded) inference definition of the
model. If compressed_definition is specified, then definition cannot be
specified.
** *`definition` (Optional, { preprocessors, trained_model })*: The inference definition for the model. If definition is specified, then
compressed_definition cannot be specified.
** *`description` (Optional, string)*: A human-readable description of the inference trained model.
** *`inference_config` (Optional, { regression, classification, text_classification, zero_shot_classification, fill_mask, ner, pass_through, text_embedding, text_expansion, question_answering })*: The default configuration for inference. This can be either a regression
or classification configuration. It must match the underlying
definition.trained_model's target_type. For pre-packaged models such as
ELSER the config is not required.
** *`input` (Optional, { field_names })*: The input field names for the model definition.
** *`metadata` (Optional, User-defined value)*: An object map that contains metadata about the model.
** *`model_type` (Optional, Enum("tree_ensemble" | "lang_ident" | "pytorch"))*: The model type.
** *`model_size_bytes` (Optional, number)*: The estimated memory usage in bytes to keep the trained model in memory.
This property is supported only if defer_definition_decompression is true
or the model definition is not supplied.
** *`platform_architecture` (Optional, string)*: The platform architecture (if applicable) of the trained mode. If the model
only works on one platform, because it is heavily optimized for a particular
processor architecture and OS combination, then this field specifies which.
The format of the string must match the platform identifiers used by Elasticsearch,
so one of, `linux-x86_64`, `linux-aarch64`, `darwin-x86_64`, `darwin-aarch64`,
or `windows-x86_64`. For portable models (those that work independent of processor
architecture or OS features), leave this field unset.
** *`tags` (Optional, string[])*: An array of tags to organize the model.
** *`prefix_strings` (Optional, { ingest, search })*: Optional prefix strings applied at inference
** *`defer_definition_decompression` (Optional, boolean)*: If set to `true` and a `compressed_definition` is provided,
the request defers definition decompression and skips relevant
validations.
** *`wait_for_completion` (Optional, boolean)*: Whether to wait for all child operations (e.g. model download)
to complete.

[discrete]
==== put_trained_model_alias
Create or update a trained model alias.
A trained model alias is a logical name used to reference a single trained
model.
You can use aliases instead of trained model identifiers to make it easier to
reference your models. For example, you can use aliases in inference
aggregations and processors.
An alias must be unique and refer to only a single trained model. However,
you can have multiple aliases for each trained model.
If you use this API to update an alias such that it references a different
trained model ID and the model uses a different type of data frame analytics,
an error occurs. For example, this situation occurs if you have a trained
model for regression analysis and a trained model for classification
analysis; you cannot reassign an alias from one type of trained model to
another.
If you use this API to update an alias and there are very few input fields in
common between the old and new trained models for the model alias, the API
returns a warning.

{ref}/put-trained-models-aliases.html[Endpoint documentation]
[source,ts]
----
client.ml.putTrainedModelAlias({ model_alias, model_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_alias` (string)*: The alias to create or update. This value cannot end in numbers.
** *`model_id` (string)*: The identifier for the trained model that the alias refers to.
** *`reassign` (Optional, boolean)*: Specifies whether the alias gets reassigned to the specified trained
model if it is already assigned to a different model. If the alias is
already assigned and this parameter is false, the API returns an error.

[discrete]
==== put_trained_model_definition_part
Create part of a trained model definition.

{ref}/put-trained-model-definition-part.html[Endpoint documentation]
[source,ts]
----
client.ml.putTrainedModelDefinitionPart({ model_id, part, definition, total_definition_length, total_parts })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (string)*: The unique identifier of the trained model.
** *`part` (number)*: The definition part number. When the definition is loaded for inference the definition parts are streamed in the
order of their part number. The first part must be `0` and the final part must be `total_parts - 1`.
** *`definition` (string)*: The definition part for the model. Must be a base64 encoded string.
** *`total_definition_length` (number)*: The total uncompressed definition length in bytes. Not base64 encoded.
** *`total_parts` (number)*: The total number of parts that will be uploaded. Must be greater than 0.

[discrete]
==== put_trained_model_vocabulary
Create a trained model vocabulary.
This API is supported only for natural language processing (NLP) models.
The vocabulary is stored in the index as described in `inference_config.*.vocabulary` of the trained model definition.

{ref}/put-trained-model-vocabulary.html[Endpoint documentation]
[source,ts]
----
client.ml.putTrainedModelVocabulary({ model_id, vocabulary })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (string)*: The unique identifier of the trained model.
** *`vocabulary` (string[])*: The model vocabulary, which must not be empty.
** *`merges` (Optional, string[])*: The optional model merges if required by the tokenizer.
** *`scores` (Optional, number[])*: The optional vocabulary value scores if required by the tokenizer.

[discrete]
==== reset_job
Reset an anomaly detection job.
All model state and results are deleted. The job is ready to start over as if
it had just been created.
It is not currently possible to reset multiple jobs using wildcards or a
comma separated list.

{ref}/ml-reset-job.html[Endpoint documentation]
[source,ts]
----
client.ml.resetJob({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: The ID of the job to reset.
** *`wait_for_completion` (Optional, boolean)*: Should this request wait until the operation has completed before
returning.
** *`delete_user_annotations` (Optional, boolean)*: Specifies whether annotations that have been added by the
user should be deleted along with any auto-generated annotations when the job is
reset.

[discrete]
==== revert_model_snapshot
Revert to a snapshot.
The machine learning features react quickly to anomalous input, learning new
behaviors in data. Highly anomalous input increases the variance in the
models whilst the system learns whether this is a new step-change in behavior
or a one-off event. In the case where this anomalous input is known to be a
one-off, then it might be appropriate to reset the model state to a time
before this event. For example, you might consider reverting to a saved
snapshot after Black Friday or a critical system failure.

{ref}/ml-revert-snapshot.html[Endpoint documentation]
[source,ts]
----
client.ml.revertModelSnapshot({ job_id, snapshot_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`snapshot_id` (string)*: You can specify `empty` as the <snapshot_id>. Reverting to the empty
snapshot means the anomaly detection job starts learning a new model from
scratch when it is started.
** *`delete_intervening_results` (Optional, boolean)*: Refer to the description for the `delete_intervening_results` query parameter.

[discrete]
==== set_upgrade_mode
Set upgrade_mode for ML indices.
Sets a cluster wide upgrade_mode setting that prepares machine learning
indices for an upgrade.
When upgrading your cluster, in some circumstances you must restart your
nodes and reindex your machine learning indices. In those circumstances,
there must be no machine learning jobs running. You can close the machine
learning jobs, do the upgrade, then open all the jobs again. Alternatively,
you can use this API to temporarily halt tasks associated with the jobs and
datafeeds and prevent new jobs from opening. You can also use this API
during upgrades that do not require you to reindex your machine learning
indices, though stopping jobs is not a requirement in that case.
You can see the current value for the upgrade_mode setting by using the get
machine learning info API.

{ref}/ml-set-upgrade-mode.html[Endpoint documentation]
[source,ts]
----
client.ml.setUpgradeMode({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`enabled` (Optional, boolean)*: When `true`, it enables `upgrade_mode` which temporarily halts all job
and datafeed tasks and prohibits new job and datafeed tasks from
starting.
** *`timeout` (Optional, string | -1 | 0)*: The time to wait for the request to be completed.

[discrete]
==== start_data_frame_analytics
Start a data frame analytics job.
A data frame analytics job can be started and stopped multiple times
throughout its lifecycle.
If the destination index does not exist, it is created automatically the
first time you start the data frame analytics job. The
`index.number_of_shards` and `index.number_of_replicas` settings for the
destination index are copied from the source index. If there are multiple
source indices, the destination index copies the highest setting values. The
mappings for the destination index are also copied from the source indices.
If there are any mapping conflicts, the job fails to start.
If the destination index exists, it is used as is. You can therefore set up
the destination index in advance with custom settings and mappings.

{ref}/start-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.startDataFrameAnalytics({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the data frame analytics job. This identifier can contain
lowercase alphanumeric characters (a-z and 0-9), hyphens, and
underscores. It must start and end with alphanumeric characters.
** *`timeout` (Optional, string | -1 | 0)*: Controls the amount of time to wait until the data frame analytics job
starts.

[discrete]
==== start_datafeed
Start datafeeds.

A datafeed must be started in order to retrieve data from Elasticsearch. A datafeed can be started and stopped
multiple times throughout its lifecycle.

Before you can start a datafeed, the anomaly detection job must be open. Otherwise, an error occurs.

If you restart a stopped datafeed, it continues processing input data from the next millisecond after it was stopped.
If new data was indexed for that exact millisecond between stopping and starting, it will be ignored.

When Elasticsearch security features are enabled, your datafeed remembers which roles the last user to create or
update it had at the time of creation or update and runs the query using those same roles. If you provided secondary
authorization headers when you created or updated the datafeed, those credentials are used instead.

{ref}/ml-start-datafeed.html[Endpoint documentation]
[source,ts]
----
client.ml.startDatafeed({ datafeed_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (string)*: A numerical character string that uniquely identifies the datafeed. This identifier can contain lowercase
alphanumeric characters (a-z and 0-9), hyphens, and underscores. It must start and end with alphanumeric
characters.
** *`end` (Optional, string | Unit)*: Refer to the description for the `end` query parameter.
** *`start` (Optional, string | Unit)*: Refer to the description for the `start` query parameter.
** *`timeout` (Optional, string | -1 | 0)*: Refer to the description for the `timeout` query parameter.

[discrete]
==== start_trained_model_deployment
Start a trained model deployment.
It allocates the model to every machine learning node.

{ref}/start-trained-model-deployment.html[Endpoint documentation]
[source,ts]
----
client.ml.startTrainedModelDeployment({ model_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (string)*: The unique identifier of the trained model. Currently, only PyTorch models are supported.
** *`cache_size` (Optional, number | string)*: The inference cache size (in memory outside the JVM heap) per node for the model.
The default value is the same size as the `model_size_bytes`. To disable the cache,
`0b` can be provided.
** *`deployment_id` (Optional, string)*: A unique identifier for the deployment of the model.
** *`number_of_allocations` (Optional, number)*: The number of model allocations on each node where the model is deployed.
All allocations on a node share the same copy of the model in memory but use
a separate set of threads to evaluate the model.
Increasing this value generally increases the throughput.
If this setting is greater than the number of hardware threads
it will automatically be changed to a value less than the number of hardware threads.
** *`priority` (Optional, Enum("normal" | "low"))*: The deployment priority.
** *`queue_capacity` (Optional, number)*: Specifies the number of inference requests that are allowed in the queue. After the number of requests exceeds
this value, new requests are rejected with a 429 error.
** *`threads_per_allocation` (Optional, number)*: Sets the number of threads used by each model allocation during inference. This generally increases
the inference speed. The inference process is a compute-bound process; any number
greater than the number of available hardware threads on the machine does not increase the
inference speed. If this setting is greater than the number of hardware threads
it will automatically be changed to a value less than the number of hardware threads.
** *`timeout` (Optional, string | -1 | 0)*: Specifies the amount of time to wait for the model to deploy.
** *`wait_for` (Optional, Enum("started" | "starting" | "fully_allocated"))*: Specifies the allocation status to wait for before returning.

[discrete]
==== stop_data_frame_analytics
Stop data frame analytics jobs.
A data frame analytics job can be started and stopped multiple times
throughout its lifecycle.

{ref}/stop-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.stopDataFrameAnalytics({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the data frame analytics job. This identifier can contain
lowercase alphanumeric characters (a-z and 0-9), hyphens, and
underscores. It must start and end with alphanumeric characters.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no data frame analytics
jobs that match.
2. Contains the _all string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

The default value is true, which returns an empty data_frame_analytics
array when there are no matches and the subset of results when there are
partial matches. If this parameter is false, the request returns a 404
status code when there are no matches or only partial matches.
** *`force` (Optional, boolean)*: If true, the data frame analytics job is stopped forcefully.
** *`timeout` (Optional, string | -1 | 0)*: Controls the amount of time to wait until the data frame analytics job
stops. Defaults to 20 seconds.

[discrete]
==== stop_datafeed
Stop datafeeds.
A datafeed that is stopped ceases to retrieve data from Elasticsearch. A datafeed can be started and stopped
multiple times throughout its lifecycle.

{ref}/ml-stop-datafeed.html[Endpoint documentation]
[source,ts]
----
client.ml.stopDatafeed({ datafeed_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (string)*: Identifier for the datafeed. You can stop multiple datafeeds in a single API request by using a comma-separated
list of datafeeds or a wildcard expression. You can close all datafeeds by using `_all` or by specifying `*` as
the identifier.
** *`allow_no_match` (Optional, boolean)*: Refer to the description for the `allow_no_match` query parameter.
** *`force` (Optional, boolean)*: Refer to the description for the `force` query parameter.
** *`timeout` (Optional, string | -1 | 0)*: Refer to the description for the `timeout` query parameter.

[discrete]
==== stop_trained_model_deployment
Stop a trained model deployment.

{ref}/stop-trained-model-deployment.html[Endpoint documentation]
[source,ts]
----
client.ml.stopTrainedModelDeployment({ model_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (string)*: The unique identifier of the trained model.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request: contains wildcard expressions and there are no deployments that match;
contains the  `_all` string or no identifiers and there are no matches; or contains wildcard expressions and
there are only partial matches. By default, it returns an empty array when there are no matches and the subset of results when there are partial matches.
If `false`, the request returns a 404 status code when there are no matches or only partial matches.
** *`force` (Optional, boolean)*: Forcefully stops the deployment, even if it is used by ingest pipelines. You can't use these pipelines until you
restart the model deployment.

[discrete]
==== update_data_frame_analytics
Update a data frame analytics job.

{ref}/update-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.updateDataFrameAnalytics({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the data frame analytics job. This identifier can contain
lowercase alphanumeric characters (a-z and 0-9), hyphens, and
underscores. It must start and end with alphanumeric characters.
** *`description` (Optional, string)*: A description of the job.
** *`model_memory_limit` (Optional, string)*: The approximate maximum amount of memory resources that are permitted for
analytical processing. If your `elasticsearch.yml` file contains an
`xpack.ml.max_model_memory_limit` setting, an error occurs when you try
to create data frame analytics jobs that have `model_memory_limit` values
greater than that setting.
** *`max_num_threads` (Optional, number)*: The maximum number of threads to be used by the analysis. Using more
threads may decrease the time necessary to complete the analysis at the
cost of using more CPU. Note that the process may use additional threads
for operational functionality other than the analysis itself.
** *`allow_lazy_start` (Optional, boolean)*: Specifies whether this job can start when there is insufficient machine
learning node capacity for it to be immediately assigned to a node.

[discrete]
==== update_datafeed
Update a datafeed.
You must stop and start the datafeed for the changes to be applied.
When Elasticsearch security features are enabled, your datafeed remembers which roles the user who updated it had at
the time of the update and runs the query using those same roles. If you provide secondary authorization headers,
those credentials are used instead.

{ref}/ml-update-datafeed.html[Endpoint documentation]
[source,ts]
----
client.ml.updateDatafeed({ datafeed_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (string)*: A numerical character string that uniquely identifies the datafeed.
This identifier can contain lowercase alphanumeric characters (a-z and 0-9), hyphens, and underscores.
It must start and end with alphanumeric characters.
** *`aggregations` (Optional, Record<string, { aggregations, meta, adjacency_matrix, auto_date_histogram, avg, avg_bucket, boxplot, bucket_script, bucket_selector, bucket_sort, bucket_count_ks_test, bucket_correlation, cardinality, categorize_text, children, composite, cumulative_cardinality, cumulative_sum, date_histogram, date_range, derivative, diversified_sampler, extended_stats, extended_stats_bucket, frequent_item_sets, filter, filters, geo_bounds, geo_centroid, geo_distance, geohash_grid, geo_line, geotile_grid, geohex_grid, global, histogram, ip_range, ip_prefix, inference, line, matrix_stats, max, max_bucket, median_absolute_deviation, min, min_bucket, missing, moving_avg, moving_percentiles, moving_fn, multi_terms, nested, normalize, parent, percentile_ranks, percentiles, percentiles_bucket, range, rare_terms, rate, reverse_nested, random_sampler, sampler, scripted_metric, serial_diff, significant_terms, significant_text, stats, stats_bucket, string_stats, sum, sum_bucket, terms, time_series, top_hits, t_test, top_metrics, value_count, weighted_avg, variable_width_histogram }>)*: If set, the datafeed performs aggregation searches. Support for aggregations is limited and should be used only
with low cardinality data.
** *`chunking_config` (Optional, { mode, time_span })*: Datafeeds might search over long time periods, for several months or years. This search is split into time
chunks in order to ensure the load on Elasticsearch is managed. Chunking configuration controls how the size of
these time chunks are calculated; it is an advanced configuration option.
** *`delayed_data_check_config` (Optional, { check_window, enabled })*: Specifies whether the datafeed checks for missing data and the size of the window. The datafeed can optionally
search over indices that have already been read in an effort to determine whether any data has subsequently been
added to the index. If missing data is found, it is a good indication that the `query_delay` is set too low and
the data is being indexed after the datafeed has passed that moment in time. This check runs only on real-time
datafeeds.
** *`frequency` (Optional, string | -1 | 0)*: The interval at which scheduled queries are made while the datafeed runs in real time. The default value is
either the bucket span for short bucket spans, or, for longer bucket spans, a sensible fraction of the bucket
span. When `frequency` is shorter than the bucket span, interim results for the last (partial) bucket are
written then eventually overwritten by the full bucket results. If the datafeed uses aggregations, this value
must be divisible by the interval of the date histogram aggregation.
** *`indices` (Optional, string[])*: An array of index names. Wildcards are supported. If any of the indices are in remote clusters, the machine
learning nodes must have the `remote_cluster_client` role.
** *`indices_options` (Optional, { allow_no_indices, expand_wildcards, ignore_unavailable, ignore_throttled })*: Specifies index expansion options that are used during search.
** *`job_id` (Optional, string)*
** *`max_empty_searches` (Optional, number)*: If a real-time datafeed has never seen any data (including during any initial training period), it automatically
stops and closes the associated job after this many real-time searches return no documents. In other words,
it stops after `frequency` times `max_empty_searches` of real-time operation. If not set, a datafeed with no
end time that sees no data remains started until it is explicitly stopped. By default, it is not set.
** *`query` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: The Elasticsearch query domain-specific language (DSL). This value corresponds to the query object in an
Elasticsearch search POST body. All the options that are supported by Elasticsearch can be used, as this
object is passed verbatim to Elasticsearch. Note that if you change the query, the analyzed data is also
changed. Therefore, the time required to learn might be long and the understandability of the results is
unpredictable. If you want to make significant changes to the source data, it is recommended that you
clone the job and datafeed and make the amendments in the clone. Let both run in parallel and close one
when you are satisfied with the results of the job.
** *`query_delay` (Optional, string | -1 | 0)*: The number of seconds behind real time that data is queried. For example, if data from 10:04 a.m. might
not be searchable in Elasticsearch until 10:06 a.m., set this property to 120 seconds. The default
value is randomly selected between `60s` and `120s`. This randomness improves the query performance
when there are multiple jobs running on the same node.
** *`runtime_mappings` (Optional, Record<string, { fields, fetch_fields, format, input_field, target_field, target_index, script, type }>)*: Specifies runtime fields for the datafeed search.
** *`script_fields` (Optional, Record<string, { script, ignore_failure }>)*: Specifies scripts that evaluate custom expressions and returns script fields to the datafeed.
The detector configuration objects in a job can contain functions that use these script fields.
** *`scroll_size` (Optional, number)*: The size parameter that is used in Elasticsearch searches when the datafeed does not use aggregations.
The maximum value is the value of `index.max_result_window`.
** *`allow_no_indices` (Optional, boolean)*: If `true`, wildcard indices expressions that resolve into no concrete indices are ignored. This includes the
`_all` string or when no indices are specified.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match. If the request can target data streams, this argument determines
whether wildcard expressions match hidden data streams. Supports a list of values. Valid values are:

* `all`: Match any data stream or index, including hidden ones.
* `closed`: Match closed, non-hidden indices. Also matches any non-hidden data stream. Data streams cannot be closed.
* `hidden`: Match hidden data streams and hidden indices. Must be combined with `open`, `closed`, or both.
* `none`: Wildcard patterns are not accepted.
* `open`: Match open, non-hidden indices. Also matches any non-hidden data stream.
** *`ignore_throttled` (Optional, boolean)*: If `true`, concrete, expanded or aliased indices are ignored when frozen.
** *`ignore_unavailable` (Optional, boolean)*: If `true`, unavailable indices (missing or closed) are ignored.

[discrete]
==== update_filter
Update a filter.
Updates the description of a filter, adds items, or removes items from the list.

{ref}/ml-update-filter.html[Endpoint documentation]
[source,ts]
----
client.ml.updateFilter({ filter_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`filter_id` (string)*: A string that uniquely identifies a filter.
** *`add_items` (Optional, string[])*: The items to add to the filter.
** *`description` (Optional, string)*: A description for the filter.
** *`remove_items` (Optional, string[])*: The items to remove from the filter.

[discrete]
==== update_job
Update an anomaly detection job.
Updates certain properties of an anomaly detection job.

{ref}/ml-update-job.html[Endpoint documentation]
[source,ts]
----
client.ml.updateJob({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the job.
** *`allow_lazy_open` (Optional, boolean)*: Advanced configuration option. Specifies whether this job can open when
there is insufficient machine learning node capacity for it to be
immediately assigned to a node. If `false` and a machine learning node
with capacity to run the job cannot immediately be found, the open
anomaly detection jobs API returns an error. However, this is also
subject to the cluster-wide `xpack.ml.max_lazy_ml_nodes` setting. If this
option is set to `true`, the open anomaly detection jobs API does not
return an error and the job waits in the opening state until sufficient
machine learning node capacity is available.
** *`analysis_limits` (Optional, { model_memory_limit })*
** *`background_persist_interval` (Optional, string | -1 | 0)*: Advanced configuration option. The time between each periodic persistence
of the model.
The default value is a randomized value between 3 to 4 hours, which
avoids all jobs persisting at exactly the same time. The smallest allowed
value is 1 hour.
For very large models (several GB), persistence could take 10-20 minutes,
so do not set the value too low.
If the job is open when you make the update, you must stop the datafeed,
close the job, then reopen the job and restart the datafeed for the
changes to take effect.
** *`custom_settings` (Optional, Record<string, User-defined value>)*: Advanced configuration option. Contains custom meta data about the job.
For example, it can contain custom URL information as shown in Adding
custom URLs to machine learning results.
** *`categorization_filters` (Optional, string[])*
** *`description` (Optional, string)*: A description of the job.
** *`model_plot_config` (Optional, { annotations_enabled, enabled, terms })*
** *`model_prune_window` (Optional, string | -1 | 0)*
** *`daily_model_snapshot_retention_after_days` (Optional, number)*: Advanced configuration option, which affects the automatic removal of old
model snapshots for this job. It specifies a period of time (in days)
after which only the first snapshot per day is retained. This period is
relative to the timestamp of the most recent snapshot for this job. Valid
values range from 0 to `model_snapshot_retention_days`. For jobs created
before version 7.8.0, the default value matches
`model_snapshot_retention_days`.
** *`model_snapshot_retention_days` (Optional, number)*: Advanced configuration option, which affects the automatic removal of old
model snapshots for this job. It specifies the maximum period of time (in
days) that snapshots are retained. This period is relative to the
timestamp of the most recent snapshot for this job.
** *`renormalization_window_days` (Optional, number)*: Advanced configuration option. The period over which adjustments to the
score are applied, as new data is seen.
** *`results_retention_days` (Optional, number)*: Advanced configuration option. The period of time (in days) that results
are retained. Age is calculated relative to the timestamp of the latest
bucket result. If this property has a non-null value, once per day at
00:30 (server time), results that are the specified number of days older
than the latest bucket result are deleted from Elasticsearch. The default
value is null, which means all results are retained.
** *`groups` (Optional, string[])*: A list of job groups. A job can belong to no groups or many.
** *`detectors` (Optional, { by_field_name, custom_rules, detector_description, detector_index, exclude_frequent, field_name, function, over_field_name, partition_field_name, use_null }[])*: An array of detector update objects.
** *`per_partition_categorization` (Optional, { enabled, stop_on_warn })*: Settings related to how categorization interacts with partition fields.

[discrete]
==== update_model_snapshot
Update a snapshot.
Updates certain properties of a snapshot.

{ref}/ml-update-snapshot.html[Endpoint documentation]
[source,ts]
----
client.ml.updateModelSnapshot({ job_id, snapshot_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`snapshot_id` (string)*: Identifier for the model snapshot.
** *`description` (Optional, string)*: A description of the model snapshot.
** *`retain` (Optional, boolean)*: If `true`, this snapshot will not be deleted during automatic cleanup of
snapshots older than `model_snapshot_retention_days`. However, this
snapshot will be deleted when the job is deleted.

[discrete]
==== update_trained_model_deployment
Update a trained model deployment.

{ref}/update-trained-model-deployment.html[Endpoint documentation]
[source,ts]
----
client.ml.updateTrainedModelDeployment({ model_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (string)*: The unique identifier of the trained model. Currently, only PyTorch models are supported.
** *`number_of_allocations` (Optional, number)*: The number of model allocations on each node where the model is deployed.
All allocations on a node share the same copy of the model in memory but use
a separate set of threads to evaluate the model.
Increasing this value generally increases the throughput.
If this setting is greater than the number of hardware threads
it will automatically be changed to a value less than the number of hardware threads.

[discrete]
==== upgrade_job_snapshot
Upgrade a snapshot.
Upgrades an anomaly detection model snapshot to the latest major version.
Over time, older snapshot formats are deprecated and removed. Anomaly
detection jobs support only snapshots that are from the current or previous
major version.
This API provides a means to upgrade a snapshot to the current major version.
This aids in preparing the cluster for an upgrade to the next major version.
Only one snapshot per anomaly detection job can be upgraded at a time and the
upgraded snapshot cannot be the current snapshot of the anomaly detection
job.

{ref}/ml-upgrade-job-model-snapshot.html[Endpoint documentation]
[source,ts]
----
client.ml.upgradeJobSnapshot({ job_id, snapshot_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`snapshot_id` (string)*: A numerical character string that uniquely identifies the model snapshot.
** *`wait_for_completion` (Optional, boolean)*: When true, the API won’t respond until the upgrade is complete.
Otherwise, it responds as soon as the upgrade task is assigned to a node.
** *`timeout` (Optional, string | -1 | 0)*: Controls the time to wait for the request to complete.

[discrete]
=== monitoring
[discrete]
==== bulk
Used by the monitoring features to send monitoring data.

{ref}/monitor-elasticsearch-cluster.html[Endpoint documentation]
[source,ts]
----
client.monitoring.bulk({ system_id, system_api_version, interval })
----

[discrete]
==== Arguments

* *Request (object):*
** *`system_id` (string)*: Identifier of the monitored system
** *`system_api_version` (string)*
** *`interval` (string | -1 | 0)*: Collection interval (e.g., '10s' or '10000ms') of the payload
** *`type` (Optional, string)*: Default document type for items which don't provide one
** *`operations` (Optional, { index, create, update, delete } | { detect_noop, doc, doc_as_upsert, script, scripted_upsert, _source, upsert } | object[])*

[discrete]
=== nodes
[discrete]
==== clear_repositories_metering_archive
You can use this API to clear the archived repositories metering information in the cluster.

{ref}/clear-repositories-metering-archive-api.html[Endpoint documentation]
[source,ts]
----
client.nodes.clearRepositoriesMeteringArchive({ node_id, max_archive_version })
----

[discrete]
==== Arguments

* *Request (object):*
** *`node_id` (string | string[])*: List of node IDs or names used to limit returned information.
All the nodes selective options are explained [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster.html#cluster-nodes).
** *`max_archive_version` (number)*: Specifies the maximum [archive_version](https://www.elastic.co/guide/en/elasticsearch/reference/current/get-repositories-metering-api.html#get-repositories-metering-api-response-body) to be cleared from the archive.

[discrete]
==== get_repositories_metering_info
You can use the cluster repositories metering API to retrieve repositories metering information in a cluster.
This API exposes monotonically non-decreasing counters and it’s expected that clients would durably store the
information needed to compute aggregations over a period of time. Additionally, the information exposed by this
API is volatile, meaning that it won’t be present after node restarts.

{ref}/get-repositories-metering-api.html[Endpoint documentation]
[source,ts]
----
client.nodes.getRepositoriesMeteringInfo({ node_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`node_id` (string | string[])*: List of node IDs or names used to limit returned information.
All the nodes selective options are explained [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster.html#cluster-nodes).

[discrete]
==== hot_threads
This API yields a breakdown of the hot threads on each selected node in the cluster.
The output is plain text with a breakdown of each node’s top hot threads.

{ref}/cluster-nodes-hot-threads.html[Endpoint documentation]
[source,ts]
----
client.nodes.hotThreads({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`node_id` (Optional, string | string[])*: List of node IDs or names used to limit returned information.
** *`ignore_idle_threads` (Optional, boolean)*: If true, known idle threads (e.g. waiting in a socket select, or to get
a task from an empty queue) are filtered out.
** *`interval` (Optional, string | -1 | 0)*: The interval to do the second sampling of threads.
** *`snapshots` (Optional, number)*: Number of samples of thread stacktrace.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response
is received before the timeout expires, the request fails and
returns an error.
** *`threads` (Optional, number)*: Specifies the number of hot threads to provide information for.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received
before the timeout expires, the request fails and returns an error.
** *`type` (Optional, Enum("cpu" | "wait" | "block" | "gpu" | "mem"))*: The type to sample.
** *`sort` (Optional, Enum("cpu" | "wait" | "block" | "gpu" | "mem"))*: The sort order for 'cpu' type (default: total)

[discrete]
==== info
Returns cluster nodes information.

{ref}/cluster-nodes-info.html[Endpoint documentation]
[source,ts]
----
client.nodes.info({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`node_id` (Optional, string | string[])*: List of node IDs or names used to limit returned information.
** *`metric` (Optional, string | string[])*: Limits the information returned to the specific metrics. Supports a list, such as http,ingest.
** *`flat_settings` (Optional, boolean)*: If true, returns settings in flat format.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== reload_secure_settings
Reloads the keystore on nodes in the cluster.

{ref}/secure-settings.html[Endpoint documentation]
[source,ts]
----
client.nodes.reloadSecureSettings({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`node_id` (Optional, string | string[])*: The names of particular nodes in the cluster to target.
** *`secure_settings_password` (Optional, string)*: The password for the Elasticsearch keystore.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== stats
Returns cluster nodes statistics.

{ref}/cluster-nodes-stats.html[Endpoint documentation]
[source,ts]
----
client.nodes.stats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`node_id` (Optional, string | string[])*: List of node IDs or names used to limit returned information.
** *`metric` (Optional, string | string[])*: Limit the information returned to the specified metrics
** *`index_metric` (Optional, string | string[])*: Limit the information returned for indices metric to the specific index metrics. It can be used only if indices (or all) metric is specified.
** *`completion_fields` (Optional, string | string[])*: List or wildcard expressions of fields to include in fielddata and suggest statistics.
** *`fielddata_fields` (Optional, string | string[])*: List or wildcard expressions of fields to include in fielddata statistics.
** *`fields` (Optional, string | string[])*: List or wildcard expressions of fields to include in the statistics.
** *`groups` (Optional, boolean)*: List of search groups to include in the search statistics.
** *`include_segment_file_sizes` (Optional, boolean)*: If true, the call reports the aggregated disk usage of each one of the Lucene index files (only applies if segment stats are requested).
** *`level` (Optional, Enum("cluster" | "indices" | "shards"))*: Indicates whether statistics are aggregated at the cluster, index, or shard level.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.
** *`types` (Optional, string[])*: A list of document types for the indexing index metric.
** *`include_unloaded_segments` (Optional, boolean)*: If `true`, the response includes information from segments that are not loaded into memory.

[discrete]
==== usage
Returns information on the usage of features.

{ref}/cluster-nodes-usage.html[Endpoint documentation]
[source,ts]
----
client.nodes.usage({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`node_id` (Optional, string | string[])*: A list of node IDs or names to limit the returned information; use `_local` to return information from the node you're connecting to, leave empty to get information from all nodes
** *`metric` (Optional, string | string[])*: Limits the information returned to the specific metrics.
A list of the following options: `_all`, `rest_actions`.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
=== query_rules
[discrete]
==== delete_rule
Deletes a query rule within a query ruleset.

{ref}/delete-query-rule.html[Endpoint documentation]
[source,ts]
----
client.queryRules.deleteRule({ ruleset_id, rule_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`ruleset_id` (string)*: The unique identifier of the query ruleset containing the rule to delete
** *`rule_id` (string)*: The unique identifier of the query rule within the specified ruleset to delete

[discrete]
==== delete_ruleset
Deletes a query ruleset.

{ref}/delete-query-ruleset.html[Endpoint documentation]
[source,ts]
----
client.queryRules.deleteRuleset({ ruleset_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`ruleset_id` (string)*: The unique identifier of the query ruleset to delete

[discrete]
==== get_rule
Returns the details about a query rule within a query ruleset

{ref}/get-query-rule.html[Endpoint documentation]
[source,ts]
----
client.queryRules.getRule({ ruleset_id, rule_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`ruleset_id` (string)*: The unique identifier of the query ruleset containing the rule to retrieve
** *`rule_id` (string)*: The unique identifier of the query rule within the specified ruleset to retrieve

[discrete]
==== get_ruleset
Returns the details about a query ruleset

{ref}/get-query-ruleset.html[Endpoint documentation]
[source,ts]
----
client.queryRules.getRuleset({ ruleset_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`ruleset_id` (string)*: The unique identifier of the query ruleset

[discrete]
==== list_rulesets
Returns summarized information about existing query rulesets.

{ref}/list-query-rulesets.html[Endpoint documentation]
[source,ts]
----
client.queryRules.listRulesets({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`from` (Optional, number)*: Starting offset (default: 0)
** *`size` (Optional, number)*: specifies a max number of results to get

[discrete]
==== put_rule
Creates or updates a query rule within a query ruleset.

{ref}/put-query-rule.html[Endpoint documentation]
[source,ts]
----
client.queryRules.putRule({ ruleset_id, rule_id, type, criteria, actions })
----

[discrete]
==== Arguments

* *Request (object):*
** *`ruleset_id` (string)*: The unique identifier of the query ruleset containing the rule to be created or updated
** *`rule_id` (string)*: The unique identifier of the query rule within the specified ruleset to be created or updated
** *`type` (Enum("pinned" | "exclude"))*
** *`criteria` ({ type, metadata, values } | { type, metadata, values }[])*
** *`actions` ({ ids, docs })*
** *`priority` (Optional, number)*

[discrete]
==== put_ruleset
Creates or updates a query ruleset.

{ref}/put-query-ruleset.html[Endpoint documentation]
[source,ts]
----
client.queryRules.putRuleset({ ruleset_id, rules })
----

[discrete]
==== Arguments

* *Request (object):*
** *`ruleset_id` (string)*: The unique identifier of the query ruleset to be created or updated
** *`rules` ({ rule_id, type, criteria, actions, priority } | { rule_id, type, criteria, actions, priority }[])*

[discrete]
==== test
Creates or updates a query ruleset.

{ref}/test-query-ruleset.html[Endpoint documentation]
[source,ts]
----
client.queryRules.test({ ruleset_id, match_criteria })
----

[discrete]
==== Arguments

* *Request (object):*
** *`ruleset_id` (string)*: The unique identifier of the query ruleset to be created or updated
** *`match_criteria` (Record<string, User-defined value>)*

[discrete]
=== rollup
[discrete]
==== delete_job
Deletes an existing rollup job.

{ref}/rollup-delete-job.html[Endpoint documentation]
[source,ts]
----
client.rollup.deleteJob({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the job.

[discrete]
==== get_jobs
Retrieves the configuration, stats, and status of rollup jobs.

{ref}/rollup-get-job.html[Endpoint documentation]
[source,ts]
----
client.rollup.getJobs({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: Identifier for the rollup job.
If it is `_all` or omitted, the API returns all rollup jobs.

[discrete]
==== get_rollup_caps
Returns the capabilities of any rollup jobs that have been configured for a specific index or index pattern.

{ref}/rollup-get-rollup-caps.html[Endpoint documentation]
[source,ts]
----
client.rollup.getRollupCaps({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: Index, indices or index-pattern to return rollup capabilities for.
`_all` may be used to fetch rollup capabilities from all jobs.

[discrete]
==== get_rollup_index_caps
Returns the rollup capabilities of all jobs inside of a rollup index (for example, the index where rollup data is stored).

{ref}/rollup-get-rollup-index-caps.html[Endpoint documentation]
[source,ts]
----
client.rollup.getRollupIndexCaps({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: Data stream or index to check for rollup capabilities.
Wildcard (`*`) expressions are supported.

[discrete]
==== put_job
Creates a rollup job.

{ref}/rollup-put-job.html[Endpoint documentation]
[source,ts]
----
client.rollup.putJob({ id, cron, groups, index_pattern, page_size, rollup_index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the rollup job. This can be any alphanumeric string and uniquely identifies the
data that is associated with the rollup job. The ID is persistent; it is stored with the rolled
up data. If you create a job, let it run for a while, then delete the job, the data that the job
rolled up is still be associated with this job ID. You cannot create a new job with the same ID
since that could lead to problems with mismatched job configurations.
** *`cron` (string)*: A cron string which defines the intervals when the rollup job should be executed. When the interval
triggers, the indexer attempts to rollup the data in the index pattern. The cron pattern is unrelated
to the time interval of the data being rolled up. For example, you may wish to create hourly rollups
of your document but to only run the indexer on a daily basis at midnight, as defined by the cron. The
cron pattern is defined just like a Watcher cron schedule.
** *`groups` ({ date_histogram, histogram, terms })*: Defines the grouping fields and aggregations that are defined for this rollup job. These fields will then be
available later for aggregating into buckets. These aggs and fields can be used in any combination. Think of
the groups configuration as defining a set of tools that can later be used in aggregations to partition the
data. Unlike raw data, we have to think ahead to which fields and aggregations might be used. Rollups provide
enough flexibility that you simply need to determine which fields are needed, not in what order they are needed.
** *`index_pattern` (string)*: The index or index pattern to roll up. Supports wildcard-style patterns (`logstash-*`). The job attempts to
rollup the entire index or index-pattern.
** *`page_size` (number)*: The number of bucket results that are processed on each iteration of the rollup indexer. A larger value tends
to execute faster, but requires more memory during processing. This value has no effect on how the data is
rolled up; it is merely used for tweaking the speed or memory cost of the indexer.
** *`rollup_index` (string)*: The index that contains the rollup results. The index can be shared with other rollup jobs. The data is stored so that it doesn’t interfere with unrelated jobs.
** *`metrics` (Optional, { field, metrics }[])*: Defines the metrics to collect for each grouping tuple. By default, only the doc_counts are collected for each
group. To make rollup useful, you will often add metrics like averages, mins, maxes, etc. Metrics are defined
on a per-field basis and for each field you configure which metric should be collected.
** *`timeout` (Optional, string | -1 | 0)*: Time to wait for the request to complete.
** *`headers` (Optional, Record<string, string | string[]>)*

[discrete]
==== rollup_search
Enables searching rolled-up data using the standard Query DSL.

{ref}/rollup-search.html[Endpoint documentation]
[source,ts]
----
client.rollup.rollupSearch({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: Enables searching rolled-up data using the standard Query DSL.
** *`aggregations` (Optional, Record<string, { aggregations, meta, adjacency_matrix, auto_date_histogram, avg, avg_bucket, boxplot, bucket_script, bucket_selector, bucket_sort, bucket_count_ks_test, bucket_correlation, cardinality, categorize_text, children, composite, cumulative_cardinality, cumulative_sum, date_histogram, date_range, derivative, diversified_sampler, extended_stats, extended_stats_bucket, frequent_item_sets, filter, filters, geo_bounds, geo_centroid, geo_distance, geohash_grid, geo_line, geotile_grid, geohex_grid, global, histogram, ip_range, ip_prefix, inference, line, matrix_stats, max, max_bucket, median_absolute_deviation, min, min_bucket, missing, moving_avg, moving_percentiles, moving_fn, multi_terms, nested, normalize, parent, percentile_ranks, percentiles, percentiles_bucket, range, rare_terms, rate, reverse_nested, random_sampler, sampler, scripted_metric, serial_diff, significant_terms, significant_text, stats, stats_bucket, string_stats, sum, sum_bucket, terms, time_series, top_hits, t_test, top_metrics, value_count, weighted_avg, variable_width_histogram }>)*: Specifies aggregations.
** *`query` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Specifies a DSL query.
** *`size` (Optional, number)*: Must be zero if set, as rollups work on pre-aggregated data.
** *`rest_total_hits_as_int` (Optional, boolean)*: Indicates whether hits.total should be rendered as an integer or an object in the rest search response
** *`typed_keys` (Optional, boolean)*: Specify whether aggregation and suggester names should be prefixed by their respective types in the response

[discrete]
==== start_job
Starts an existing, stopped rollup job.

{ref}/rollup-start-job.html[Endpoint documentation]
[source,ts]
----
client.rollup.startJob({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the rollup job.

[discrete]
==== stop_job
Stops an existing, started rollup job.

{ref}/rollup-stop-job.html[Endpoint documentation]
[source,ts]
----
client.rollup.stopJob({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the rollup job.
** *`timeout` (Optional, string | -1 | 0)*: If `wait_for_completion` is `true`, the API blocks for (at maximum) the specified duration while waiting for the job to stop.
If more than `timeout` time has passed, the API throws a timeout exception.
** *`wait_for_completion` (Optional, boolean)*: If set to `true`, causes the API to block until the indexer state completely stops.
If set to `false`, the API returns immediately and the indexer is stopped asynchronously in the background.

[discrete]
=== search_application
[discrete]
==== delete
Delete a search application.
Remove a search application and its associated alias. Indices attached to the search application are not removed.

{ref}/delete-search-application.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.delete({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the search application to delete

[discrete]
==== delete_behavioral_analytics
Delete a behavioral analytics collection.
The associated data stream is also deleted.

{ref}/delete-analytics-collection.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.deleteBehavioralAnalytics({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the analytics collection to be deleted

[discrete]
==== get
Get search application details.

{ref}/get-search-application.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.get({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the search application

[discrete]
==== get_behavioral_analytics
Get behavioral analytics collections.

{ref}/list-analytics-collection.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.getBehavioralAnalytics({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string[])*: A list of analytics collections to limit the returned information

[discrete]
==== list
Returns the existing search applications.

{ref}/list-search-applications.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.list({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`q` (Optional, string)*: Query in the Lucene query string syntax.
** *`from` (Optional, number)*: Starting offset.
** *`size` (Optional, number)*: Specifies a max number of results to get.

[discrete]
==== post_behavioral_analytics_event
Creates a behavioral analytics event for existing collection.

http://todo.com/tbd[Endpoint documentation]
[source,ts]
----
client.searchApplication.postBehavioralAnalyticsEvent()
----


[discrete]
==== put
Create or update a search application.

{ref}/put-search-application.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.put({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the search application to be created or updated.
** *`search_application` (Optional, { name, indices, updated_at_millis, analytics_collection_name, template })*
** *`create` (Optional, boolean)*: If `true`, this request cannot replace or update existing Search Applications.

[discrete]
==== put_behavioral_analytics
Create a behavioral analytics collection.

{ref}/put-analytics-collection.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.putBehavioralAnalytics({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the analytics collection to be created or updated.

[discrete]
==== render_query
Renders a query for given search application search parameters

{ref}/search-application-render-query.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.renderQuery()
----


[discrete]
==== search
Run a search application search.
Generate and run an Elasticsearch query that uses the specified query parameteter and the search template associated with the search application or default template.
Unspecified template parameters are assigned their default values if applicable.

{ref}/search-application-search.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.search({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the search application to be searched.
** *`params` (Optional, Record<string, User-defined value>)*: Query parameters specific to this request, which will override any defaults specified in the template.
** *`typed_keys` (Optional, boolean)*: Determines whether aggregation names are prefixed by their respective types in the response.

[discrete]
=== searchable_snapshots
[discrete]
==== cache_stats
Retrieve node-level cache statistics about searchable snapshots.

{ref}/searchable-snapshots-apis.html[Endpoint documentation]
[source,ts]
----
client.searchableSnapshots.cacheStats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`node_id` (Optional, string | string[])*: A list of node IDs or names to limit the returned information; use `_local` to return information from the node you're connecting to, leave empty to get information from all nodes
** *`master_timeout` (Optional, string | -1 | 0)*

[discrete]
==== clear_cache
Clear the cache of searchable snapshots.

{ref}/searchable-snapshots-apis.html[Endpoint documentation]
[source,ts]
----
client.searchableSnapshots.clearCache({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: A list of index names
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Whether to expand wildcard expression to concrete indices that are open, closed or both.
** *`allow_no_indices` (Optional, boolean)*: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes `_all` string or when no indices have been specified)
** *`ignore_unavailable` (Optional, boolean)*: Whether specified concrete indices should be ignored when unavailable (missing or closed)
** *`pretty` (Optional, boolean)*
** *`human` (Optional, boolean)*

[discrete]
==== mount
Mount a snapshot as a searchable index.

{ref}/searchable-snapshots-api-mount-snapshot.html[Endpoint documentation]
[source,ts]
----
client.searchableSnapshots.mount({ repository, snapshot, index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`repository` (string)*: The name of the repository containing the snapshot of the index to mount
** *`snapshot` (string)*: The name of the snapshot of the index to mount
** *`index` (string)*
** *`renamed_index` (Optional, string)*
** *`index_settings` (Optional, Record<string, User-defined value>)*
** *`ignore_index_settings` (Optional, string[])*
** *`master_timeout` (Optional, string | -1 | 0)*: Explicit operation timeout for connection to master node
** *`wait_for_completion` (Optional, boolean)*: Should this request wait until the operation has completed before returning
** *`storage` (Optional, string)*: Selects the kind of local storage used to accelerate searches. Experimental, and defaults to `full_copy`

[discrete]
==== stats
Retrieve shard-level statistics about searchable snapshots.

{ref}/searchable-snapshots-apis.html[Endpoint documentation]
[source,ts]
----
client.searchableSnapshots.stats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: A list of index names
** *`level` (Optional, Enum("cluster" | "indices" | "shards"))*: Return stats aggregated at cluster, index or shard level

[discrete]
=== security
[discrete]
==== activate_user_profile
Activate a user profile.

Create or update a user profile on behalf of another user.

{ref}/security-api-activate-user-profile.html[Endpoint documentation]
[source,ts]
----
client.security.activateUserProfile({ grant_type })
----

[discrete]
==== Arguments

* *Request (object):*
** *`grant_type` (Enum("password" | "access_token"))*
** *`access_token` (Optional, string)*
** *`password` (Optional, string)*
** *`username` (Optional, string)*

[discrete]
==== authenticate
Authenticate a user.

Authenticates a user and returns information about the authenticated user.
Include the user information in a [basic auth header](https://en.wikipedia.org/wiki/Basic_access_authentication).
A successful call returns a JSON structure that shows user information such as their username, the roles that are assigned to the user, any assigned metadata, and information about the realms that authenticated and authorized the user.
If the user cannot be authenticated, this API returns a 401 status code.

{ref}/security-api-authenticate.html[Endpoint documentation]
[source,ts]
----
client.security.authenticate()
----


[discrete]
==== bulk_delete_role
Bulk delete roles.

The role management APIs are generally the preferred way to manage roles, rather than using file-based role management.
The bulk delete roles API cannot delete roles that are defined in roles files.

{ref}/security-api-bulk-delete-role.html[Endpoint documentation]
[source,ts]
----
client.security.bulkDeleteRole({ names })
----

[discrete]
==== Arguments

* *Request (object):*
** *`names` (string[])*: An array of role names to delete
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` (the default) then refresh the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== bulk_put_role
Bulk create or update roles.

The role management APIs are generally the preferred way to manage roles, rather than using file-based role management.
The bulk create or update roles API cannot update roles that are defined in roles files.

{ref}/security-api-bulk-put-role.html[Endpoint documentation]
[source,ts]
----
client.security.bulkPutRole({ roles })
----

[discrete]
==== Arguments

* *Request (object):*
** *`roles` (Record<string, { cluster, indices, global, applications, metadata, run_as, description, transient_metadata }>)*: A dictionary of role name to RoleDescriptor objects to add or update
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` (the default) then refresh the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== bulk_update_api_keys
Updates the attributes of multiple existing API keys.

{ref}/security-api-bulk-update-api-keys.html[Endpoint documentation]
[source,ts]
----
client.security.bulkUpdateApiKeys()
----


[discrete]
==== change_password
Change passwords.

Change the passwords of users in the native realm and built-in users.

{ref}/security-api-change-password.html[Endpoint documentation]
[source,ts]
----
client.security.changePassword({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`username` (Optional, string)*: The user whose password you want to change. If you do not specify this
parameter, the password is changed for the current user.
** *`password` (Optional, string)*: The new password value. Passwords must be at least 6 characters long.
** *`password_hash` (Optional, string)*: A hash of the new password value. This must be produced using the same
hashing algorithm as has been configured for password storage. For more details,
see the explanation of the `xpack.security.authc.password_hashing.algorithm`
setting.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` (the default) then refresh the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== clear_api_key_cache
Clear the API key cache.

Evict a subset of all entries from the API key cache.
The cache is also automatically cleared on state changes of the security index.

{ref}/security-api-clear-api-key-cache.html[Endpoint documentation]
[source,ts]
----
client.security.clearApiKeyCache({ ids })
----

[discrete]
==== Arguments

* *Request (object):*
** *`ids` (string | string[])*: List of API key IDs to evict from the API key cache.
To evict all API keys, use `*`.
Does not support other wildcard patterns.

[discrete]
==== clear_cached_privileges
Clear the privileges cache.

Evict privileges from the native application privilege cache.
The cache is also automatically cleared for applications that have their privileges updated.

{ref}/security-api-clear-privilege-cache.html[Endpoint documentation]
[source,ts]
----
client.security.clearCachedPrivileges({ application })
----

[discrete]
==== Arguments

* *Request (object):*
** *`application` (string)*: A list of application names

[discrete]
==== clear_cached_realms
Clear the user cache.

Evict users from the user cache. You can completely clear the cache or evict specific users.

{ref}/security-api-clear-cache.html[Endpoint documentation]
[source,ts]
----
client.security.clearCachedRealms({ realms })
----

[discrete]
==== Arguments

* *Request (object):*
** *`realms` (string | string[])*: List of realms to clear
** *`usernames` (Optional, string[])*: List of usernames to clear from the cache

[discrete]
==== clear_cached_roles
Clear the roles cache.

Evict roles from the native role cache.

{ref}/security-api-clear-role-cache.html[Endpoint documentation]
[source,ts]
----
client.security.clearCachedRoles({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: Role name

[discrete]
==== clear_cached_service_tokens
Clear service account token caches.

Evict a subset of all entries from the service account token caches.

{ref}/security-api-clear-service-token-caches.html[Endpoint documentation]
[source,ts]
----
client.security.clearCachedServiceTokens({ namespace, service, name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`namespace` (string)*: An identifier for the namespace
** *`service` (string)*: An identifier for the service name
** *`name` (string | string[])*: A list of service token names

[discrete]
==== create_api_key
Create an API key.

Create an API key for access without requiring basic authentication.
A successful request returns a JSON structure that contains the API key, its unique id, and its name.
If applicable, it also returns expiration information for the API key in milliseconds.
NOTE: By default, API keys never expire. You can specify expiration information when you create the API keys.

{ref}/security-api-create-api-key.html[Endpoint documentation]
[source,ts]
----
client.security.createApiKey({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`expiration` (Optional, string | -1 | 0)*: Expiration time for the API key. By default, API keys never expire.
** *`name` (Optional, string)*: Specifies the name for this API key.
** *`role_descriptors` (Optional, Record<string, { cluster, indices, global, applications, metadata, run_as, description, transient_metadata }>)*: An array of role descriptors for this API key. This parameter is optional. When it is not specified or is an empty array, then the API key will have a point in time snapshot of permissions of the authenticated user. If you supply role descriptors then the resultant permissions would be an intersection of API keys permissions and authenticated user’s permissions thereby limiting the access scope for API keys. The structure of role descriptor is the same as the request for create role API. For more details, see create or update roles API.
** *`metadata` (Optional, Record<string, User-defined value>)*: Arbitrary metadata that you want to associate with the API key. It supports nested data structure. Within the metadata object, keys beginning with `_` are reserved for system usage.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` (the default) then refresh the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== create_cross_cluster_api_key
Create a cross-cluster API key.

Create an API key of the `cross_cluster` type for the API key based remote cluster access.
A `cross_cluster` API key cannot be used to authenticate through the REST interface.

IMPORTANT: To authenticate this request you must use a credential that is not an API key. Even if you use an API key that has the required privilege, the API returns an error.

Cross-cluster API keys are created by the Elasticsearch API key service, which is automatically enabled.

NOTE: Unlike REST API keys, a cross-cluster API key does not capture permissions of the authenticated user. The API key’s effective permission is exactly as specified with the `access` property.

A successful request returns a JSON structure that contains the API key, its unique ID, and its name. If applicable, it also returns expiration information for the API key in milliseconds.

By default, API keys never expire. You can specify expiration information when you create the API keys.

Cross-cluster API keys can only be updated with the update cross-cluster API key API.
Attempting to update them with the update REST API key API or the bulk update REST API keys API will result in an error.

{ref}/security-api-create-cross-cluster-api-key.html[Endpoint documentation]
[source,ts]
----
client.security.createCrossClusterApiKey({ access, name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`access` ({ replication, search })*: The access to be granted to this API key.
The access is composed of permissions for cross-cluster search and cross-cluster replication.
At least one of them must be specified.

NOTE: No explicit privileges should be specified for either search or replication access.
The creation process automatically converts the access specification to a role descriptor which has relevant privileges assigned accordingly.
** *`name` (string)*: Specifies the name for this API key.
** *`expiration` (Optional, string | -1 | 0)*: Expiration time for the API key.
By default, API keys never expire.
** *`metadata` (Optional, Record<string, User-defined value>)*: Arbitrary metadata that you want to associate with the API key.
It supports nested data structure.
Within the metadata object, keys beginning with `_` are reserved for system usage.

[discrete]
==== create_service_token
Create a service account token.

Create a service accounts token for access without requiring basic authentication.

{ref}/security-api-create-service-token.html[Endpoint documentation]
[source,ts]
----
client.security.createServiceToken({ namespace, service })
----

[discrete]
==== Arguments

* *Request (object):*
** *`namespace` (string)*: An identifier for the namespace
** *`service` (string)*: An identifier for the service name
** *`name` (Optional, string)*: An identifier for the token name
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` then refresh the affected shards to make this operation visible to search, if `wait_for` (the default) then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== delete_privileges
Delete application privileges.

{ref}/security-api-delete-privilege.html[Endpoint documentation]
[source,ts]
----
client.security.deletePrivileges({ application, name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`application` (string)*: Application name
** *`name` (string | string[])*: Privilege name
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` (the default) then refresh the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== delete_role
Delete roles.

Delete roles in the native realm.

{ref}/security-api-delete-role.html[Endpoint documentation]
[source,ts]
----
client.security.deleteRole({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Role name
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` (the default) then refresh the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== delete_role_mapping
Delete role mappings.

{ref}/security-api-delete-role-mapping.html[Endpoint documentation]
[source,ts]
----
client.security.deleteRoleMapping({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Role-mapping name
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` (the default) then refresh the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== delete_service_token
Delete service account tokens.

Delete service account tokens for a service in a specified namespace.

{ref}/security-api-delete-service-token.html[Endpoint documentation]
[source,ts]
----
client.security.deleteServiceToken({ namespace, service, name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`namespace` (string)*: An identifier for the namespace
** *`service` (string)*: An identifier for the service name
** *`name` (string)*: An identifier for the token name
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` then refresh the affected shards to make this operation visible to search, if `wait_for` (the default) then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== delete_user
Delete users.

Delete users from the native realm.

{ref}/security-api-delete-user.html[Endpoint documentation]
[source,ts]
----
client.security.deleteUser({ username })
----

[discrete]
==== Arguments

* *Request (object):*
** *`username` (string)*: username
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` (the default) then refresh the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== disable_user
Disable users.

Disable users in the native realm.

{ref}/security-api-disable-user.html[Endpoint documentation]
[source,ts]
----
client.security.disableUser({ username })
----

[discrete]
==== Arguments

* *Request (object):*
** *`username` (string)*: The username of the user to disable
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` (the default) then refresh the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== disable_user_profile
Disable a user profile.

Disable user profiles so that they are not visible in user profile searches.

{ref}/security-api-disable-user-profile.html[Endpoint documentation]
[source,ts]
----
client.security.disableUserProfile({ uid })
----

[discrete]
==== Arguments

* *Request (object):*
** *`uid` (string)*: Unique identifier for the user profile.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If 'true', Elasticsearch refreshes the affected shards to make this operation
visible to search, if 'wait_for' then wait for a refresh to make this operation
visible to search, if 'false' do nothing with refreshes.

[discrete]
==== enable_user
Enable users.

Enable users in the native realm.

{ref}/security-api-enable-user.html[Endpoint documentation]
[source,ts]
----
client.security.enableUser({ username })
----

[discrete]
==== Arguments

* *Request (object):*
** *`username` (string)*: The username of the user to enable
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` (the default) then refresh the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== enable_user_profile
Enable a user profile.

Enable user profiles to make them visible in user profile searches.

{ref}/security-api-enable-user-profile.html[Endpoint documentation]
[source,ts]
----
client.security.enableUserProfile({ uid })
----

[discrete]
==== Arguments

* *Request (object):*
** *`uid` (string)*: Unique identifier for the user profile.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If 'true', Elasticsearch refreshes the affected shards to make this operation
visible to search, if 'wait_for' then wait for a refresh to make this operation
visible to search, if 'false' do nothing with refreshes.

[discrete]
==== enroll_kibana
Enroll Kibana.

Enable a Kibana instance to configure itself for communication with a secured Elasticsearch cluster.

{ref}/security-api-kibana-enrollment.html[Endpoint documentation]
[source,ts]
----
client.security.enrollKibana()
----


[discrete]
==== enroll_node
Enroll a node.

Enroll a new node to allow it to join an existing cluster with security features enabled.

{ref}/security-api-node-enrollment.html[Endpoint documentation]
[source,ts]
----
client.security.enrollNode()
----


[discrete]
==== get_api_key
Get API key information.

Retrieves information for one or more API keys.
NOTE: If you have only the `manage_own_api_key` privilege, this API returns only the API keys that you own.
If you have `read_security`, `manage_api_key` or greater privileges (including `manage_security`), this API returns all API keys regardless of ownership.

{ref}/security-api-get-api-key.html[Endpoint documentation]
[source,ts]
----
client.security.getApiKey({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: An API key id.
This parameter cannot be used with any of `name`, `realm_name` or `username`.
** *`name` (Optional, string)*: An API key name.
This parameter cannot be used with any of `id`, `realm_name` or `username`.
It supports prefix search with wildcard.
** *`owner` (Optional, boolean)*: A boolean flag that can be used to query API keys owned by the currently authenticated user.
The `realm_name` or `username` parameters cannot be specified when this parameter is set to `true` as they are assumed to be the currently authenticated ones.
** *`realm_name` (Optional, string)*: The name of an authentication realm.
This parameter cannot be used with either `id` or `name` or when `owner` flag is set to `true`.
** *`username` (Optional, string)*: The username of a user.
This parameter cannot be used with either `id` or `name` or when `owner` flag is set to `true`.
** *`with_limited_by` (Optional, boolean)*: Return the snapshot of the owner user's role descriptors
associated with the API key. An API key's actual
permission is the intersection of its assigned role
descriptors and the owner user's role descriptors.
** *`active_only` (Optional, boolean)*: A boolean flag that can be used to query API keys that are currently active. An API key is considered active if it is neither invalidated, nor expired at query time. You can specify this together with other parameters such as `owner` or `name`. If `active_only` is false, the response will include both active and inactive (expired or invalidated) keys.
** *`with_profile_uid` (Optional, boolean)*: Determines whether to also retrieve the profile uid, for the API key owner principal, if it exists.

[discrete]
==== get_builtin_privileges
Get builtin privileges.

Get the list of cluster privileges and index privileges that are available in this version of Elasticsearch.

{ref}/security-api-get-builtin-privileges.html[Endpoint documentation]
[source,ts]
----
client.security.getBuiltinPrivileges()
----


[discrete]
==== get_privileges
Get application privileges.

{ref}/security-api-get-privileges.html[Endpoint documentation]
[source,ts]
----
client.security.getPrivileges({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`application` (Optional, string)*: Application name
** *`name` (Optional, string | string[])*: Privilege name

[discrete]
==== get_role
Get roles.

Get roles in the native realm.
The role management APIs are generally the preferred way to manage roles, rather than using file-based role management.
The get roles API cannot retrieve roles that are defined in roles files.

{ref}/security-api-get-role.html[Endpoint documentation]
[source,ts]
----
client.security.getRole({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string | string[])*: The name of the role. You can specify multiple roles as a list. If you do not specify this parameter, the API returns information about all roles.

[discrete]
==== get_role_mapping
Get role mappings.

Role mappings define which roles are assigned to each user.
The role mapping APIs are generally the preferred way to manage role mappings rather than using role mapping files.
The get role mappings API cannot retrieve role mappings that are defined in role mapping files.

{ref}/security-api-get-role-mapping.html[Endpoint documentation]
[source,ts]
----
client.security.getRoleMapping({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string | string[])*: The distinct name that identifies the role mapping. The name is used solely as an identifier to facilitate interaction via the API; it does not affect the behavior of the mapping in any way. You can specify multiple mapping names as a list. If you do not specify this parameter, the API returns information about all role mappings.

[discrete]
==== get_service_accounts
Get service accounts.

Get a list of service accounts that match the provided path parameters.

{ref}/security-api-get-service-accounts.html[Endpoint documentation]
[source,ts]
----
client.security.getServiceAccounts({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`namespace` (Optional, string)*: Name of the namespace. Omit this parameter to retrieve information about all service accounts. If you omit this parameter, you must also omit the `service` parameter.
** *`service` (Optional, string)*: Name of the service name. Omit this parameter to retrieve information about all service accounts that belong to the specified `namespace`.

[discrete]
==== get_service_credentials
Get service account credentials.

{ref}/security-api-get-service-credentials.html[Endpoint documentation]
[source,ts]
----
client.security.getServiceCredentials({ namespace, service })
----

[discrete]
==== Arguments

* *Request (object):*
** *`namespace` (string)*: Name of the namespace.
** *`service` (string)*: Name of the service name.

[discrete]
==== get_settings
Retrieve settings for the security system indices

{ref}/security-api-get-settings.html[Endpoint documentation]
[source,ts]
----
client.security.getSettings()
----


[discrete]
==== get_token
Get a token.

Create a bearer token for access without requiring basic authentication.

{ref}/security-api-get-token.html[Endpoint documentation]
[source,ts]
----
client.security.getToken({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`grant_type` (Optional, Enum("password" | "client_credentials" | "_kerberos" | "refresh_token"))*
** *`scope` (Optional, string)*
** *`password` (Optional, string)*
** *`kerberos_ticket` (Optional, string)*
** *`refresh_token` (Optional, string)*
** *`username` (Optional, string)*

[discrete]
==== get_user
Get users.

Get information about users in the native realm and built-in users.

{ref}/security-api-get-user.html[Endpoint documentation]
[source,ts]
----
client.security.getUser({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`username` (Optional, string | string[])*: An identifier for the user. You can specify multiple usernames as a list. If you omit this parameter, the API retrieves information about all users.
** *`with_profile_uid` (Optional, boolean)*: If true will return the User Profile ID for a user, if any.

[discrete]
==== get_user_privileges
Get user privileges.

{ref}/security-api-get-user-privileges.html[Endpoint documentation]
[source,ts]
----
client.security.getUserPrivileges({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`application` (Optional, string)*: The name of the application. Application privileges are always associated with exactly one application. If you do not specify this parameter, the API returns information about all privileges for all applications.
** *`priviledge` (Optional, string)*: The name of the privilege. If you do not specify this parameter, the API returns information about all privileges for the requested application.
** *`username` (Optional, string | null)*

[discrete]
==== get_user_profile
Get a user profile.

Get a user's profile using the unique profile ID.

{ref}/security-api-get-user-profile.html[Endpoint documentation]
[source,ts]
----
client.security.getUserProfile({ uid })
----

[discrete]
==== Arguments

* *Request (object):*
** *`uid` (string | string[])*: A unique identifier for the user profile.
** *`data` (Optional, string | string[])*: List of filters for the `data` field of the profile document.
To return all content use `data=*`. To return a subset of content
use `data=<key>` to retrieve content nested under the specified `<key>`.
By default returns no `data` content.

[discrete]
==== grant_api_key
Grant an API key.

Create an API key on behalf of another user.
This API is similar to the create API keys API, however it creates the API key for a user that is different than the user that runs the API.
The caller must have authentication credentials (either an access token, or a username and password) for the user on whose behalf the API key will be created.
It is not possible to use this API to create an API key without that user’s credentials.
The user, for whom the authentication credentials is provided, can optionally "run as" (impersonate) another user.
In this case, the API key will be created on behalf of the impersonated user.

This API is intended be used by applications that need to create and manage API keys for end users, but cannot guarantee that those users have permission to create API keys on their own behalf.

A successful grant API key API call returns a JSON structure that contains the API key, its unique id, and its name.
If applicable, it also returns expiration information for the API key in milliseconds.

By default, API keys never expire. You can specify expiration information when you create the API keys.

{ref}/security-api-grant-api-key.html[Endpoint documentation]
[source,ts]
----
client.security.grantApiKey({ api_key, grant_type })
----

[discrete]
==== Arguments

* *Request (object):*
** *`api_key` ({ name, expiration, role_descriptors, metadata })*: Defines the API key.
** *`grant_type` (Enum("access_token" | "password"))*: The type of grant. Supported grant types are: `access_token`, `password`.
** *`access_token` (Optional, string)*: The user’s access token.
If you specify the `access_token` grant type, this parameter is required.
It is not valid with other grant types.
** *`username` (Optional, string)*: The user name that identifies the user.
If you specify the `password` grant type, this parameter is required.
It is not valid with other grant types.
** *`password` (Optional, string)*: The user’s password. If you specify the `password` grant type, this parameter is required.
It is not valid with other grant types.
** *`run_as` (Optional, string)*: The name of the user to be impersonated.

[discrete]
==== has_privileges
Check user privileges.

Determine whether the specified user has a specified list of privileges.

{ref}/security-api-has-privileges.html[Endpoint documentation]
[source,ts]
----
client.security.hasPrivileges({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`user` (Optional, string)*: Username
** *`application` (Optional, { application, privileges, resources }[])*
** *`cluster` (Optional, Enum("all" | "cancel_task" | "create_snapshot" | "cross_cluster_replication" | "cross_cluster_search" | "delegate_pki" | "grant_api_key" | "manage" | "manage_api_key" | "manage_autoscaling" | "manage_behavioral_analytics" | "manage_ccr" | "manage_data_frame_transforms" | "manage_data_stream_global_retention" | "manage_enrich" | "manage_ilm" | "manage_index_templates" | "manage_inference" | "manage_ingest_pipelines" | "manage_logstash_pipelines" | "manage_ml" | "manage_oidc" | "manage_own_api_key" | "manage_pipeline" | "manage_rollup" | "manage_saml" | "manage_search_application" | "manage_search_query_rules" | "manage_search_synonyms" | "manage_security" | "manage_service_account" | "manage_slm" | "manage_token" | "manage_transform" | "manage_user_profile" | "manage_watcher" | "monitor" | "monitor_data_frame_transforms" | "monitor_data_stream_global_retention" | "monitor_enrich" | "monitor_inference" | "monitor_ml" | "monitor_rollup" | "monitor_snapshot" | "monitor_text_structure" | "monitor_transform" | "monitor_watcher" | "none" | "post_behavioral_analytics_event" | "read_ccr" | "read_fleet_secrets" | "read_ilm" | "read_pipeline" | "read_security" | "read_slm" | "transport_client" | "write_connector_secrets" | "write_fleet_secrets")[])*: A list of the cluster privileges that you want to check.
** *`index` (Optional, { names, privileges, allow_restricted_indices }[])*

[discrete]
==== has_privileges_user_profile
Check user profile privileges.

Determine whether the users associated with the specified user profile IDs have all the requested privileges.

{ref}/security-api-has-privileges-user-profile.html[Endpoint documentation]
[source,ts]
----
client.security.hasPrivilegesUserProfile({ uids, privileges })
----

[discrete]
==== Arguments

* *Request (object):*
** *`uids` (string[])*: A list of profile IDs. The privileges are checked for associated users of the profiles.
** *`privileges` ({ application, cluster, index })*

[discrete]
==== invalidate_api_key
Invalidate API keys.

This API invalidates API keys created by the create API key or grant API key APIs.
Invalidated API keys fail authentication, but they can still be viewed using the get API key information and query API key information APIs, for at least the configured retention period, until they are automatically deleted.
The `manage_api_key` privilege allows deleting any API keys.
The `manage_own_api_key` only allows deleting API keys that are owned by the user.
In addition, with the `manage_own_api_key` privilege, an invalidation request must be issued in one of the three formats:
- Set the parameter `owner=true`.
- Or, set both `username` and `realm_name` to match the user’s identity.
- Or, if the request is issued by an API key, that is to say an API key invalidates itself, specify its ID in the `ids` field.

{ref}/security-api-invalidate-api-key.html[Endpoint documentation]
[source,ts]
----
client.security.invalidateApiKey({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*
** *`ids` (Optional, string[])*: A list of API key ids.
This parameter cannot be used with any of `name`, `realm_name`, or `username`.
** *`name` (Optional, string)*: An API key name.
This parameter cannot be used with any of `ids`, `realm_name` or `username`.
** *`owner` (Optional, boolean)*: Can be used to query API keys owned by the currently authenticated user.
The `realm_name` or `username` parameters cannot be specified when this parameter is set to `true` as they are assumed to be the currently authenticated ones.
** *`realm_name` (Optional, string)*: The name of an authentication realm.
This parameter cannot be used with either `ids` or `name`, or when `owner` flag is set to `true`.
** *`username` (Optional, string)*: The username of a user.
This parameter cannot be used with either `ids` or `name`, or when `owner` flag is set to `true`.

[discrete]
==== invalidate_token
Invalidate a token.

The access tokens returned by the get token API have a finite period of time for which they are valid.
After that time period, they can no longer be used.
The time period is defined by the `xpack.security.authc.token.timeout` setting.

The refresh tokens returned by the get token API are only valid for 24 hours. They can also be used exactly once.
If you want to invalidate one or more access or refresh tokens immediately, use this invalidate token API.

{ref}/security-api-invalidate-token.html[Endpoint documentation]
[source,ts]
----
client.security.invalidateToken({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`token` (Optional, string)*
** *`refresh_token` (Optional, string)*
** *`realm_name` (Optional, string)*
** *`username` (Optional, string)*

[discrete]
==== oidc_authenticate
Exchanges an OpenID Connection authentication response message for an Elasticsearch access token and refresh token pair

{ref}/security-api-oidc-authenticate.html[Endpoint documentation]
[source,ts]
----
client.security.oidcAuthenticate()
----


[discrete]
==== oidc_logout
Invalidates a refresh token and access token that was generated from the OpenID Connect Authenticate API

{ref}/security-api-oidc-logout.html[Endpoint documentation]
[source,ts]
----
client.security.oidcLogout()
----


[discrete]
==== oidc_prepare_authentication
Creates an OAuth 2.0 authentication request as a URL string

{ref}/security-api-oidc-prepare-authentication.html[Endpoint documentation]
[source,ts]
----
client.security.oidcPrepareAuthentication()
----


[discrete]
==== put_privileges
Create or update application privileges.

{ref}/security-api-put-privileges.html[Endpoint documentation]
[source,ts]
----
client.security.putPrivileges({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`privileges` (Optional, Record<string, Record<string, { allocate, delete, downsample, freeze, forcemerge, migrate, readonly, rollover, set_priority, searchable_snapshot, shrink, unfollow, wait_for_snapshot }>>)*
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` (the default) then refresh the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== put_role
Create or update roles.

The role management APIs are generally the preferred way to manage roles in the native realm, rather than using file-based role management.
The create or update roles API cannot update roles that are defined in roles files.
File-based role management is not available in Elastic Serverless.

{ref}/security-api-put-role.html[Endpoint documentation]
[source,ts]
----
client.security.putRole({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the role.
** *`applications` (Optional, { application, privileges, resources }[])*: A list of application privilege entries.
** *`cluster` (Optional, Enum("all" | "cancel_task" | "create_snapshot" | "cross_cluster_replication" | "cross_cluster_search" | "delegate_pki" | "grant_api_key" | "manage" | "manage_api_key" | "manage_autoscaling" | "manage_behavioral_analytics" | "manage_ccr" | "manage_data_frame_transforms" | "manage_data_stream_global_retention" | "manage_enrich" | "manage_ilm" | "manage_index_templates" | "manage_inference" | "manage_ingest_pipelines" | "manage_logstash_pipelines" | "manage_ml" | "manage_oidc" | "manage_own_api_key" | "manage_pipeline" | "manage_rollup" | "manage_saml" | "manage_search_application" | "manage_search_query_rules" | "manage_search_synonyms" | "manage_security" | "manage_service_account" | "manage_slm" | "manage_token" | "manage_transform" | "manage_user_profile" | "manage_watcher" | "monitor" | "monitor_data_frame_transforms" | "monitor_data_stream_global_retention" | "monitor_enrich" | "monitor_inference" | "monitor_ml" | "monitor_rollup" | "monitor_snapshot" | "monitor_text_structure" | "monitor_transform" | "monitor_watcher" | "none" | "post_behavioral_analytics_event" | "read_ccr" | "read_fleet_secrets" | "read_ilm" | "read_pipeline" | "read_security" | "read_slm" | "transport_client" | "write_connector_secrets" | "write_fleet_secrets")[])*: A list of cluster privileges. These privileges define the cluster-level actions for users with this role.
** *`global` (Optional, Record<string, User-defined value>)*: An object defining global privileges. A global privilege is a form of cluster privilege that is request-aware. Support for global privileges is currently limited to the management of application privileges.
** *`indices` (Optional, { field_security, names, privileges, query, allow_restricted_indices }[])*: A list of indices permissions entries.
** *`remote_indices` (Optional, { clusters, field_security, names, privileges, query, allow_restricted_indices }[])*: A list of remote indices permissions entries.
** *`metadata` (Optional, Record<string, User-defined value>)*: Optional metadata. Within the metadata object, keys that begin with an underscore (`_`) are reserved for system use.
** *`run_as` (Optional, string[])*: A list of users that the owners of this role can impersonate. *Note*: in Serverless, the run-as feature is disabled. For API compatibility, you can still specify an empty `run_as` field, but a non-empty list will be rejected.
** *`description` (Optional, string)*: Optional description of the role descriptor
** *`transient_metadata` (Optional, Record<string, User-defined value>)*: Indicates roles that might be incompatible with the current cluster license, specifically roles with document and field level security. When the cluster license doesn’t allow certain features for a given role, this parameter is updated dynamically to list the incompatible features. If `enabled` is `false`, the role is ignored, but is still listed in the response from the authenticate API.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` (the default) then refresh the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== put_role_mapping
Create or update role mappings.

Role mappings define which roles are assigned to each user.
Each mapping has rules that identify users and a list of roles that are granted to those users.
The role mapping APIs are generally the preferred way to manage role mappings rather than using role mapping files. The create or update role mappings API cannot update role mappings that are defined in role mapping files.

This API does not create roles. Rather, it maps users to existing roles.
Roles can be created by using the create or update roles API or roles files.

{ref}/security-api-put-role-mapping.html[Endpoint documentation]
[source,ts]
----
client.security.putRoleMapping({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Role-mapping name
** *`enabled` (Optional, boolean)*
** *`metadata` (Optional, Record<string, User-defined value>)*
** *`roles` (Optional, string[])*
** *`role_templates` (Optional, { format, template }[])*
** *`rules` (Optional, { any, all, field, except })*
** *`run_as` (Optional, string[])*
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` (the default) then refresh the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== put_user
Create or update users.

A password is required for adding a new user but is optional when updating an existing user.
To change a user’s password without updating any other fields, use the change password API.

{ref}/security-api-put-user.html[Endpoint documentation]
[source,ts]
----
client.security.putUser({ username })
----

[discrete]
==== Arguments

* *Request (object):*
** *`username` (string)*: The username of the User
** *`email` (Optional, string | null)*
** *`full_name` (Optional, string | null)*
** *`metadata` (Optional, Record<string, User-defined value>)*
** *`password` (Optional, string)*
** *`password_hash` (Optional, string)*
** *`roles` (Optional, string[])*
** *`enabled` (Optional, boolean)*
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` (the default) then refresh the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== query_api_keys
Find API keys with a query.

Get a paginated list of API keys and their information. You can optionally filter the results with a query.

{ref}/security-api-query-api-key.html[Endpoint documentation]
[source,ts]
----
client.security.queryApiKeys({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`aggregations` (Optional, Record<string, { aggregations, meta, cardinality, composite, date_range, filter, filters, missing, range, terms, value_count }>)*: Any aggregations to run over the corpus of returned API keys.
Aggregations and queries work together. Aggregations are computed only on the API keys that match the query.
This supports only a subset of aggregation types, namely: `terms`, `range`, `date_range`, `missing`,
`cardinality`, `value_count`, `composite`, `filter`, and `filters`.
Additionally, aggregations only run over the same subset of fields that query works with.
** *`query` (Optional, { bool, exists, ids, match, match_all, prefix, range, simple_query_string, term, terms, wildcard })*: A query to filter which API keys to return.
If the query parameter is missing, it is equivalent to a `match_all` query.
The query supports a subset of query types, including `match_all`, `bool`, `term`, `terms`, `match`,
`ids`, `prefix`, `wildcard`, `exists`, `range`, and `simple_query_string`.
You can query the following public information associated with an API key: `id`, `type`, `name`,
`creation`, `expiration`, `invalidated`, `invalidation`, `username`, `realm`, and `metadata`.
** *`from` (Optional, number)*: Starting document offset.
By default, you cannot page through more than 10,000 hits using the from and size parameters.
To page through more hits, use the `search_after` parameter.
** *`sort` (Optional, string | { _score, _doc, _geo_distance, _script } | string | { _score, _doc, _geo_distance, _script }[])*: Other than `id`, all public fields of an API key are eligible for sorting.
In addition, sort can also be applied to the `_doc` field to sort by index order.
** *`size` (Optional, number)*: The number of hits to return.
By default, you cannot page through more than 10,000 hits using the `from` and `size` parameters.
To page through more hits, use the `search_after` parameter.
** *`search_after` (Optional, number | number | string | boolean | null | User-defined value[])*: Search after definition
** *`with_limited_by` (Optional, boolean)*: Return the snapshot of the owner user's role descriptors associated with the API key.
An API key's actual permission is the intersection of its assigned role descriptors and the owner user's role descriptors.
** *`with_profile_uid` (Optional, boolean)*: Determines whether to also retrieve the profile uid, for the API key owner principal, if it exists.
** *`typed_keys` (Optional, boolean)*: Determines whether aggregation names are prefixed by their respective types in the response.

[discrete]
==== query_role
Find roles with a query.

Get roles in a paginated manner. You can optionally filter the results with a query.

{ref}/security-api-query-role.html[Endpoint documentation]
[source,ts]
----
client.security.queryRole({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`query` (Optional, { bool, exists, ids, match, match_all, prefix, range, simple_query_string, term, terms, wildcard })*: A query to filter which roles to return.
If the query parameter is missing, it is equivalent to a `match_all` query.
The query supports a subset of query types, including `match_all`, `bool`, `term`, `terms`, `match`,
`ids`, `prefix`, `wildcard`, `exists`, `range`, and `simple_query_string`.
You can query the following information associated with roles: `name`, `description`, `metadata`,
`applications.application`, `applications.privileges`, `applications.resources`.
** *`from` (Optional, number)*: Starting document offset.
By default, you cannot page through more than 10,000 hits using the from and size parameters.
To page through more hits, use the `search_after` parameter.
** *`sort` (Optional, string | { _score, _doc, _geo_distance, _script } | string | { _score, _doc, _geo_distance, _script }[])*: All public fields of a role are eligible for sorting.
In addition, sort can also be applied to the `_doc` field to sort by index order.
** *`size` (Optional, number)*: The number of hits to return.
By default, you cannot page through more than 10,000 hits using the `from` and `size` parameters.
To page through more hits, use the `search_after` parameter.
** *`search_after` (Optional, number | number | string | boolean | null | User-defined value[])*: Search after definition

[discrete]
==== query_user
Find users with a query.

Get information for users in a paginated manner.
You can optionally filter the results with a query.

{ref}/security-api-query-user.html[Endpoint documentation]
[source,ts]
----
client.security.queryUser({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`query` (Optional, { ids, bool, exists, match, match_all, prefix, range, simple_query_string, term, terms, wildcard })*: A query to filter which users to return.
If the query parameter is missing, it is equivalent to a `match_all` query.
The query supports a subset of query types, including `match_all`, `bool`, `term`, `terms`, `match`,
`ids`, `prefix`, `wildcard`, `exists`, `range`, and `simple_query_string`.
You can query the following information associated with user: `username`, `roles`, `enabled`
** *`from` (Optional, number)*: Starting document offset.
By default, you cannot page through more than 10,000 hits using the from and size parameters.
To page through more hits, use the `search_after` parameter.
** *`sort` (Optional, string | { _score, _doc, _geo_distance, _script } | string | { _score, _doc, _geo_distance, _script }[])*: Fields eligible for sorting are: username, roles, enabled
In addition, sort can also be applied to the `_doc` field to sort by index order.
** *`size` (Optional, number)*: The number of hits to return.
By default, you cannot page through more than 10,000 hits using the `from` and `size` parameters.
To page through more hits, use the `search_after` parameter.
** *`search_after` (Optional, number | number | string | boolean | null | User-defined value[])*: Search after definition
** *`with_profile_uid` (Optional, boolean)*: If true will return the User Profile ID for the users in the query result, if any.

[discrete]
==== saml_authenticate
Authenticate SAML.

Submits a SAML response message to Elasticsearch for consumption.

{ref}/security-api-saml-authenticate.html[Endpoint documentation]
[source,ts]
----
client.security.samlAuthenticate({ content, ids })
----

[discrete]
==== Arguments

* *Request (object):*
** *`content` (string)*: The SAML response as it was sent by the user’s browser, usually a Base64 encoded XML document.
** *`ids` (string | string[])*: A json array with all the valid SAML Request Ids that the caller of the API has for the current user.
** *`realm` (Optional, string)*: The name of the realm that should authenticate the SAML response. Useful in cases where many SAML realms are defined.

[discrete]
==== saml_complete_logout
Logout of SAML completely.

Verifies the logout response sent from the SAML IdP.

{ref}/security-api-saml-complete-logout.html[Endpoint documentation]
[source,ts]
----
client.security.samlCompleteLogout({ realm, ids })
----

[discrete]
==== Arguments

* *Request (object):*
** *`realm` (string)*: The name of the SAML realm in Elasticsearch for which the configuration is used to verify the logout response.
** *`ids` (string | string[])*: A json array with all the valid SAML Request Ids that the caller of the API has for the current user.
** *`query_string` (Optional, string)*: If the SAML IdP sends the logout response with the HTTP-Redirect binding, this field must be set to the query string of the redirect URI.
** *`content` (Optional, string)*: If the SAML IdP sends the logout response with the HTTP-Post binding, this field must be set to the value of the SAMLResponse form parameter from the logout response.

[discrete]
==== saml_invalidate
Invalidate SAML.

Submits a SAML LogoutRequest message to Elasticsearch for consumption.

{ref}/security-api-saml-invalidate.html[Endpoint documentation]
[source,ts]
----
client.security.samlInvalidate({ query_string })
----

[discrete]
==== Arguments

* *Request (object):*
** *`query_string` (string)*: The query part of the URL that the user was redirected to by the SAML IdP to initiate the Single Logout.
This query should include a single parameter named SAMLRequest that contains a SAML logout request that is deflated and Base64 encoded.
If the SAML IdP has signed the logout request, the URL should include two extra parameters named SigAlg and Signature that contain the algorithm used for the signature and the signature value itself.
In order for Elasticsearch to be able to verify the IdP’s signature, the value of the query_string field must be an exact match to the string provided by the browser.
The client application must not attempt to parse or process the string in any way.
** *`acs` (Optional, string)*: The Assertion Consumer Service URL that matches the one of the SAML realm in Elasticsearch that should be used. You must specify either this parameter or the realm parameter.
** *`realm` (Optional, string)*: The name of the SAML realm in Elasticsearch the configuration. You must specify either this parameter or the acs parameter.

[discrete]
==== saml_logout
Logout of SAML.

Submits a request to invalidate an access token and refresh token.

{ref}/security-api-saml-logout.html[Endpoint documentation]
[source,ts]
----
client.security.samlLogout({ token })
----

[discrete]
==== Arguments

* *Request (object):*
** *`token` (string)*: The access token that was returned as a response to calling the SAML authenticate API.
Alternatively, the most recent token that was received after refreshing the original one by using a refresh_token.
** *`refresh_token` (Optional, string)*: The refresh token that was returned as a response to calling the SAML authenticate API.
Alternatively, the most recent refresh token that was received after refreshing the original access token.

[discrete]
==== saml_prepare_authentication
Prepare SAML authentication.

Creates a SAML authentication request (`<AuthnRequest>`) as a URL string, based on the configuration of the respective SAML realm in Elasticsearch.

{ref}/security-api-saml-prepare-authentication.html[Endpoint documentation]
[source,ts]
----
client.security.samlPrepareAuthentication({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`acs` (Optional, string)*: The Assertion Consumer Service URL that matches the one of the SAML realms in Elasticsearch.
The realm is used to generate the authentication request. You must specify either this parameter or the realm parameter.
** *`realm` (Optional, string)*: The name of the SAML realm in Elasticsearch for which the configuration is used to generate the authentication request.
You must specify either this parameter or the acs parameter.
** *`relay_state` (Optional, string)*: A string that will be included in the redirect URL that this API returns as the RelayState query parameter.
If the Authentication Request is signed, this value is used as part of the signature computation.

[discrete]
==== saml_service_provider_metadata
Create SAML service provider metadata.

Generate SAML metadata for a SAML 2.0 Service Provider.

{ref}/security-api-saml-sp-metadata.html[Endpoint documentation]
[source,ts]
----
client.security.samlServiceProviderMetadata({ realm_name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`realm_name` (string)*: The name of the SAML realm in Elasticsearch.

[discrete]
==== suggest_user_profiles
Suggest a user profile.

Get suggestions for user profiles that match specified search criteria.

{ref}/security-api-suggest-user-profile.html[Endpoint documentation]
[source,ts]
----
client.security.suggestUserProfiles({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string)*: Query string used to match name-related fields in user profile documents.
Name-related fields are the user's `username`, `full_name`, and `email`.
** *`size` (Optional, number)*: Number of profiles to return.
** *`data` (Optional, string | string[])*: List of filters for the `data` field of the profile document.
To return all content use `data=*`. To return a subset of content
use `data=<key>` to retrieve content nested under the specified `<key>`.
By default returns no `data` content.
** *`hint` (Optional, { uids, labels })*: Extra search criteria to improve relevance of the suggestion result.
Profiles matching the spcified hint are ranked higher in the response.
Profiles not matching the hint don't exclude the profile from the response
as long as the profile matches the `name` field query.

[discrete]
==== update_api_key
Update an API key.

Updates attributes of an existing API key.
Users can only update API keys that they created or that were granted to them.
Use this API to update API keys created by the create API Key or grant API Key APIs.
If you need to apply the same update to many API keys, you can use bulk update API Keys to reduce overhead.
It’s not possible to update expired API keys, or API keys that have been invalidated by invalidate API Key.
This API supports updates to an API key’s access scope and metadata.
The access scope of an API key is derived from the `role_descriptors` you specify in the request, and a snapshot of the owner user’s permissions at the time of the request.
The snapshot of the owner’s permissions is updated automatically on every call.
If you don’t specify `role_descriptors` in the request, a call to this API might still change the API key’s access scope.
This change can occur if the owner user’s permissions have changed since the API key was created or last modified.
To update another user’s API key, use the `run_as` feature to submit a request on behalf of another user.
IMPORTANT: It’s not possible to use an API key as the authentication credential for this API.
To update an API key, the owner user’s credentials are required.

{ref}/security-api-update-api-key.html[Endpoint documentation]
[source,ts]
----
client.security.updateApiKey({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: The ID of the API key to update.
** *`role_descriptors` (Optional, Record<string, { cluster, indices, global, applications, metadata, run_as, description, transient_metadata }>)*: An array of role descriptors for this API key. This parameter is optional. When it is not specified or is an empty array, then the API key will have a point in time snapshot of permissions of the authenticated user. If you supply role descriptors then the resultant permissions would be an intersection of API keys permissions and authenticated user’s permissions thereby limiting the access scope for API keys. The structure of role descriptor is the same as the request for create role API. For more details, see create or update roles API.
** *`metadata` (Optional, Record<string, User-defined value>)*: Arbitrary metadata that you want to associate with the API key. It supports nested data structure. Within the metadata object, keys beginning with _ are reserved for system usage.
** *`expiration` (Optional, string | -1 | 0)*: Expiration time for the API key.

[discrete]
==== update_cross_cluster_api_key
Update a cross-cluster API key.

Update the attributes of an existing cross-cluster API key, which is used for API key based remote cluster access.

{ref}/security-api-update-cross-cluster-api-key.html[Endpoint documentation]
[source,ts]
----
client.security.updateCrossClusterApiKey({ id, access })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: The ID of the cross-cluster API key to update.
** *`access` ({ replication, search })*: The access to be granted to this API key.
The access is composed of permissions for cross cluster search and cross cluster replication.
At least one of them must be specified.
When specified, the new access assignment fully replaces the previously assigned access.
** *`expiration` (Optional, string | -1 | 0)*: Expiration time for the API key.
By default, API keys never expire. This property can be omitted to leave the value unchanged.
** *`metadata` (Optional, Record<string, User-defined value>)*: Arbitrary metadata that you want to associate with the API key.
It supports nested data structure.
Within the metadata object, keys beginning with `_` are reserved for system usage.
When specified, this information fully replaces metadata previously associated with the API key.

[discrete]
==== update_settings
Update settings for the security system index

{ref}/security-api-update-settings.html[Endpoint documentation]
[source,ts]
----
client.security.updateSettings()
----


[discrete]
==== update_user_profile_data
Update user profile data.

Update specific data for the user profile that is associated with a unique ID.

{ref}/security-api-update-user-profile-data.html[Endpoint documentation]
[source,ts]
----
client.security.updateUserProfileData({ uid })
----

[discrete]
==== Arguments

* *Request (object):*
** *`uid` (string)*: A unique identifier for the user profile.
** *`labels` (Optional, Record<string, User-defined value>)*: Searchable data that you want to associate with the user profile. This
field supports a nested data structure.
** *`data` (Optional, Record<string, User-defined value>)*: Non-searchable data that you want to associate with the user profile.
This field supports a nested data structure.
** *`if_seq_no` (Optional, number)*: Only perform the operation if the document has this sequence number.
** *`if_primary_term` (Optional, number)*: Only perform the operation if the document has this primary term.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If 'true', Elasticsearch refreshes the affected shards to make this operation
visible to search, if 'wait_for' then wait for a refresh to make this operation
visible to search, if 'false' do nothing with refreshes.

[discrete]
=== shutdown
[discrete]
==== delete_node
Removes a node from the shutdown list. Designed for indirect use by ECE/ESS and ECK. Direct use is not supported.

https://www.elastic.co/guide/en/elasticsearch/reference/current[Endpoint documentation]
[source,ts]
----
client.shutdown.deleteNode({ node_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`node_id` (string)*: The node id of node to be removed from the shutdown state
** *`master_timeout` (Optional, Enum("nanos" | "micros" | "ms" | "s" | "m" | "h" | "d"))*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, Enum("nanos" | "micros" | "ms" | "s" | "m" | "h" | "d"))*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== get_node
Retrieve status of a node or nodes that are currently marked as shutting down. Designed for indirect use by ECE/ESS and ECK. Direct use is not supported.

https://www.elastic.co/guide/en/elasticsearch/reference/current[Endpoint documentation]
[source,ts]
----
client.shutdown.getNode({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`node_id` (Optional, string | string[])*: Which node for which to retrieve the shutdown status
** *`master_timeout` (Optional, Enum("nanos" | "micros" | "ms" | "s" | "m" | "h" | "d"))*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, Enum("nanos" | "micros" | "ms" | "s" | "m" | "h" | "d"))*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== put_node
Adds a node to be shut down. Designed for indirect use by ECE/ESS and ECK. Direct use is not supported.

https://www.elastic.co/guide/en/elasticsearch/reference/current[Endpoint documentation]
[source,ts]
----
client.shutdown.putNode({ node_id, type, reason })
----

[discrete]
==== Arguments

* *Request (object):*
** *`node_id` (string)*: The node id of node to be shut down
** *`type` (Enum("restart" | "remove" | "replace"))*: Valid values are restart, remove, or replace.
Use restart when you need to temporarily shut down a node to perform an upgrade, make configuration changes, or perform other maintenance.
Because the node is expected to rejoin the cluster, data is not migrated off of the node.
Use remove when you need to permanently remove a node from the cluster.
The node is not marked ready for shutdown until data is migrated off of the node Use replace to do a 1:1 replacement of a node with another node.
Certain allocation decisions will be ignored (such as disk watermarks) in the interest of true replacement of the source node with the target node.
During a replace-type shutdown, rollover and index creation may result in unassigned shards, and shrink may fail until the replacement is complete.
** *`reason` (string)*: A human-readable reason that the node is being shut down.
This field provides information for other cluster operators; it does not affect the shut down process.
** *`allocation_delay` (Optional, string)*: Only valid if type is restart.
Controls how long Elasticsearch will wait for the node to restart and join the cluster before reassigning its shards to other nodes.
This works the same as delaying allocation with the index.unassigned.node_left.delayed_timeout setting.
If you specify both a restart allocation delay and an index-level allocation delay, the longer of the two is used.
** *`target_node_name` (Optional, string)*: Only valid if type is replace.
Specifies the name of the node that is replacing the node being shut down.
Shards from the shut down node are only allowed to be allocated to the target node, and no other data will be allocated to the target node.
During relocation of data certain allocation rules are ignored, such as disk watermarks or user attribute filtering rules.
** *`master_timeout` (Optional, Enum("nanos" | "micros" | "ms" | "s" | "m" | "h" | "d"))*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, Enum("nanos" | "micros" | "ms" | "s" | "m" | "h" | "d"))*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
=== simulate
[discrete]
==== ingest
Simulates running ingest with example documents.

{ref}/simulate-ingest-api.html[Endpoint documentation]
[source,ts]
----
client.simulate.ingest()
----


[discrete]
=== slm
[discrete]
==== delete_lifecycle
Deletes an existing snapshot lifecycle policy.

{ref}/slm-api-delete-policy.html[Endpoint documentation]
[source,ts]
----
client.slm.deleteLifecycle({ policy_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`policy_id` (string)*: The id of the snapshot lifecycle policy to remove

[discrete]
==== execute_lifecycle
Immediately creates a snapshot according to the lifecycle policy, without waiting for the scheduled time.

{ref}/slm-api-execute-lifecycle.html[Endpoint documentation]
[source,ts]
----
client.slm.executeLifecycle({ policy_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`policy_id` (string)*: The id of the snapshot lifecycle policy to be executed

[discrete]
==== execute_retention
Deletes any snapshots that are expired according to the policy's retention rules.

{ref}/slm-api-execute-retention.html[Endpoint documentation]
[source,ts]
----
client.slm.executeRetention()
----


[discrete]
==== get_lifecycle
Retrieves one or more snapshot lifecycle policy definitions and information about the latest snapshot attempts.

{ref}/slm-api-get-policy.html[Endpoint documentation]
[source,ts]
----
client.slm.getLifecycle({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`policy_id` (Optional, string | string[])*: List of snapshot lifecycle policies to retrieve

[discrete]
==== get_stats
Returns global and policy-level statistics about actions taken by snapshot lifecycle management.

{ref}/slm-api-get-stats.html[Endpoint documentation]
[source,ts]
----
client.slm.getStats()
----


[discrete]
==== get_status
Retrieves the status of snapshot lifecycle management (SLM).

{ref}/slm-api-get-status.html[Endpoint documentation]
[source,ts]
----
client.slm.getStatus()
----


[discrete]
==== put_lifecycle
Creates or updates a snapshot lifecycle policy.

{ref}/slm-api-put-policy.html[Endpoint documentation]
[source,ts]
----
client.slm.putLifecycle({ policy_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`policy_id` (string)*: ID for the snapshot lifecycle policy you want to create or update.
** *`config` (Optional, { ignore_unavailable, indices, include_global_state, feature_states, metadata, partial })*: Configuration for each snapshot created by the policy.
** *`name` (Optional, string)*: Name automatically assigned to each snapshot created by the policy. Date math is supported. To prevent conflicting snapshot names, a UUID is automatically appended to each snapshot name.
** *`repository` (Optional, string)*: Repository used to store snapshots created by this policy. This repository must exist prior to the policy’s creation. You can create a repository using the snapshot repository API.
** *`retention` (Optional, { expire_after, max_count, min_count })*: Retention rules used to retain and delete snapshots created by the policy.
** *`schedule` (Optional, string)*: Periodic or absolute schedule at which the policy creates snapshots. SLM applies schedule changes immediately.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== start
Turns on snapshot lifecycle management (SLM).

{ref}/slm-api-start.html[Endpoint documentation]
[source,ts]
----
client.slm.start()
----


[discrete]
==== stop
Turns off snapshot lifecycle management (SLM).

{ref}/slm-api-stop.html[Endpoint documentation]
[source,ts]
----
client.slm.stop()
----


[discrete]
=== snapshot
[discrete]
==== cleanup_repository
Triggers the review of a snapshot repository’s contents and deletes any stale data not referenced by existing snapshots.

{ref}/clean-up-snapshot-repo-api.html[Endpoint documentation]
[source,ts]
----
client.snapshot.cleanupRepository({ repository })
----

[discrete]
==== Arguments

* *Request (object):*
** *`repository` (string)*: Snapshot repository to clean up.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.

[discrete]
==== clone
Clones indices from one snapshot into another snapshot in the same repository.

{ref}/modules-snapshots.html[Endpoint documentation]
[source,ts]
----
client.snapshot.clone({ repository, snapshot, target_snapshot, indices })
----

[discrete]
==== Arguments

* *Request (object):*
** *`repository` (string)*: A repository name
** *`snapshot` (string)*: The name of the snapshot to clone from
** *`target_snapshot` (string)*: The name of the cloned snapshot to create
** *`indices` (string)*
** *`master_timeout` (Optional, string | -1 | 0)*: Explicit operation timeout for connection to master node
** *`timeout` (Optional, string | -1 | 0)*

[discrete]
==== create
Creates a snapshot in a repository.

{ref}/modules-snapshots.html[Endpoint documentation]
[source,ts]
----
client.snapshot.create({ repository, snapshot })
----

[discrete]
==== Arguments

* *Request (object):*
** *`repository` (string)*: Repository for the snapshot.
** *`snapshot` (string)*: Name of the snapshot. Must be unique in the repository.
** *`ignore_unavailable` (Optional, boolean)*: If `true`, the request ignores data streams and indices in `indices` that are missing or closed. If `false`, the request returns an error for any data stream or index that is missing or closed.
** *`include_global_state` (Optional, boolean)*: If `true`, the current cluster state is included in the snapshot. The cluster state includes persistent cluster settings, composable index templates, legacy index templates, ingest pipelines, and ILM policies. It also includes data stored in system indices, such as Watches and task records (configurable via `feature_states`).
** *`indices` (Optional, string | string[])*: Data streams and indices to include in the snapshot. Supports multi-target syntax. Includes all data streams and indices by default.
** *`feature_states` (Optional, string[])*: Feature states to include in the snapshot. Each feature state includes one or more system indices containing related data. You can view a list of eligible features using the get features API. If `include_global_state` is `true`, all current feature states are included by default. If `include_global_state` is `false`, no feature states are included by default.
** *`metadata` (Optional, Record<string, User-defined value>)*: Optional metadata for the snapshot. May have any contents. Must be less than 1024 bytes. This map is not automatically generated by Elasticsearch.
** *`partial` (Optional, boolean)*: If `true`, allows restoring a partial snapshot of indices with unavailable shards. Only shards that were successfully included in the snapshot will be restored. All missing shards will be recreated as empty. If `false`, the entire restore operation will fail if one or more indices included in the snapshot do not have all primary shards available.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`wait_for_completion` (Optional, boolean)*: If `true`, the request returns a response when the snapshot is complete. If `false`, the request returns a response when the snapshot initializes.

[discrete]
==== create_repository
Creates a repository.

{ref}/modules-snapshots.html[Endpoint documentation]
[source,ts]
----
client.snapshot.createRepository({ repository })
----

[discrete]
==== Arguments

* *Request (object):*
** *`repository` (string)*: A repository name
** *`master_timeout` (Optional, string | -1 | 0)*: Explicit operation timeout for connection to master node
** *`timeout` (Optional, string | -1 | 0)*: Explicit operation timeout
** *`verify` (Optional, boolean)*: Whether to verify the repository after creation

[discrete]
==== delete
Deletes one or more snapshots.

{ref}/modules-snapshots.html[Endpoint documentation]
[source,ts]
----
client.snapshot.delete({ repository, snapshot })
----

[discrete]
==== Arguments

* *Request (object):*
** *`repository` (string)*: A repository name
** *`snapshot` (string)*: A list of snapshot names
** *`master_timeout` (Optional, string | -1 | 0)*: Explicit operation timeout for connection to master node

[discrete]
==== delete_repository
Deletes a repository.

{ref}/modules-snapshots.html[Endpoint documentation]
[source,ts]
----
client.snapshot.deleteRepository({ repository })
----

[discrete]
==== Arguments

* *Request (object):*
** *`repository` (string | string[])*: Name of the snapshot repository to unregister. Wildcard (`*`) patterns are supported.
** *`master_timeout` (Optional, string | -1 | 0)*: Explicit operation timeout for connection to master node
** *`timeout` (Optional, string | -1 | 0)*: Explicit operation timeout

[discrete]
==== get
Returns information about a snapshot.

{ref}/modules-snapshots.html[Endpoint documentation]
[source,ts]
----
client.snapshot.get({ repository, snapshot })
----

[discrete]
==== Arguments

* *Request (object):*
** *`repository` (string)*: List of snapshot repository names used to limit the request. Wildcard (*) expressions are supported.
** *`snapshot` (string | string[])*: List of snapshot names to retrieve. Also accepts wildcards (*).
- To get information about all snapshots in a registered repository, use a wildcard (*) or _all.
- To get information about any snapshots that are currently running, use _current.
** *`ignore_unavailable` (Optional, boolean)*: If false, the request returns an error for any snapshots that are unavailable.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`verbose` (Optional, boolean)*: If true, returns additional information about each snapshot such as the version of Elasticsearch which took the snapshot, the start and end times of the snapshot, and the number of shards snapshotted.
** *`index_details` (Optional, boolean)*: If true, returns additional information about each index in the snapshot comprising the number of shards in the index, the total size of the index in bytes, and the maximum number of segments per shard in the index. Defaults to false, meaning that this information is omitted.
** *`index_names` (Optional, boolean)*: If true, returns the name of each index in each snapshot.
** *`include_repository` (Optional, boolean)*: If true, returns the repository name in each snapshot.
** *`sort` (Optional, Enum("start_time" | "duration" | "name" | "index_count" | "repository" | "shard_count" | "failed_shard_count"))*: Allows setting a sort order for the result. Defaults to start_time, i.e. sorting by snapshot start time stamp.
** *`size` (Optional, number)*: Maximum number of snapshots to return. Defaults to 0 which means return all that match the request without limit.
** *`order` (Optional, Enum("asc" | "desc"))*: Sort order. Valid values are asc for ascending and desc for descending order. Defaults to asc, meaning ascending order.
** *`after` (Optional, string)*: Offset identifier to start pagination from as returned by the next field in the response body.
** *`offset` (Optional, number)*: Numeric offset to start pagination from based on the snapshots matching this request. Using a non-zero value for this parameter is mutually exclusive with using the after parameter. Defaults to 0.
** *`from_sort_value` (Optional, string)*: Value of the current sort column at which to start retrieval. Can either be a string snapshot- or repository name when sorting by snapshot or repository name, a millisecond time value or a number when sorting by index- or shard count.
** *`slm_policy_filter` (Optional, string)*: Filter snapshots by a list of SLM policy names that snapshots belong to. Also accepts wildcards (*) and combinations of wildcards followed by exclude patterns starting with -. To include snapshots not created by an SLM policy you can use the special pattern _none that will match all snapshots without an SLM policy.

[discrete]
==== get_repository
Returns information about a repository.

{ref}/modules-snapshots.html[Endpoint documentation]
[source,ts]
----
client.snapshot.getRepository({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`repository` (Optional, string | string[])*: A list of repository names
** *`local` (Optional, boolean)*: Return local information, do not retrieve the state from master node (default: false)
** *`master_timeout` (Optional, string | -1 | 0)*: Explicit operation timeout for connection to master node

[discrete]
==== repository_analyze
Analyzes a repository for correctness and performance

{ref}/modules-snapshots.html[Endpoint documentation]
[source,ts]
----
client.snapshot.repositoryAnalyze()
----


[discrete]
==== restore
Restores a snapshot.

{ref}/modules-snapshots.html[Endpoint documentation]
[source,ts]
----
client.snapshot.restore({ repository, snapshot })
----

[discrete]
==== Arguments

* *Request (object):*
** *`repository` (string)*: A repository name
** *`snapshot` (string)*: A snapshot name
** *`feature_states` (Optional, string[])*
** *`ignore_index_settings` (Optional, string[])*
** *`ignore_unavailable` (Optional, boolean)*
** *`include_aliases` (Optional, boolean)*
** *`include_global_state` (Optional, boolean)*
** *`index_settings` (Optional, { index, mode, routing_path, soft_deletes, sort, number_of_shards, number_of_replicas, number_of_routing_shards, check_on_startup, codec, routing_partition_size, load_fixed_bitset_filters_eagerly, hidden, auto_expand_replicas, merge, search, refresh_interval, max_result_window, max_inner_result_window, max_rescore_window, max_docvalue_fields_search, max_script_fields, max_ngram_diff, max_shingle_diff, blocks, max_refresh_listeners, analyze, highlight, max_terms_count, max_regex_length, routing, gc_deletes, default_pipeline, final_pipeline, lifecycle, provided_name, creation_date, creation_date_string, uuid, version, verified_before_close, format, max_slices_per_scroll, translog, query_string, priority, top_metrics_max_size, analysis, settings, time_series, queries, similarity, mapping, indexing.slowlog, indexing_pressure, store })*
** *`indices` (Optional, string | string[])*
** *`partial` (Optional, boolean)*
** *`rename_pattern` (Optional, string)*
** *`rename_replacement` (Optional, string)*
** *`master_timeout` (Optional, string | -1 | 0)*: Explicit operation timeout for connection to master node
** *`wait_for_completion` (Optional, boolean)*: Should this request wait until the operation has completed before returning

[discrete]
==== status
Returns information about the status of a snapshot.

{ref}/modules-snapshots.html[Endpoint documentation]
[source,ts]
----
client.snapshot.status({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`repository` (Optional, string)*: A repository name
** *`snapshot` (Optional, string | string[])*: A list of snapshot names
** *`ignore_unavailable` (Optional, boolean)*: Whether to ignore unavailable snapshots, defaults to false which means a SnapshotMissingException is thrown
** *`master_timeout` (Optional, string | -1 | 0)*: Explicit operation timeout for connection to master node

[discrete]
==== verify_repository
Verifies a repository.

{ref}/modules-snapshots.html[Endpoint documentation]
[source,ts]
----
client.snapshot.verifyRepository({ repository })
----

[discrete]
==== Arguments

* *Request (object):*
** *`repository` (string)*: A repository name
** *`master_timeout` (Optional, string | -1 | 0)*: Explicit operation timeout for connection to master node
** *`timeout` (Optional, string | -1 | 0)*: Explicit operation timeout

[discrete]
=== sql
[discrete]
==== clear_cursor
Clears the SQL cursor

{ref}/clear-sql-cursor-api.html[Endpoint documentation]
[source,ts]
----
client.sql.clearCursor({ cursor })
----

[discrete]
==== Arguments

* *Request (object):*
** *`cursor` (string)*: Cursor to clear.

[discrete]
==== delete_async
Deletes an async SQL search or a stored synchronous SQL search. If the search is still running, the API cancels it.

{ref}/delete-async-sql-search-api.html[Endpoint documentation]
[source,ts]
----
client.sql.deleteAsync({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the search.

[discrete]
==== get_async
Returns the current status and available results for an async SQL search or stored synchronous SQL search

{ref}/get-async-sql-search-api.html[Endpoint documentation]
[source,ts]
----
client.sql.getAsync({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the search.
** *`delimiter` (Optional, string)*: Separator for CSV results. The API only supports this parameter for CSV responses.
** *`format` (Optional, string)*: Format for the response. You must specify a format using this parameter or the
Accept HTTP header. If you specify both, the API uses this parameter.
** *`keep_alive` (Optional, string | -1 | 0)*: Retention period for the search and its results. Defaults
to the `keep_alive` period for the original SQL search.
** *`wait_for_completion_timeout` (Optional, string | -1 | 0)*: Period to wait for complete results. Defaults to no timeout,
meaning the request waits for complete search results.

[discrete]
==== get_async_status
Returns the current status of an async SQL search or a stored synchronous SQL search

{ref}/get-async-sql-search-status-api.html[Endpoint documentation]
[source,ts]
----
client.sql.getAsyncStatus({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the search.

[discrete]
==== query
Executes a SQL request

{ref}/sql-search-api.html[Endpoint documentation]
[source,ts]
----
client.sql.query({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`catalog` (Optional, string)*: Default catalog (cluster) for queries. If unspecified, the queries execute on the data in the local cluster only.
** *`columnar` (Optional, boolean)*: If true, the results in a columnar fashion: one row represents all the values of a certain column from the current page of results.
** *`cursor` (Optional, string)*: Cursor used to retrieve a set of paginated results.
If you specify a cursor, the API only uses the `columnar` and `time_zone` request body parameters.
It ignores other request body parameters.
** *`fetch_size` (Optional, number)*: The maximum number of rows (or entries) to return in one response
** *`filter` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Elasticsearch query DSL for additional filtering.
** *`query` (Optional, string)*: SQL query to run.
** *`request_timeout` (Optional, string | -1 | 0)*: The timeout before the request fails.
** *`page_timeout` (Optional, string | -1 | 0)*: The timeout before a pagination request fails.
** *`time_zone` (Optional, string)*: ISO-8601 time zone ID for the search.
** *`field_multi_value_leniency` (Optional, boolean)*: Throw an exception when encountering multiple values for a field (default) or be lenient and return the first value from the list (without any guarantees of what that will be - typically the first in natural ascending order).
** *`runtime_mappings` (Optional, Record<string, { fields, fetch_fields, format, input_field, target_field, target_index, script, type }>)*: Defines one or more runtime fields in the search request. These fields take
precedence over mapped fields with the same name.
** *`wait_for_completion_timeout` (Optional, string | -1 | 0)*: Period to wait for complete results. Defaults to no timeout, meaning the request waits for complete search results. If the search doesn’t finish within this period, the search becomes async.
** *`params` (Optional, Record<string, User-defined value>)*: Values for parameters in the query.
** *`keep_alive` (Optional, string | -1 | 0)*: Retention period for an async or saved synchronous search.
** *`keep_on_completion` (Optional, boolean)*: If true, Elasticsearch stores synchronous searches if you also specify the wait_for_completion_timeout parameter. If false, Elasticsearch only stores async searches that don’t finish before the wait_for_completion_timeout.
** *`index_using_frozen` (Optional, boolean)*: If true, the search can run on frozen indices. Defaults to false.
** *`format` (Optional, Enum("csv" | "json" | "tsv" | "txt" | "yaml" | "cbor" | "smile"))*: Format for the response.

[discrete]
==== translate
Translates SQL into Elasticsearch queries

{ref}/sql-translate-api.html[Endpoint documentation]
[source,ts]
----
client.sql.translate({ query })
----

[discrete]
==== Arguments

* *Request (object):*
** *`query` (string)*: SQL query to run.
** *`fetch_size` (Optional, number)*: The maximum number of rows (or entries) to return in one response.
** *`filter` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Elasticsearch query DSL for additional filtering.
** *`time_zone` (Optional, string)*: ISO-8601 time zone ID for the search.

[discrete]
=== ssl
[discrete]
==== certificates
Get SSL certificates.

Get information about the X.509 certificates that are used to encrypt communications in the cluster.
The API returns a list that includes certificates from all TLS contexts including:

- Settings for transport and HTTP interfaces
- TLS settings that are used within authentication realms
- TLS settings for remote monitoring exporters

The list includes certificates that are used for configuring trust, such as those configured in the `xpack.security.transport.ssl.truststore` and `xpack.security.transport.ssl.certificate_authorities` settings.
It also includes certificates that are used for configuring server identity, such as `xpack.security.http.ssl.keystore` and `xpack.security.http.ssl.certificate settings`.

The list does not include certificates that are sourced from the default SSL context of the Java Runtime Environment (JRE), even if those certificates are in use within Elasticsearch.

NOTE: When a PKCS#11 token is configured as the truststore of the JRE, the API returns all the certificates that are included in the PKCS#11 token irrespective of whether these are used in the Elasticsearch TLS configuration.

If Elasticsearch is configured to use a keystore or truststore, the API output includes all certificates in that store, even though some of the certificates might not be in active use within the cluster.

{ref}/security-api-ssl.html[Endpoint documentation]
[source,ts]
----
client.ssl.certificates()
----


[discrete]
=== synonyms
[discrete]
==== delete_synonym
Deletes a synonym set

{ref}/delete-synonyms-set.html[Endpoint documentation]
[source,ts]
----
client.synonyms.deleteSynonym({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: The id of the synonyms set to be deleted

[discrete]
==== delete_synonym_rule
Deletes a synonym rule in a synonym set

{ref}/delete-synonym-rule.html[Endpoint documentation]
[source,ts]
----
client.synonyms.deleteSynonymRule({ set_id, rule_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`set_id` (string)*: The id of the synonym set to be updated
** *`rule_id` (string)*: The id of the synonym rule to be deleted

[discrete]
==== get_synonym
Retrieves a synonym set

{ref}/get-synonyms-set.html[Endpoint documentation]
[source,ts]
----
client.synonyms.getSynonym({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: "The id of the synonyms set to be retrieved
** *`from` (Optional, number)*: Starting offset for query rules to be retrieved
** *`size` (Optional, number)*: specifies a max number of query rules to retrieve

[discrete]
==== get_synonym_rule
Retrieves a synonym rule from a synonym set

{ref}/get-synonym-rule.html[Endpoint documentation]
[source,ts]
----
client.synonyms.getSynonymRule({ set_id, rule_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`set_id` (string)*: The id of the synonym set to retrieve the synonym rule from
** *`rule_id` (string)*: The id of the synonym rule to retrieve

[discrete]
==== get_synonyms_sets
Retrieves a summary of all defined synonym sets

{ref}/list-synonyms-sets.html[Endpoint documentation]
[source,ts]
----
client.synonyms.getSynonymsSets({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`from` (Optional, number)*: Starting offset
** *`size` (Optional, number)*: specifies a max number of results to get

[discrete]
==== put_synonym
Creates or updates a synonym set.

{ref}/put-synonyms-set.html[Endpoint documentation]
[source,ts]
----
client.synonyms.putSynonym({ id, synonyms_set })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: The id of the synonyms set to be created or updated
** *`synonyms_set` ({ id, synonyms } | { id, synonyms }[])*: The synonym set information to update

[discrete]
==== put_synonym_rule
Creates or updates a synonym rule in a synonym set

{ref}/put-synonym-rule.html[Endpoint documentation]
[source,ts]
----
client.synonyms.putSynonymRule({ set_id, rule_id, synonyms })
----

[discrete]
==== Arguments

* *Request (object):*
** *`set_id` (string)*: The id of the synonym set to be updated with the synonym rule
** *`rule_id` (string)*: The id of the synonym rule to be updated or created
** *`synonyms` (string)*

[discrete]
=== tasks
[discrete]
==== cancel
Cancels a task, if it can be cancelled through an API.

{ref}/tasks.html[Endpoint documentation]
[source,ts]
----
client.tasks.cancel({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`task_id` (Optional, string | number)*: ID of the task.
** *`actions` (Optional, string | string[])*: List or wildcard expression of actions used to limit the request.
** *`nodes` (Optional, string[])*: List of node IDs or names used to limit the request.
** *`parent_task_id` (Optional, string)*: Parent task ID used to limit the tasks.
** *`wait_for_completion` (Optional, boolean)*: Should the request block until the cancellation of the task and its descendant tasks is completed. Defaults to false

[discrete]
==== get
Get task information.
Returns information about the tasks currently executing in the cluster.

{ref}/tasks.html[Endpoint documentation]
[source,ts]
----
client.tasks.get({ task_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`task_id` (string)*: ID of the task.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.
** *`wait_for_completion` (Optional, boolean)*: If `true`, the request blocks until the task has completed.

[discrete]
==== list
The task management API returns information about tasks currently executing on one or more nodes in the cluster.

{ref}/tasks.html[Endpoint documentation]
[source,ts]
----
client.tasks.list({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`actions` (Optional, string | string[])*: List or wildcard expression of actions used to limit the request.
** *`detailed` (Optional, boolean)*: If `true`, the response includes detailed information about shard recoveries.
** *`group_by` (Optional, Enum("nodes" | "parents" | "none"))*: Key used to group tasks in the response.
** *`node_id` (Optional, string[])*: List of node IDs or names used to limit returned information.
** *`parent_task_id` (Optional, string)*: Parent task ID used to limit returned information. To return all tasks, omit this parameter or use a value of `-1`.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.
** *`wait_for_completion` (Optional, boolean)*: If `true`, the request blocks until the operation is complete.

[discrete]
=== text_structure
[discrete]
==== find_field_structure
Finds the structure of a text field in an index.

{ref}/find-field-structure.html[Endpoint documentation]
[source,ts]
----
client.textStructure.findFieldStructure()
----


[discrete]
==== find_message_structure
Finds the structure of a list of messages. The messages must contain data that is suitable to be ingested into Elasticsearch.

{ref}/find-message-structure.html[Endpoint documentation]
[source,ts]
----
client.textStructure.findMessageStructure()
----


[discrete]
==== find_structure
Finds the structure of a text file. The text file must contain data that is suitable to be ingested into Elasticsearch.

{ref}/find-structure.html[Endpoint documentation]
[source,ts]
----
client.textStructure.findStructure({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`text_files` (Optional, TJsonDocument[])*
** *`charset` (Optional, string)*: The text’s character set. It must be a character set that is supported by the JVM that Elasticsearch uses. For example, UTF-8, UTF-16LE, windows-1252, or EUC-JP. If this parameter is not specified, the structure finder chooses an appropriate character set.
** *`column_names` (Optional, string)*: If you have set format to delimited, you can specify the column names in a list. If this parameter is not specified, the structure finder uses the column names from the header row of the text. If the text does not have a header role, columns are named "column1", "column2", "column3", etc.
** *`delimiter` (Optional, string)*: If you have set format to delimited, you can specify the character used to delimit the values in each row. Only a single character is supported; the delimiter cannot have multiple characters. By default, the API considers the following possibilities: comma, tab, semi-colon, and pipe (|). In this default scenario, all rows must have the same number of fields for the delimited format to be detected. If you specify a delimiter, up to 10% of the rows can have a different number of columns than the first row.
** *`ecs_compatibility` (Optional, string)*: The mode of compatibility with ECS compliant Grok patterns (disabled or v1, default: disabled).
** *`explain` (Optional, boolean)*: If this parameter is set to true, the response includes a field named explanation, which is an array of strings that indicate how the structure finder produced its result.
** *`format` (Optional, string)*: The high level structure of the text. Valid values are ndjson, xml, delimited, and semi_structured_text. By default, the API chooses the format. In this default scenario, all rows must have the same number of fields for a delimited format to be detected. If the format is set to delimited and the delimiter is not set, however, the API tolerates up to 5% of rows that have a different number of columns than the first row.
** *`grok_pattern` (Optional, string)*: If you have set format to semi_structured_text, you can specify a Grok pattern that is used to extract fields from every message in the text. The name of the timestamp field in the Grok pattern must match what is specified in the timestamp_field parameter. If that parameter is not specified, the name of the timestamp field in the Grok pattern must match "timestamp". If grok_pattern is not specified, the structure finder creates a Grok pattern.
** *`has_header_row` (Optional, boolean)*: If you have set format to delimited, you can use this parameter to indicate whether the column names are in the first row of the text. If this parameter is not specified, the structure finder guesses based on the similarity of the first row of the text to other rows.
** *`line_merge_size_limit` (Optional, number)*: The maximum number of characters in a message when lines are merged to form messages while analyzing semi-structured text. If you have extremely long messages you may need to increase this, but be aware that this may lead to very long processing times if the way to group lines into messages is misdetected.
** *`lines_to_sample` (Optional, number)*: The number of lines to include in the structural analysis, starting from the beginning of the text. The minimum is 2; If the value of this parameter is greater than the number of lines in the text, the analysis proceeds (as long as there are at least two lines in the text) for all of the lines.
** *`quote` (Optional, string)*: If you have set format to delimited, you can specify the character used to quote the values in each row if they contain newlines or the delimiter character. Only a single character is supported. If this parameter is not specified, the default value is a double quote ("). If your delimited text format does not use quoting, a workaround is to set this argument to a character that does not appear anywhere in the sample.
** *`should_trim_fields` (Optional, boolean)*: If you have set format to delimited, you can specify whether values between delimiters should have whitespace trimmed from them. If this parameter is not specified and the delimiter is pipe (|), the default value is true. Otherwise, the default value is false.
** *`timeout` (Optional, string | -1 | 0)*: Sets the maximum amount of time that the structure analysis make take. If the analysis is still running when the timeout expires then it will be aborted.
** *`timestamp_field` (Optional, string)*: Optional parameter to specify the timestamp field in the file
** *`timestamp_format` (Optional, string)*: The Java time format of the timestamp field in the text.

[discrete]
==== test_grok_pattern
Tests a Grok pattern on some text.

{ref}/test-grok-pattern.html[Endpoint documentation]
[source,ts]
----
client.textStructure.testGrokPattern({ grok_pattern, text })
----

[discrete]
==== Arguments

* *Request (object):*
** *`grok_pattern` (string)*: Grok pattern to run on the text.
** *`text` (string[])*: Lines of text to run the Grok pattern on.
** *`ecs_compatibility` (Optional, string)*: The mode of compatibility with ECS compliant Grok patterns (disabled or v1, default: disabled).

[discrete]
=== transform
[discrete]
==== delete_transform
Delete a transform.
Deletes a transform.

{ref}/delete-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.deleteTransform({ transform_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (string)*: Identifier for the transform.
** *`force` (Optional, boolean)*: If this value is false, the transform must be stopped before it can be deleted. If true, the transform is
deleted regardless of its current state.
** *`delete_dest_index` (Optional, boolean)*: If this value is true, the destination index is deleted together with the transform. If false, the destination
index will not be deleted
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== get_node_stats
Retrieves transform usage information for transform nodes.
[source,ts]
----
client.transform.getNodeStats()
----


[discrete]
==== get_transform
Get transforms.
Retrieves configuration information for transforms.

{ref}/get-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.getTransform({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (Optional, string | string[])*: Identifier for the transform. It can be a transform identifier or a
wildcard expression. You can get information for all transforms by using
`_all`, by specifying `*` as the `<transform_id>`, or by omitting the
`<transform_id>`.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no transforms that match.
2. Contains the _all string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

If this parameter is false, the request returns a 404 status code when
there are no matches or only partial matches.
** *`from` (Optional, number)*: Skips the specified number of transforms.
** *`size` (Optional, number)*: Specifies the maximum number of transforms to obtain.
** *`exclude_generated` (Optional, boolean)*: Excludes fields that were automatically added when creating the
transform. This allows the configuration to be in an acceptable format to
be retrieved and then added to another cluster.

[discrete]
==== get_transform_stats
Get transform stats.
Retrieves usage information for transforms.

{ref}/get-transform-stats.html[Endpoint documentation]
[source,ts]
----
client.transform.getTransformStats({ transform_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (string | string[])*: Identifier for the transform. It can be a transform identifier or a
wildcard expression. You can get information for all transforms by using
`_all`, by specifying `*` as the `<transform_id>`, or by omitting the
`<transform_id>`.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no transforms that match.
2. Contains the _all string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

If this parameter is false, the request returns a 404 status code when
there are no matches or only partial matches.
** *`from` (Optional, number)*: Skips the specified number of transforms.
** *`size` (Optional, number)*: Specifies the maximum number of transforms to obtain.
** *`timeout` (Optional, string | -1 | 0)*: Controls the time to wait for the stats

[discrete]
==== preview_transform
Preview a transform.
Generates a preview of the results that you will get when you create a transform with the same configuration.

It returns a maximum of 100 results. The calculations are based on all the current data in the source index. It also
generates a list of mappings and settings for the destination index. These values are determined based on the field
types of the source index and the transform aggregations.

{ref}/preview-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.previewTransform({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (Optional, string)*: Identifier for the transform to preview. If you specify this path parameter, you cannot provide transform
configuration details in the request body.
** *`dest` (Optional, { index, op_type, pipeline, routing, version_type })*: The destination for the transform.
** *`description` (Optional, string)*: Free text description of the transform.
** *`frequency` (Optional, string | -1 | 0)*: The interval between checks for changes in the source indices when the
transform is running continuously. Also determines the retry interval in
the event of transient failures while the transform is searching or
indexing. The minimum value is 1s and the maximum is 1h.
** *`pivot` (Optional, { aggregations, group_by })*: The pivot method transforms the data by aggregating and grouping it.
These objects define the group by fields and the aggregation to reduce
the data.
** *`source` (Optional, { index, query, remote, size, slice, sort, _source, runtime_mappings })*: The source of the data for the transform.
** *`settings` (Optional, { align_checkpoints, dates_as_epoch_millis, deduce_mappings, docs_per_second, max_page_search_size, unattended })*: Defines optional transform settings.
** *`sync` (Optional, { time })*: Defines the properties transforms require to run continuously.
** *`retention_policy` (Optional, { time })*: Defines a retention policy for the transform. Data that meets the defined
criteria is deleted from the destination index.
** *`latest` (Optional, { sort, unique_key })*: The latest method transforms the data by finding the latest document for
each unique key.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the
timeout expires, the request fails and returns an error.

[discrete]
==== put_transform
Create a transform.
Creates a transform.

A transform copies data from source indices, transforms it, and persists it into an entity-centric destination index. You can also think of the destination index as a two-dimensional tabular data structure (known as
a data frame). The ID for each document in the data frame is generated from a hash of the entity, so there is a
unique row per entity.

You must choose either the latest or pivot method for your transform; you cannot use both in a single transform. If
you choose to use the pivot method for your transform, the entities are defined by the set of `group_by` fields in
the pivot object. If you choose to use the latest method, the entities are defined by the `unique_key` field values
in the latest object.

You must have `create_index`, `index`, and `read` privileges on the destination index and `read` and
`view_index_metadata` privileges on the source indices. When Elasticsearch security features are enabled, the
transform remembers which roles the user that created it had at the time of creation and uses those same roles. If
those roles do not have the required privileges on the source and destination indices, the transform fails when it
attempts unauthorized operations.

NOTE: You must use Kibana or this API to create a transform. Do not add a transform directly into any
`.transform-internal*` indices using the Elasticsearch index API. If Elasticsearch security features are enabled, do
not give users any privileges on `.transform-internal*` indices. If you used transforms prior to 7.5, also do not
give users any privileges on `.data-frame-internal*` indices.

{ref}/put-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.putTransform({ transform_id, dest, source })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (string)*: Identifier for the transform. This identifier can contain lowercase alphanumeric characters (a-z and 0-9),
hyphens, and underscores. It has a 64 character limit and must start and end with alphanumeric characters.
** *`dest` ({ index, op_type, pipeline, routing, version_type })*: The destination for the transform.
** *`source` ({ index, query, remote, size, slice, sort, _source, runtime_mappings })*: The source of the data for the transform.
** *`description` (Optional, string)*: Free text description of the transform.
** *`frequency` (Optional, string | -1 | 0)*: The interval between checks for changes in the source indices when the transform is running continuously. Also
determines the retry interval in the event of transient failures while the transform is searching or indexing.
The minimum value is `1s` and the maximum is `1h`.
** *`latest` (Optional, { sort, unique_key })*: The latest method transforms the data by finding the latest document for each unique key.
** *`_meta` (Optional, Record<string, User-defined value>)*: Defines optional transform metadata.
** *`pivot` (Optional, { aggregations, group_by })*: The pivot method transforms the data by aggregating and grouping it. These objects define the group by fields
and the aggregation to reduce the data.
** *`retention_policy` (Optional, { time })*: Defines a retention policy for the transform. Data that meets the defined criteria is deleted from the
destination index.
** *`settings` (Optional, { align_checkpoints, dates_as_epoch_millis, deduce_mappings, docs_per_second, max_page_search_size, unattended })*: Defines optional transform settings.
** *`sync` (Optional, { time })*: Defines the properties transforms require to run continuously.
** *`defer_validation` (Optional, boolean)*: When the transform is created, a series of validations occur to ensure its success. For example, there is a
check for the existence of the source indices and a check that the destination index is not part of the source
index pattern. You can use this parameter to skip the checks, for example when the source index does not exist
until after the transform is created. The validations are always run when you start the transform, however, with
the exception of privilege checks.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== reset_transform
Reset a transform.
Resets a transform.
Before you can reset it, you must stop it; alternatively, use the `force` query parameter.
If the destination index was created by the transform, it is deleted.

{ref}/reset-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.resetTransform({ transform_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (string)*: Identifier for the transform. This identifier can contain lowercase alphanumeric characters (a-z and 0-9),
hyphens, and underscores. It has a 64 character limit and must start and end with alphanumeric characters.
** *`force` (Optional, boolean)*: If this value is `true`, the transform is reset regardless of its current state. If it's `false`, the transform
must be stopped before it can be reset.

[discrete]
==== schedule_now_transform
Schedule a transform to start now.
Instantly runs a transform to process data.

If you _schedule_now a transform, it will process the new data instantly,
without waiting for the configured frequency interval. After _schedule_now API is called,
the transform will be processed again at now + frequency unless _schedule_now API
is called again in the meantime.

{ref}/schedule-now-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.scheduleNowTransform({ transform_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (string)*: Identifier for the transform.
** *`timeout` (Optional, string | -1 | 0)*: Controls the time to wait for the scheduling to take place

[discrete]
==== start_transform
Start a transform.
Starts a transform.

When you start a transform, it creates the destination index if it does not already exist. The `number_of_shards` is
set to `1` and the `auto_expand_replicas` is set to `0-1`. If it is a pivot transform, it deduces the mapping
definitions for the destination index from the source indices and the transform aggregations. If fields in the
destination index are derived from scripts (as in the case of `scripted_metric` or `bucket_script` aggregations),
the transform uses dynamic mappings unless an index template exists. If it is a latest transform, it does not deduce
mapping definitions; it uses dynamic mappings. To use explicit mappings, create the destination index before you
start the transform. Alternatively, you can create an index template, though it does not affect the deduced mappings
in a pivot transform.

When the transform starts, a series of validations occur to ensure its success. If you deferred validation when you
created the transform, they occur when you start the transform—​with the exception of privilege checks. When
Elasticsearch security features are enabled, the transform remembers which roles the user that created it had at the
time of creation and uses those same roles. If those roles do not have the required privileges on the source and
destination indices, the transform fails when it attempts unauthorized operations.

{ref}/start-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.startTransform({ transform_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (string)*: Identifier for the transform.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.
** *`from` (Optional, string)*: Restricts the set of transformed entities to those changed after this time. Relative times like now-30d are supported. Only applicable for continuous transforms.

[discrete]
==== stop_transform
Stop transforms.
Stops one or more transforms.

{ref}/stop-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.stopTransform({ transform_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (string)*: Identifier for the transform. To stop multiple transforms, use a list or a wildcard expression.
To stop all transforms, use `_all` or `*` as the identifier.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request: contains wildcard expressions and there are no transforms that match;
contains the `_all` string or no identifiers and there are no matches; contains wildcard expressions and there
are only partial matches.

If it is true, the API returns a successful acknowledgement message when there are no matches. When there are
only partial matches, the API stops the appropriate transforms.

If it is false, the request returns a 404 status code when there are no matches or only partial matches.
** *`force` (Optional, boolean)*: If it is true, the API forcefully stops the transforms.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response when `wait_for_completion` is `true`. If no response is received before the
timeout expires, the request returns a timeout exception. However, the request continues processing and
eventually moves the transform to a STOPPED state.
** *`wait_for_checkpoint` (Optional, boolean)*: If it is true, the transform does not completely stop until the current checkpoint is completed. If it is false,
the transform stops as soon as possible.
** *`wait_for_completion` (Optional, boolean)*: If it is true, the API blocks until the indexer state completely stops. If it is false, the API returns
immediately and the indexer is stopped asynchronously in the background.

[discrete]
==== update_transform
Update a transform.
Updates certain properties of a transform.

All updated properties except `description` do not take effect until after the transform starts the next checkpoint,
thus there is data consistency in each checkpoint. To use this API, you must have `read` and `view_index_metadata`
privileges for the source indices. You must also have `index` and `read` privileges for the destination index. When
Elasticsearch security features are enabled, the transform remembers which roles the user who updated it had at the
time of update and runs with those privileges.

{ref}/update-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.updateTransform({ transform_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (string)*: Identifier for the transform.
** *`dest` (Optional, { index, op_type, pipeline, routing, version_type })*: The destination for the transform.
** *`description` (Optional, string)*: Free text description of the transform.
** *`frequency` (Optional, string | -1 | 0)*: The interval between checks for changes in the source indices when the
transform is running continuously. Also determines the retry interval in
the event of transient failures while the transform is searching or
indexing. The minimum value is 1s and the maximum is 1h.
** *`_meta` (Optional, Record<string, User-defined value>)*: Defines optional transform metadata.
** *`source` (Optional, { index, query, remote, size, slice, sort, _source, runtime_mappings })*: The source of the data for the transform.
** *`settings` (Optional, { align_checkpoints, dates_as_epoch_millis, deduce_mappings, docs_per_second, max_page_search_size, unattended })*: Defines optional transform settings.
** *`sync` (Optional, { time })*: Defines the properties transforms require to run continuously.
** *`retention_policy` (Optional, { time } | null)*: Defines a retention policy for the transform. Data that meets the defined
criteria is deleted from the destination index.
** *`defer_validation` (Optional, boolean)*: When true, deferrable validations are not run. This behavior may be
desired if the source index does not exist until after the transform is
created.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the
timeout expires, the request fails and returns an error.

[discrete]
==== upgrade_transforms
Upgrades all transforms.
This API identifies transforms that have a legacy configuration format and upgrades them to the latest version. It
also cleans up the internal data structures that store the transform state and checkpoints. The upgrade does not
affect the source and destination indices. The upgrade also does not affect the roles that transforms use when
Elasticsearch security features are enabled; the role used to read source data and write to the destination index
remains unchanged.

{ref}/upgrade-transforms.html[Endpoint documentation]
[source,ts]
----
client.transform.upgradeTransforms({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`dry_run` (Optional, boolean)*: When true, the request checks for updates but does not run them.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and
returns an error.

[discrete]
=== watcher
[discrete]
==== ack_watch
Acknowledges a watch, manually throttling the execution of the watch's actions.

{ref}/watcher-api-ack-watch.html[Endpoint documentation]
[source,ts]
----
client.watcher.ackWatch({ watch_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`watch_id` (string)*: Watch ID
** *`action_id` (Optional, string | string[])*: A list of the action ids to be acked

[discrete]
==== activate_watch
Activates a currently inactive watch.

{ref}/watcher-api-activate-watch.html[Endpoint documentation]
[source,ts]
----
client.watcher.activateWatch({ watch_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`watch_id` (string)*: Watch ID

[discrete]
==== deactivate_watch
Deactivates a currently active watch.

{ref}/watcher-api-deactivate-watch.html[Endpoint documentation]
[source,ts]
----
client.watcher.deactivateWatch({ watch_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`watch_id` (string)*: Watch ID

[discrete]
==== delete_watch
Removes a watch from Watcher.

{ref}/watcher-api-delete-watch.html[Endpoint documentation]
[source,ts]
----
client.watcher.deleteWatch({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Watch ID

[discrete]
==== execute_watch
This API can be used to force execution of the watch outside of its triggering logic or to simulate the watch execution for debugging purposes.
For testing and debugging purposes, you also have fine-grained control on how the watch runs. You can execute the watch without executing all of its actions or alternatively by simulating them. You can also force execution by ignoring the watch condition and control whether a watch record would be written to the watch history after execution.

{ref}/watcher-api-execute-watch.html[Endpoint documentation]
[source,ts]
----
client.watcher.executeWatch({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: Identifier for the watch.
** *`action_modes` (Optional, Record<string, Enum("simulate" | "force_simulate" | "execute" | "force_execute" | "skip")>)*: Determines how to handle the watch actions as part of the watch execution.
** *`alternative_input` (Optional, Record<string, User-defined value>)*: When present, the watch uses this object as a payload instead of executing its own input.
** *`ignore_condition` (Optional, boolean)*: When set to `true`, the watch execution uses the always condition. This can also be specified as an HTTP parameter.
** *`record_execution` (Optional, boolean)*: When set to `true`, the watch record representing the watch execution result is persisted to the `.watcher-history` index for the current time. In addition, the status of the watch is updated, possibly throttling subsequent executions. This can also be specified as an HTTP parameter.
** *`simulated_actions` (Optional, { actions, all, use_all })*
** *`trigger_data` (Optional, { scheduled_time, triggered_time })*: This structure is parsed as the data of the trigger event that will be used during the watch execution
** *`watch` (Optional, { actions, condition, input, metadata, status, throttle_period, throttle_period_in_millis, transform, trigger })*: When present, this watch is used instead of the one specified in the request. This watch is not persisted to the index and record_execution cannot be set.
** *`debug` (Optional, boolean)*: Defines whether the watch runs in debug mode.

[discrete]
==== get_settings
Retrieve settings for the watcher system index

{ref}/watcher-api-get-settings.html[Endpoint documentation]
[source,ts]
----
client.watcher.getSettings()
----


[discrete]
==== get_watch
Retrieves a watch by its ID.

{ref}/watcher-api-get-watch.html[Endpoint documentation]
[source,ts]
----
client.watcher.getWatch({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Watch ID

[discrete]
==== put_watch
Creates a new watch, or updates an existing one.

{ref}/watcher-api-put-watch.html[Endpoint documentation]
[source,ts]
----
client.watcher.putWatch({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Watch ID
** *`actions` (Optional, Record<string, { add_backing_index, remove_backing_index }>)*
** *`condition` (Optional, { always, array_compare, compare, never, script })*
** *`input` (Optional, { chain, http, search, simple })*
** *`metadata` (Optional, Record<string, User-defined value>)*
** *`throttle_period` (Optional, string)*
** *`transform` (Optional, { chain, script, search })*
** *`trigger` (Optional, { schedule })*
** *`active` (Optional, boolean)*: Specify whether the watch is in/active by default
** *`if_primary_term` (Optional, number)*: only update the watch if the last operation that has changed the watch has the specified primary term
** *`if_seq_no` (Optional, number)*: only update the watch if the last operation that has changed the watch has the specified sequence number
** *`version` (Optional, number)*: Explicit version number for concurrency control

[discrete]
==== query_watches
Retrieves stored watches.

{ref}/watcher-api-query-watches.html[Endpoint documentation]
[source,ts]
----
client.watcher.queryWatches({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`from` (Optional, number)*: The offset from the first result to fetch. Needs to be non-negative.
** *`size` (Optional, number)*: The number of hits to return. Needs to be non-negative.
** *`query` (Optional, { bool, boosting, common, combined_fields, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, knn, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule, script, script_score, semantic, shape, simple_query_string, span_containing, span_field_masking, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, sparse_vector, term, terms, terms_set, text_expansion, weighted_tokens, wildcard, wrapper, type })*: Optional, query filter watches to be returned.
** *`sort` (Optional, string | { _score, _doc, _geo_distance, _script } | string | { _score, _doc, _geo_distance, _script }[])*: Optional sort definition.
** *`search_after` (Optional, number | number | string | boolean | null | User-defined value[])*: Optional search After to do pagination using last hit’s sort values.

[discrete]
==== start
Starts Watcher if it is not already running.

{ref}/watcher-api-start.html[Endpoint documentation]
[source,ts]
----
client.watcher.start()
----


[discrete]
==== stats
Retrieves the current Watcher metrics.

{ref}/watcher-api-stats.html[Endpoint documentation]
[source,ts]
----
client.watcher.stats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`metric` (Optional, Enum("_all" | "queued_watches" | "current_watches" | "pending_watches") | Enum("_all" | "queued_watches" | "current_watches" | "pending_watches")[])*: Defines which additional metrics are included in the response.
** *`emit_stacktraces` (Optional, boolean)*: Defines whether stack traces are generated for each watch that is running.

[discrete]
==== stop
Stops Watcher if it is running.

{ref}/watcher-api-stop.html[Endpoint documentation]
[source,ts]
----
client.watcher.stop()
----


[discrete]
==== update_settings
Update settings for the watcher system index

{ref}/watcher-api-update-settings.html[Endpoint documentation]
[source,ts]
----
client.watcher.updateSettings()
----


[discrete]
=== xpack
[discrete]
==== info
Provides general information about the installed X-Pack features.

{ref}/info-api.html[Endpoint documentation]
[source,ts]
----
client.xpack.info({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`categories` (Optional, Enum("build" | "features" | "license")[])*: A list of the information categories to include in the response. For example, `build,license,features`.
** *`accept_enterprise` (Optional, boolean)*: If this param is used it must be set to true
** *`human` (Optional, boolean)*: Defines whether additional human-readable information is included in the response. In particular, it adds descriptions and a tag line.

[discrete]
==== usage
This API provides information about which features are currently enabled and available under the current license and some usage statistics.

{ref}/usage-api.html[Endpoint documentation]
[source,ts]
----
client.xpack.usage({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.

