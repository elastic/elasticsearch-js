// This file is autogenerated, DO NOT EDIT
// Use `node scripts/generate-docs-examples.js` to generate the docs examples

[source, js]
----
const response = await client.inference.put({
  task_type: "sparse_embedding",
  inference_id: "small_chunk_size",
  inference_config: {
    service: "elasticsearch",
    service_settings: {
      num_allocations: 1,
      num_threads: 1,
    },
    chunking_settings: {
      strategy: "sentence",
      max_chunk_size: 100,
      sentence_overlap: 0,
    },
  },
});
console.log(response);
----
